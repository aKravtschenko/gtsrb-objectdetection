{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import cv2 \n",
    "import subprocess\n",
    "import time\n",
    "import uuid\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as minidom\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was successfully built\n"
     ]
    }
   ],
   "source": [
    "base_config_path = './pipeline.config'\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(base_config_path)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "print(\"Model was successfully built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "#Checking for gpu\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "   raise SystemError('GPU device not found')\n",
    "\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(21)\n",
    "tf.compat.v1.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id_mapping:\n",
    "    # 1:'Speed limit (30km/h)', \n",
    "    # 2:'Speed limit (50km/h)',\n",
    "    # 12:'Priority road', \n",
    "    # 14:'Stop', \n",
    "    # 17:'No entry',\n",
    "    # 41:'Ende des Überholverbots',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE_TRAIN = 50 # Default: 50 Images of each class excluding values in :id_mapping. Used for Trainset\n",
    "SAMPLE_SIZE_TEST = 10 # Default: 10 Images of each class excluding values in :id_mapping . Used for Testset\n",
    "\n",
    "IMAGE_WIDTH = 280 # Image width in pixels to resize for visualization\n",
    "IMAGE_HEIGHT = 280 # Image height in pixels to resize for visualization\n",
    "\n",
    "train_path = './GTSRB/Final_Training/Images' # Location of the training images\n",
    "test_path = './GTSRB/Final_Test/Images' # Location of the test images\n",
    "\n",
    "id_mapping = {1: 1, 2: 2, 12: 3, 14: 4, 17: 5}\n",
    "unknown_label = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"efficientdet_d1_coco17_tpu-32\"\n",
    "MODEL_NAME = \"Efficientdet_8000_640_d1\"\n",
    "IS_SSD = True # True if SSD model is used. False otherwise \n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = 5\n",
    "NUM_STEPS = 8000\n",
    "NUM_EVAL_STEPS = 1000\n",
    "use_bfloat16 = False # Use bfloat16 True for TPU\n",
    "\n",
    "NOTES = \"\" # Training Notes for the log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for Evaluation\n",
    "if IS_SSD:\n",
    "    TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=MODEL_NAME)\n",
    "    TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)\n",
    "    TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short.pbtxt\"\n",
    "else:\n",
    "    TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=MODEL_NAME)\n",
    "    TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "    TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for Training and Inference\n",
    "if IS_SSD:\n",
    "    # Download from path \n",
    "    base_config_path = \"./base_models/ssd/{name}/pipeline.config\".format(name=BASE_MODEL)\n",
    "    base_checkpoint_path = \"./base_models/ssd/{name}/checkpoint/ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "    # Save to path\n",
    "    model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)\n",
    "    config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "    # Upload from path\n",
    "    labelmap_path = \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    train_record_path = \"./myModules/records/trainWoUnknown.record\"\n",
    "    test_record_path = \"./myModules/records/testWoUnknown.record\"\n",
    "    \n",
    "    inference_path = './inference/ssd/{name}'.format(name=MODEL_NAME)\n",
    "else:\n",
    "    base_config_path = \"D:\\\\Desktop-Short\\\\base_models\\\\faster_rcnn\\\\{name}\\\\pipeline.config\".format(name=BASE_MODEL)\n",
    "    base_checkpoint_path = \"D:\\\\Desktop-Short\\\\base_models\\\\{name}\\\\checkpoint\\\\ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "    # Save to path\n",
    "    model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "    config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "    # Upload from path\n",
    "    labelmap_path = \"./myModules/label_map_short.pbtxt\"\n",
    "    train_record_path = \"./myModules/records/train.record\"\n",
    "    test_record_path = \"./myModules/records/test.record\"\n",
    "    \n",
    "    inference_path = './inference/faster_rcnn/{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTrain(rootpath, cache_path):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "    Arguments:\n",
    "        rootpath: path to the traffic sign data, for example './GTSRB/Training'\n",
    "        cache_path: path to the cached DataFrame file, default is 'traffic_signs_data.pkl'\n",
    "    Returns:\n",
    "        DataFrame containing labels, image shapes, ROIs, and image paths\n",
    "    '''\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading: {cache_path}\")\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    data = []\n",
    "    # loop over all 43 classes\n",
    "    for c in range(0, 43):\n",
    "        prefix = f\"{rootpath}/{c:05d}/\"  # subdirectory for class\n",
    "        gtFile = open(prefix + f'GT-{c:05d}.csv')  # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';')  # csv parser for annotations file\n",
    "        next(gtReader)  # skip header\n",
    "\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            img_path = prefix + row[0]\n",
    "            img = plt.imread(img_path)  # load image\n",
    " \n",
    "            label = int(row[7])  # the 8th column is the label\n",
    "            height = img.shape[0]  # height of the image\n",
    "            width = img.shape[1]  # width of the image\n",
    "            #channels = img.shape[2] if len(img.shape) > 2 else 1  # channels of the image (default to 1 if grayscale)\n",
    "            roi_x1 = int(row[3])  # ROI X1 coordinate\n",
    "            roi_y1 = int(row[4])  # ROI Y1 coordinate\n",
    "            roi_x2 = int(row[5])  # ROI X2 coordinate\n",
    "            roi_y2 = int(row[6])  # ROI Y2 coordinate\n",
    "\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "\n",
    "        gtFile.close()\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    \n",
    "    df.to_pickle(cache_path)\n",
    "    print(f\"Dataframe saved in: {cache_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTest(rootpath, cache_path):\n",
    "    '''Reads the final test data for the German Traffic Sign Recognition Benchmark.\n",
    "    Arguments: path to the final test data, for example './GTSRB/Final_Test/Images'\n",
    "    Returns: DataFrame with image data, labels, image shapes, and ROI\n",
    "    '''\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading: {cache_path}\")\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    # Pfad zur CSV-Datei\n",
    "    csv_file = os.path.join(rootpath, 'GT-final_test_gt.csv')\n",
    "    \n",
    "    # Lade die CSV-Datei\n",
    "    df = pd.read_csv(csv_file, delimiter=';')\n",
    "    \n",
    "    # Liste zum Speichern der Daten\n",
    "    data = []\n",
    "    \n",
    "    # Schleife über alle Zeilen der CSV-Datei\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.join(rootpath, row['Filename'])\n",
    "        img = plt.imread(img_path)  # Lade das Bild\n",
    "        \n",
    "        if img is not None:\n",
    "            label = int(row['ClassId'])\n",
    "            height, width, channels = img.shape\n",
    "            roi_x1 = int(row['Roi.X1'])\n",
    "            roi_y1 = int(row['Roi.Y1'])\n",
    "            roi_x2 = int(row['Roi.X2'])\n",
    "            roi_y2 = int(row['Roi.Y2'])\n",
    "            \n",
    "            # Füge die Daten als Zeile hinzu\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "        else:\n",
    "            print(f\"Bild {img_path} konnte nicht geladen werden.\")\n",
    "    \n",
    "    # Erstelle ein DataFrame aus der Liste\n",
    "    df_test = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    df_test.to_pickle(cache_path)\n",
    "    print(f\"Dataframe saved in: {cache_path}\")\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))  # Resize to 280x280\n",
    "    return np.array(image).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_labels(df, selected_labels, unknown_label, sample_size):\n",
    "    \"\"\"\n",
    "    Reassign labels in the dataframe according to selected_labels.\n",
    "    Keep all rows for the labels in selected_labels.\n",
    "    For remaining labels, sample a specified number of rows and reassign them to unknown_label.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with a column named 'Label'.\n",
    "    selected_labels (dict): A dictionary mapping old labels to new labels.\n",
    "    unknown_label (int): The label to assign to the remaining sampled rows.\n",
    "    sample_size (int): The number of rows to sample for each remaining label. Default is 15.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new dataframe with reassigned labels.\n",
    "    \"\"\"\n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    for old_label, new_label in selected_labels.items():\n",
    "        label_df = df[df['Label'] == old_label].copy()\n",
    "        label_df['Label'] = new_label\n",
    "        new_df = pd.concat([new_df, label_df])\n",
    "\n",
    "    unique_labels = df['Label'].unique()\n",
    "    remaining_labels = [label for label in unique_labels if label not in selected_labels]\n",
    "\n",
    "    for label in remaining_labels:\n",
    "        label_df = df[df['Label'] == label]\n",
    "        if len(label_df) > sample_size:\n",
    "            selected_rows = label_df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            selected_rows = label_df\n",
    "        selected_rows['Label'] = unknown_label\n",
    "        new_df = pd.concat([new_df, selected_rows])\n",
    "\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Dataset for YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN_DF = \"./myModules/data/df_train_raw.pkl\"\n",
    "PATH_TO_TEST_DF = \"./myModules/data/df_test_raw.pkl\"\n",
    "\n",
    "PATH_TO_TRAIN_ANNOTATIONS = './yoloNoUnkData/Train/Annotations/'\n",
    "PATH_TO_TRAIN_IMAGES = './yoloNoUnkData/Train/Images/'\n",
    "\n",
    "PATH_TO_TEST_ANNOTATIONS = './yoloNoUnkData/Test/Annotations/'\n",
    "PATH_TO_TEST_IMAGES = './yoloNoUnkData/Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_xml(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ET.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voc_annotation_for_trainset(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        annotation = ET.Element('annotation')\n",
    "        \n",
    "        # Extract subfolder name (e.g., '0000')\n",
    "        subfolder_name = os.path.basename(os.path.dirname(row['Path']))\n",
    "        \n",
    "        # Create unique filename with subfolder prefix (e.g., '0000_00000_00000.xml')\n",
    "        os.path.basename(row['Path']).replace('.ppm', '.jpg')\n",
    "        filename = f\"{subfolder_name}_{os.path.splitext(os.path.basename(row['Path']))[0]}.xml\"\n",
    "        \n",
    "        # Create annotation elements\n",
    "        ET.SubElement(annotation, 'folder').text = 'Images'\n",
    "        ET.SubElement(annotation, 'filename').text = filename.replace('.xml', '.jpg')\n",
    "        \n",
    "        size = ET.SubElement(annotation, 'size')\n",
    "        ET.SubElement(size, 'width').text = str(row['Width'])\n",
    "        ET.SubElement(size, 'height').text = str(row['Height'])\n",
    "        ET.SubElement(size, 'depth').text = '3'  # Assuming RGB images\n",
    "\n",
    "        segmented = ET.SubElement(annotation, 'segmented').text = '0'\n",
    "        \n",
    "        obj = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(obj, 'name').text = str(row['Label'])\n",
    "        ET.SubElement(obj, 'pose').text = 'Unspecified'\n",
    "        ET.SubElement(obj, 'truncated').text = '0'\n",
    "        ET.SubElement(obj, 'difficult').text = '0'\n",
    "        \n",
    "        bndbox = ET.SubElement(obj, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(row['Roi.X1'])\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(row['Roi.Y1'])\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(row['Roi.X2'])\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(row['Roi.Y2'])\n",
    "        \n",
    "        # Write XML file with pretty print\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(prettify_xml(annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voc_annotation_for_testset(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        annotation = ET.Element('annotation')\n",
    "        ET.SubElement(annotation, 'folder').text = 'Images'\n",
    "        ET.SubElement(annotation, 'filename').text = os.path.basename(row['Path']).replace('.ppm', '.jpg')\n",
    "        \n",
    "        size = ET.SubElement(annotation, 'size')\n",
    "        ET.SubElement(size, 'width').text = str(row['Width'])\n",
    "        ET.SubElement(size, 'height').text = str(row['Height'])\n",
    "        ET.SubElement(size, 'depth').text = '3'  # Assuming RGB images\n",
    "\n",
    "        segmented = ET.SubElement(annotation, 'segmented').text = '0'\n",
    "        \n",
    "        obj = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(obj, 'name').text = str(row['Label'])\n",
    "        ET.SubElement(obj, 'pose').text = 'Unspecified'\n",
    "        ET.SubElement(obj, 'truncated').text = '0'\n",
    "        ET.SubElement(obj, 'difficult').text = '0'\n",
    "        \n",
    "        bndbox = ET.SubElement(obj, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(row['Roi.X1'])\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(row['Roi.Y1'])\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(row['Roi.X2'])\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(row['Roi.Y2'])\n",
    "        \n",
    "        # Write XML file with pretty print\n",
    "        output_file = os.path.join(output_dir, os.path.basename(row['Path']).replace('.ppm', '.xml'))\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(prettify_xml(annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ppm_to_jpg_from_df_train(df, jpg_root):\n",
    "    if not os.path.exists(jpg_root):\n",
    "        os.makedirs(jpg_root)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        img_path = row['Path']\n",
    "        if img_path.endswith('.ppm'):\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Extrahiere den Dateinamen und den Subordner aus dem Pfad\n",
    "            folder_name = os.path.basename(os.path.dirname(img_path))  # Subordner\n",
    "            filename = os.path.basename(img_path).replace('.ppm', '.jpg')\n",
    "            jpg_filename = f\"{folder_name}_{filename}\"  # Füge den Subordner-Namen vor dem Dateinamen hinzu\n",
    "            \n",
    "            # Erstelle den Ziel-JPEG-Pfad\n",
    "            jpg_path = os.path.join(jpg_root, jpg_filename)\n",
    "            \n",
    "            # Speichere das Bild im JPEG-Format\n",
    "            img.save(jpg_path, 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ppm_to_jpg_from_df(df, jpg_dir):\n",
    "    if not os.path.exists(jpg_dir):\n",
    "        os.makedirs(jpg_dir)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        img_path = row['Path']\n",
    "        if img_path.endswith('.ppm'):\n",
    "            img = Image.open(img_path)\n",
    "            # Extrahiere den Dateinamen und den Unterordner aus dem Pfad\n",
    "            subfolder = os.path.basename(os.path.dirname(img_path))\n",
    "            filename = os.path.basename(img_path).replace('.ppm', '.jpg')\n",
    "            # Erstelle den Zielpfad inklusive Unterordner\n",
    "            jpg_subdir = os.path.join(jpg_dir, subfolder)\n",
    "            if not os.path.exists(jpg_subdir):\n",
    "                os.makedirs(jpg_subdir)\n",
    "            jpg_path = os.path.join(jpg_subdir, filename)\n",
    "            img.save(jpg_path, 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = readTrafficSignsTrain(train_path, PATH_TO_TRAIN_DF)\n",
    "create_voc_annotation_for_trainset(df_final_train, PATH_TO_TRAIN_ANNOTATIONS)\n",
    "convert_ppm_to_jpg_from_df_train(df_final_train, PATH_TO_TRAIN_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = readTrafficSignsTest(test_path, PATH_TO_TEST_DF)\n",
    "create_voc_annotation_for_testset(df_final_test, PATH_TO_TEST_ANNOTATIONS)\n",
    "convert_ppm_to_jpg_from_df(df_final_test, PATH_TO_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./myModules/data/df_train_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "df_train_cache = readTrafficSignsTrain(train_path, \"./myModules/data/df_train_raw.pkl\")\n",
    "df_train = pd.DataFrame()\n",
    "df_train = df_train_cache.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./myModules/data/df_test_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "df_test_cache = readTrafficSignsTest(test_path, \"./myModules/data/df_test_raw.pkl\")\n",
    "df_test = pd.DataFrame()\n",
    "df_test = df_test_cache.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_train = reassign_labels(df_train, id_mapping, unknown_label, SAMPLE_SIZE_TRAIN)\n",
    "df_final_test = reassign_labels(df_test, id_mapping, unknown_label, SAMPLE_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to exlude the unknown Labels\n",
    "df_final_train = df_final_train[df_final_train['Label'] != 6]\n",
    "df_final_test = df_final_test[df_final_test['Label'] != 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2790 entries, 0 to 2789\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Width   2790 non-null   int64 \n",
      " 1   Height  2790 non-null   int64 \n",
      " 2   Roi.X1  2790 non-null   int64 \n",
      " 3   Roi.Y1  2790 non-null   int64 \n",
      " 4   Roi.X2  2790 non-null   int64 \n",
      " 5   Roi.Y2  2790 non-null   int64 \n",
      " 6   Path    2790 non-null   object\n",
      " 7   Label   2790 non-null   int64 \n",
      "dtypes: int64(7), object(1)\n",
      "memory usage: 196.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8460 entries, 0 to 8459\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Width   8460 non-null   int64 \n",
      " 1   Height  8460 non-null   int64 \n",
      " 2   Roi.X1  8460 non-null   int64 \n",
      " 3   Roi.Y1  8460 non-null   int64 \n",
      " 4   Roi.X2  8460 non-null   int64 \n",
      " 5   Roi.Y2  8460 non-null   int64 \n",
      " 6   Path    8460 non-null   object\n",
      " 7   Label   8460 non-null   int64 \n",
      "dtypes: int64(7), object(1)\n",
      "memory usage: 594.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Verfify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "short_classes = { 1: 'Geschwindigkeitsbegrenzung (30km/h)', \n",
    "                  2: 'Geschwindigkeitsbegrenzung (50km/h)', \n",
    "                  3: 'Vorrangstraße', \n",
    "                  4: 'Stop', \n",
    "                  5: 'Einfahrt verboten',\n",
    "                  6: 'Unbekannt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_train['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_test['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_label_df = df_final_train[df_final_train['Label'] == 6]\n",
    "\n",
    "# Wähle zufällig 100 Bilder aus\n",
    "image_paths_to_display = unknown_label_df['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "# Plotten der ausgewählten Bilder\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_final = df_final_train['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_final)\n",
    "\n",
    "label_counts_named_final = {short_classes[key]: value for key, value in label_counts_final.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_final.keys()), y=list(label_counts_named_final.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen für bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_test = df_final_test['Label'].value_counts().sort_index()\n",
    "label_counts_train = df_final_train['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_train = {short_classes[key]: value for key, value in label_counts_train.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_train.keys()), y=list(label_counts_named_train.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen für bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_test = {short_classes[key]: value for key, value in label_counts_test.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_test.keys()), y=list(label_counts_named_test.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Testdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen für bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Tf-records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(row):\n",
    "    img_path = row['Path']\n",
    "    # Lade das Bild und konvertiere es in ein kompatibles Format (z.B. JPEG)\n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    with io.BytesIO() as output:\n",
    "        image.save(output, format=\"JPEG\")\n",
    "        encoded_jpg = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = row['Path'].encode('utf8')\n",
    "    image_format = b'jpeg'  # Ändere dies entsprechend des konvertierten Bildformats\n",
    "    xmins = [row['Roi.X1'] / width]\n",
    "    xmaxs = [row['Roi.X2'] / width]\n",
    "    ymins = [row['Roi.Y1'] / height]\n",
    "    ymaxs = [row['Roi.Y2'] / height]\n",
    "    classes_text = [str(row['Label']).encode('utf8')]\n",
    "    classes = [int(row['Label'])]\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_jpg])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n",
    "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n",
    "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n",
    "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n",
    "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n",
    "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(df, output_path):\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    for _, row in df.iterrows():\n",
    "        tf_example = create_tf_example(row)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tf_record(df_final_train, './myModules/records/trainWoUnknown.record')\n",
    "create_tf_record(df_final_test, './myModules/records/testWoUnknown.record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Verify Tensord records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_example(serialized_example):\n",
    "    feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/class/text': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(record_file):\n",
    "    raw_dataset = tf.data.TFRecordDataset(record_file)\n",
    "    parsed_dataset = raw_dataset.map(parse_tf_example)\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_at_index(parsed_dataset, index):\n",
    "    for i, tf_example in enumerate(parsed_dataset):\n",
    "        if i == index:\n",
    "            height = tf_example['image/height'].numpy()\n",
    "            width = tf_example['image/width'].numpy()\n",
    "            encoded_image = tf_example['image/encoded'].numpy()\n",
    "            xmin = tf_example['image/object/bbox/xmin'].numpy()\n",
    "            xmax = tf_example['image/object/bbox/xmax'].numpy()\n",
    "            ymin = tf_example['image/object/bbox/ymin'].numpy()\n",
    "            ymax = tf_example['image/object/bbox/ymax'].numpy()\n",
    "            label = tf_example['image/object/class/label'].numpy()\n",
    "\n",
    "            image = tf.image.decode_jpeg(encoded_image)\n",
    "            image_np = image.numpy()\n",
    "\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(image_np)\n",
    "            plt.title(f'Label: {label}')\n",
    "\n",
    "            # Draw the bounding box\n",
    "            plt.gca().add_patch(plt.Rectangle((xmin * width, ymin * height), \n",
    "                                              (xmax - xmin) * width, (ymax - ymin) * height,\n",
    "                                              edgecolor='green', facecolor='none', linewidth=2))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record = read_tfrecord('./myModules/records/trainWoUnknown.record')\n",
    "test_record = read_tfrecord('./myModules/records/testWoUnknown.record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_at_index(test_record, 888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image_at_index(train_record, 8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_length(df):\n",
    "    count_images = len(df)\n",
    "    labels_count = df['Label'].value_counts()\n",
    "\n",
    "    sorted_labels_count = labels_count.sort_index()\n",
    "    return sorted_labels_count.tolist(), count_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_info(batch, classes, steps, eval_steps, name, bfloat16, pipeline, \n",
    "                   checkpoint_path, labelmap, train_record, test_record, config, \n",
    "                   trained_model, train_time, df_train, df_test, notes):\n",
    "    \n",
    "    \n",
    "    test_count, test_len = count_length(df_test)\n",
    "    train_count, train_len = count_length(df_train)   \n",
    "    log_contents = f\"\"\"\n",
    "Trainingparameters:\n",
    "    BATCH_SIZE = {batch} # Cannot be higher than 2-4 (lack of ressources)\n",
    "    NUM_CLASSES = {classes} # Total number of classes to train is 43 - used for training are only 6-7\n",
    "    NUM_STEPS = {steps}\n",
    "    NUM_EVAL_STEPS = {eval_steps}\n",
    "    MODEL_NAME = {name}\n",
    "    use_bfloat16 = {bfloat16} # Use bfloat16 = True for trainign with TPU\n",
    "    \n",
    "Locations: \n",
    "    Path to Pipeline-Config = {pipeline}\n",
    "    Checkpoint from transfermodel  = {checkpoint_path}\n",
    "\n",
    "    Path to the trainedmodel = {trained_model}\n",
    "    Config of the trainedmodel = {config}\n",
    "    \n",
    "Used records: \n",
    "    Used labelmap = {labelmap}\n",
    "    Used train_record = {train_record}\n",
    "    Used test_record = {test_record}\n",
    "\n",
    "Generell Information: \n",
    "    Time needed for modeltraining = {train_time}\n",
    "    Length of traindataset = {train_len}\n",
    "    Counted values for each Label form Traindataset = {train_count}\n",
    "    Length of testdataset = {test_len}\n",
    "    Counted values for each Label form Testdataset = {test_count}\n",
    "\"\"\"\n",
    "\n",
    "    log_id = uuid.uuid4()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_entry = f\"ID: {log_id}\\nTimestamp: {timestamp}\\n\\n{log_contents}\\nNotes: {notes}\\n\"\n",
    "\n",
    "    filename = f\"./myModules/log/{name}_{log_id}.txt\"\n",
    "    with open(filename, \"a\") as file:\n",
    "        file.write(log_entry)\n",
    "        file.write(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_config_path) as f:\n",
    "    config = f.read()\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "  \n",
    "  # Set labelmap path\n",
    "  config = re.sub('label_map_path: \".*?\"', \n",
    "             'label_map_path: \"{}\"'.format(labelmap_path), config)\n",
    "  \n",
    "  # Set fine_tune_checkpoint path\n",
    "  config = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                  'fine_tune_checkpoint: \"{}\"'.format(base_checkpoint_path), config)\n",
    "  \n",
    "  # Set train tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(train_record_path), config)\n",
    "  \n",
    "  # Set test tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(test_record_path), config)\n",
    "  \n",
    "  # Set number of classes.\n",
    "  config = re.sub('num_classes: [0-9]+',\n",
    "                  'num_classes: {}'.format(NUM_CLASSES), config)\n",
    "  \n",
    "  # Set batch size\n",
    "  config = re.sub('batch_size: [0-9]+',\n",
    "                  'batch_size: {}'.format(BATCH_SIZE), config)\n",
    "  \n",
    "  # Set training steps\n",
    "  config = re.sub('num_steps: [0-9]+',\n",
    "                  'num_steps: {}'.format(NUM_STEPS), config)\n",
    "  \n",
    "    # Set use_bfloat16\n",
    "  config = re.sub('use_bfloat16: (true|false)',\n",
    "                  'use_bfloat16: {}'.format(str(use_bfloat16).lower()), config)\n",
    "  \n",
    "  # Set fine-tune checkpoint type to detection\n",
    "  config = re.sub('fine_tune_checkpoint_type: \"classification\"', \n",
    "             'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n",
    "  \n",
    "  f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model {\n",
      "  ssd {\n",
      "    num_classes: 5\n",
      "    image_resizer {\n",
      "      keep_aspect_ratio_resizer {\n",
      "        min_dimension: 640\n",
      "        max_dimension: 640\n",
      "        pad_to_max_dimension: true\n",
      "      }\n",
      "    }\n",
      "    feature_extractor {\n",
      "      type: \"ssd_efficientnet-b1_bifpn_keras\"\n",
      "      conv_hyperparams {\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 3.9999998989515007e-05\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          truncated_normal_initializer {\n",
      "            mean: 0.0\n",
      "            stddev: 0.029999999329447746\n",
      "          }\n",
      "        }\n",
      "        activation: SWISH\n",
      "        batch_norm {\n",
      "          decay: 0.9900000095367432\n",
      "          scale: true\n",
      "          epsilon: 0.0010000000474974513\n",
      "        }\n",
      "        force_use_bias: true\n",
      "      }\n",
      "      bifpn {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "        num_iterations: 4\n",
      "        num_filters: 88\n",
      "      }\n",
      "    }\n",
      "    box_coder {\n",
      "      faster_rcnn_box_coder {\n",
      "        y_scale: 1.0\n",
      "        x_scale: 1.0\n",
      "        height_scale: 1.0\n",
      "        width_scale: 1.0\n",
      "      }\n",
      "    }\n",
      "    matcher {\n",
      "      argmax_matcher {\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.5\n",
      "        ignore_thresholds: false\n",
      "        negatives_lower_than_unmatched: true\n",
      "        force_match_for_each_row: true\n",
      "        use_matmul_gather: true\n",
      "      }\n",
      "    }\n",
      "    similarity_calculator {\n",
      "      iou_similarity {\n",
      "      }\n",
      "    }\n",
      "    box_predictor {\n",
      "      weight_shared_convolutional_box_predictor {\n",
      "        conv_hyperparams {\n",
      "          regularizer {\n",
      "            l2_regularizer {\n",
      "              weight: 3.9999998989515007e-05\n",
      "            }\n",
      "          }\n",
      "          initializer {\n",
      "            random_normal_initializer {\n",
      "              mean: 0.0\n",
      "              stddev: 0.009999999776482582\n",
      "            }\n",
      "          }\n",
      "          activation: SWISH\n",
      "          batch_norm {\n",
      "            decay: 0.9900000095367432\n",
      "            scale: true\n",
      "            epsilon: 0.0010000000474974513\n",
      "          }\n",
      "          force_use_bias: true\n",
      "        }\n",
      "        depth: 88\n",
      "        num_layers_before_predictor: 3\n",
      "        kernel_size: 3\n",
      "        class_prediction_bias_init: -4.599999904632568\n",
      "        use_depthwise: true\n",
      "      }\n",
      "    }\n",
      "    anchor_generator {\n",
      "      multiscale_anchor_generator {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "        anchor_scale: 4.0\n",
      "        aspect_ratios: 1.0\n",
      "        aspect_ratios: 2.0\n",
      "        aspect_ratios: 0.5\n",
      "        aspect_ratios: 3.0\n",
      "        aspect_ratios: 0.333\n",
      "        aspect_ratios: 1.5\n",
      "        aspect_ratios: 0.667\n",
      "        scales_per_octave: 3\n",
      "      }\n",
      "    }\n",
      "    post_processing {\n",
      "      batch_non_max_suppression {\n",
      "        score_threshold: 9.99999993922529e-09\n",
      "        iou_threshold: 0.5\n",
      "        max_detections_per_class: 100\n",
      "        max_total_detections: 100\n",
      "      }\n",
      "      score_converter: SIGMOID\n",
      "    }\n",
      "    normalize_loss_by_num_matches: true\n",
      "    loss {\n",
      "      localization_loss {\n",
      "        weighted_smooth_l1 {\n",
      "        }\n",
      "      }\n",
      "      classification_loss {\n",
      "        weighted_sigmoid_focal {\n",
      "          gamma: 1.5\n",
      "          alpha: 0.25\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 1.0\n",
      "    }\n",
      "    encode_background_as_zeros: true\n",
      "    normalize_loc_loss_by_codesize: true\n",
      "    inplace_batchnorm_update: true\n",
      "    freeze_batchnorm: false\n",
      "    add_background_class: false\n",
      "  }\n",
      "}\n",
      "train_config {\n",
      "  batch_size: 6\n",
      "  data_augmentation_options {\n",
      "    random_horizontal_flip {\n",
      "    }\n",
      "  }\n",
      "  data_augmentation_options {\n",
      "    random_scale_crop_and_pad_to_square {\n",
      "      output_size: 640\n",
      "      scale_min: 0.10000000149011612\n",
      "      scale_max: 2.0\n",
      "    }\n",
      "  }\n",
      "  sync_replicas: true\n",
      "  optimizer {\n",
      "    momentum_optimizer {\n",
      "      learning_rate {\n",
      "        cosine_decay_learning_rate {\n",
      "          learning_rate_base: 0.02\n",
      "          total_steps: 300000\n",
      "          warmup_learning_rate: 0.0066\n",
      "          warmup_steps: 2500\n",
      "        }\n",
      "      }\n",
      "      momentum_optimizer_value: 0.8999999761581421\n",
      "    }\n",
      "    use_moving_average: false\n",
      "  }\n",
      "  fine_tune_checkpoint: \"./base_models/ssd/efficientdet_d1_coco17_tpu-32/checkpoint/ckpt-0\"\n",
      "  num_steps: 8000\n",
      "  startup_delay_steps: 0.0\n",
      "  replicas_to_aggregate: 8\n",
      "  max_number_of_boxes: 100\n",
      "  unpad_groundtruth_tensors: false\n",
      "  fine_tune_checkpoint_type: \"detection\"\n",
      "  use_bfloat16: false\n",
      "  fine_tune_checkpoint_version: V2\n",
      "}\n",
      "train_input_reader: {\n",
      "  label_map_path: \"./myModules/label_map_short_woUnk.pbtxt\"\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/trainWoUnknown.record\"\n",
      "  }\n",
      "}\n",
      "\n",
      "eval_config: {\n",
      "  metrics_set: \"coco_detection_metrics\"\n",
      "  use_moving_averages: false\n",
      "  batch_size: 6;\n",
      "}\n",
      "\n",
      "eval_input_reader: {\n",
      "  label_map_path: \"./myModules/label_map_short_woUnk.pbtxt\"\n",
      "  shuffle: false\n",
      "  num_epochs: 1\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/trainWoUnknown.record\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(config_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "WARNING:tensorflow:From C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py:100: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "W0727 13:02:28.862229 18776 deprecation.py:350] From C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py:100: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "2024-07-27 13:02:28.863685: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-27 13:02:28.942213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0',), communication = CommunicationImplementation.AUTO\n",
      "I0727 13:02:29.202762 18776 collective_all_reduce_strategy.py:446] Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0',), communication = CommunicationImplementation.AUTO\n",
      "INFO:tensorflow:Maybe overwriting train_steps: 8000\n",
      "I0727 13:02:29.212424 18776 config_util.py:552] Maybe overwriting train_steps: 8000\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0727 13:02:29.212424 18776 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "I0727 13:02:29.221989 18776 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
      "I0727 13:02:29.221989 18776 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 88\n",
      "I0727 13:02:29.221989 18776 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 4\n",
      "I0727 13:02:29.224993 18776 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I0727 13:02:29.271984 18776 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I0727 13:02:29.271984 18776 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I0727 13:02:29.471012 18776 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I0727 13:02:29.471012 18776 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0727 13:02:29.819396 18776 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0727 13:02:29.819396 18776 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I0727 13:02:30.165445 18776 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I0727 13:02:30.165445 18776 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I0727 13:02:30.630625 18776 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I0727 13:02:30.630625 18776 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I0727 13:02:31.077234 18776 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I0727 13:02:31.077234 18776 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I0727 13:02:31.630671 18776 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I0727 13:02:31.630671 18776 efficientnet_model.py:143] round_filter input=320 output=320\n",
      "I0727 13:02:31.856361 18776 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
      "I0727 13:02:31.907979 18776 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W0727 13:02:31.956421 18776 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['./myModules/records/trainWoUnknown.record']\n",
      "I0727 13:02:31.964967 18776 dataset_builder.py:162] Reading unweighted datasets: ['./myModules/records/trainWoUnknown.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['./myModules/records/trainWoUnknown.record']\n",
      "I0727 13:02:31.964967 18776 dataset_builder.py:79] Reading record datasets for input file: ['./myModules/records/trainWoUnknown.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I0727 13:02:31.964967 18776 dataset_builder.py:80] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0727 13:02:31.964967 18776 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W0727 13:02:31.971021 18776 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0727 13:02:32.080777 18776 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W0727 13:02:38.323539 18776 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0727 13:02:41.897341 18776 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n",
      "I0727 13:02:50.503139 20376 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0727 13:03:01.657060 20932 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-27 13:03:07.903090: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8907\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0727 13:03:15.371911 20520 deprecation.py:554] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "I0727 13:03:18.103225 20520 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "W0727 13:03:24.204919 20520 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "I0727 13:03:29.766187  9480 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "W0727 13:03:35.380747  9480 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "I0727 13:03:39.812777  3828 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "W0727 13:03:45.554970  3828 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "I0727 13:03:51.872291 14328 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "W0727 13:03:57.199579 14328 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2024-07-27 13:04:27.522068: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.17GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 13:04:27.562902: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.21GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 13:04:27.709314: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1007.72MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 13:04:27.733653: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 13:04:28.116545: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.00GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 13:04:28.120699: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 13:04:28.287073: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.47GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 13:04:28.288900: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 13:04:28.296957: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 13:04:28.301114: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "INFO:tensorflow:Step 100 per-step time 1.550s\n",
      "I0727 13:05:50.114995 18776 model_lib_v2.py:705] Step 100 per-step time 1.550s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 1.0355402,\n",
      " 'Loss/localization_loss': 0.019417845,\n",
      " 'Loss/regularization_loss': 0.029548604,\n",
      " 'Loss/total_loss': 1.0845068,\n",
      " 'learning_rate': 0.007136}\n",
      "I0727 13:05:50.116025 18776 model_lib_v2.py:708] {'Loss/classification_loss': 1.0355402,\n",
      " 'Loss/localization_loss': 0.019417845,\n",
      " 'Loss/regularization_loss': 0.029548604,\n",
      " 'Loss/total_loss': 1.0845068,\n",
      " 'learning_rate': 0.007136}\n",
      "INFO:tensorflow:Step 200 per-step time 0.822s\n",
      "I0727 13:07:12.356245 18776 model_lib_v2.py:705] Step 200 per-step time 0.822s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.770502,\n",
      " 'Loss/localization_loss': 0.016917039,\n",
      " 'Loss/regularization_loss': 0.029600853,\n",
      " 'Loss/total_loss': 0.8170199,\n",
      " 'learning_rate': 0.0076719997}\n",
      "I0727 13:07:12.356245 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.770502,\n",
      " 'Loss/localization_loss': 0.016917039,\n",
      " 'Loss/regularization_loss': 0.029600853,\n",
      " 'Loss/total_loss': 0.8170199,\n",
      " 'learning_rate': 0.0076719997}\n",
      "INFO:tensorflow:Step 300 per-step time 0.820s\n",
      "I0727 13:08:34.254446 18776 model_lib_v2.py:705] Step 300 per-step time 0.820s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.6060593,\n",
      " 'Loss/localization_loss': 0.012081675,\n",
      " 'Loss/regularization_loss': 0.029624915,\n",
      " 'Loss/total_loss': 0.64776593,\n",
      " 'learning_rate': 0.008208}\n",
      "I0727 13:08:34.255453 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.6060593,\n",
      " 'Loss/localization_loss': 0.012081675,\n",
      " 'Loss/regularization_loss': 0.029624915,\n",
      " 'Loss/total_loss': 0.64776593,\n",
      " 'learning_rate': 0.008208}\n",
      "INFO:tensorflow:Step 400 per-step time 0.823s\n",
      "I0727 13:09:56.596391 18776 model_lib_v2.py:705] Step 400 per-step time 0.823s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.48338732,\n",
      " 'Loss/localization_loss': 0.012263296,\n",
      " 'Loss/regularization_loss': 0.02964654,\n",
      " 'Loss/total_loss': 0.52529716,\n",
      " 'learning_rate': 0.0087439995}\n",
      "I0727 13:09:56.596391 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.48338732,\n",
      " 'Loss/localization_loss': 0.012263296,\n",
      " 'Loss/regularization_loss': 0.02964654,\n",
      " 'Loss/total_loss': 0.52529716,\n",
      " 'learning_rate': 0.0087439995}\n",
      "INFO:tensorflow:Step 500 per-step time 0.814s\n",
      "I0727 13:11:18.038239 18776 model_lib_v2.py:705] Step 500 per-step time 0.814s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.46122932,\n",
      " 'Loss/localization_loss': 0.00915557,\n",
      " 'Loss/regularization_loss': 0.029675202,\n",
      " 'Loss/total_loss': 0.5000601,\n",
      " 'learning_rate': 0.00928}\n",
      "I0727 13:11:18.039264 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.46122932,\n",
      " 'Loss/localization_loss': 0.00915557,\n",
      " 'Loss/regularization_loss': 0.029675202,\n",
      " 'Loss/total_loss': 0.5000601,\n",
      " 'learning_rate': 0.00928}\n",
      "INFO:tensorflow:Step 600 per-step time 0.833s\n",
      "I0727 13:12:41.311780 18776 model_lib_v2.py:705] Step 600 per-step time 0.833s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.4436377,\n",
      " 'Loss/localization_loss': 0.0074750814,\n",
      " 'Loss/regularization_loss': 0.029680032,\n",
      " 'Loss/total_loss': 0.48079282,\n",
      " 'learning_rate': 0.009816}\n",
      "I0727 13:12:41.311780 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.4436377,\n",
      " 'Loss/localization_loss': 0.0074750814,\n",
      " 'Loss/regularization_loss': 0.029680032,\n",
      " 'Loss/total_loss': 0.48079282,\n",
      " 'learning_rate': 0.009816}\n",
      "INFO:tensorflow:Step 700 per-step time 0.800s\n",
      "I0727 13:14:01.395267 18776 model_lib_v2.py:705] Step 700 per-step time 0.800s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.6941147,\n",
      " 'Loss/localization_loss': 0.0045377547,\n",
      " 'Loss/regularization_loss': 0.029683739,\n",
      " 'Loss/total_loss': 0.72833616,\n",
      " 'learning_rate': 0.010352}\n",
      "I0727 13:14:01.396269 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.6941147,\n",
      " 'Loss/localization_loss': 0.0045377547,\n",
      " 'Loss/regularization_loss': 0.029683739,\n",
      " 'Loss/total_loss': 0.72833616,\n",
      " 'learning_rate': 0.010352}\n",
      "INFO:tensorflow:Step 800 per-step time 0.799s\n",
      "I0727 13:15:21.284310 18776 model_lib_v2.py:705] Step 800 per-step time 0.799s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.46006644,\n",
      " 'Loss/localization_loss': 0.008288307,\n",
      " 'Loss/regularization_loss': 0.029705908,\n",
      " 'Loss/total_loss': 0.49806064,\n",
      " 'learning_rate': 0.010887999}\n",
      "I0727 13:15:21.285310 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.46006644,\n",
      " 'Loss/localization_loss': 0.008288307,\n",
      " 'Loss/regularization_loss': 0.029705908,\n",
      " 'Loss/total_loss': 0.49806064,\n",
      " 'learning_rate': 0.010887999}\n",
      "INFO:tensorflow:Step 900 per-step time 0.803s\n",
      "I0727 13:16:41.532524 18776 model_lib_v2.py:705] Step 900 per-step time 0.803s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.5186268,\n",
      " 'Loss/localization_loss': 0.006983895,\n",
      " 'Loss/regularization_loss': 0.029713035,\n",
      " 'Loss/total_loss': 0.5553237,\n",
      " 'learning_rate': 0.011423999}\n",
      "I0727 13:16:41.532524 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.5186268,\n",
      " 'Loss/localization_loss': 0.006983895,\n",
      " 'Loss/regularization_loss': 0.029713035,\n",
      " 'Loss/total_loss': 0.5553237,\n",
      " 'learning_rate': 0.011423999}\n",
      "INFO:tensorflow:Step 1000 per-step time 0.765s\n",
      "I0727 13:17:58.142281 18776 model_lib_v2.py:705] Step 1000 per-step time 0.765s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.37056276,\n",
      " 'Loss/localization_loss': 0.003975586,\n",
      " 'Loss/regularization_loss': 0.029756732,\n",
      " 'Loss/total_loss': 0.4042951,\n",
      " 'learning_rate': 0.01196}\n",
      "I0727 13:17:58.143279 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.37056276,\n",
      " 'Loss/localization_loss': 0.003975586,\n",
      " 'Loss/regularization_loss': 0.029756732,\n",
      " 'Loss/total_loss': 0.4042951,\n",
      " 'learning_rate': 0.01196}\n",
      "INFO:tensorflow:Step 1100 per-step time 0.806s\n",
      "I0727 13:19:18.669389 18776 model_lib_v2.py:705] Step 1100 per-step time 0.806s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.3869284,\n",
      " 'Loss/localization_loss': 0.004884808,\n",
      " 'Loss/regularization_loss': 0.029775213,\n",
      " 'Loss/total_loss': 0.42158842,\n",
      " 'learning_rate': 0.012496}\n",
      "I0727 13:19:18.670408 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.3869284,\n",
      " 'Loss/localization_loss': 0.004884808,\n",
      " 'Loss/regularization_loss': 0.029775213,\n",
      " 'Loss/total_loss': 0.42158842,\n",
      " 'learning_rate': 0.012496}\n",
      "INFO:tensorflow:Step 1200 per-step time 0.799s\n",
      "I0727 13:20:38.571394 18776 model_lib_v2.py:705] Step 1200 per-step time 0.799s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.34954137,\n",
      " 'Loss/localization_loss': 0.004550803,\n",
      " 'Loss/regularization_loss': 0.029821124,\n",
      " 'Loss/total_loss': 0.3839133,\n",
      " 'learning_rate': 0.013032}\n",
      "I0727 13:20:38.572400 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.34954137,\n",
      " 'Loss/localization_loss': 0.004550803,\n",
      " 'Loss/regularization_loss': 0.029821124,\n",
      " 'Loss/total_loss': 0.3839133,\n",
      " 'learning_rate': 0.013032}\n",
      "INFO:tensorflow:Step 1300 per-step time 0.801s\n",
      "I0727 13:21:58.691871 18776 model_lib_v2.py:705] Step 1300 per-step time 0.801s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.32998347,\n",
      " 'Loss/localization_loss': 0.0051758206,\n",
      " 'Loss/regularization_loss': 0.029845169,\n",
      " 'Loss/total_loss': 0.36500448,\n",
      " 'learning_rate': 0.013567999}\n",
      "I0727 13:21:58.692867 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.32998347,\n",
      " 'Loss/localization_loss': 0.0051758206,\n",
      " 'Loss/regularization_loss': 0.029845169,\n",
      " 'Loss/total_loss': 0.36500448,\n",
      " 'learning_rate': 0.013567999}\n",
      "INFO:tensorflow:Step 1400 per-step time 0.825s\n",
      "I0727 13:23:21.230187 18776 model_lib_v2.py:705] Step 1400 per-step time 0.825s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.54273784,\n",
      " 'Loss/localization_loss': 0.004301254,\n",
      " 'Loss/regularization_loss': 0.029883247,\n",
      " 'Loss/total_loss': 0.57692236,\n",
      " 'learning_rate': 0.014103999}\n",
      "I0727 13:23:21.231181 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.54273784,\n",
      " 'Loss/localization_loss': 0.004301254,\n",
      " 'Loss/regularization_loss': 0.029883247,\n",
      " 'Loss/total_loss': 0.57692236,\n",
      " 'learning_rate': 0.014103999}\n",
      "INFO:tensorflow:Step 1500 per-step time 0.805s\n",
      "I0727 13:24:41.605771 18776 model_lib_v2.py:705] Step 1500 per-step time 0.805s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.5050834,\n",
      " 'Loss/localization_loss': 0.0040253024,\n",
      " 'Loss/regularization_loss': 0.029899258,\n",
      " 'Loss/total_loss': 0.5390079,\n",
      " 'learning_rate': 0.01464}\n",
      "I0727 13:24:41.605771 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.5050834,\n",
      " 'Loss/localization_loss': 0.0040253024,\n",
      " 'Loss/regularization_loss': 0.029899258,\n",
      " 'Loss/total_loss': 0.5390079,\n",
      " 'learning_rate': 0.01464}\n",
      "INFO:tensorflow:Step 1600 per-step time 0.855s\n",
      "I0727 13:26:07.205102 18776 model_lib_v2.py:705] Step 1600 per-step time 0.855s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.45655447,\n",
      " 'Loss/localization_loss': 0.0035059573,\n",
      " 'Loss/regularization_loss': 0.029922444,\n",
      " 'Loss/total_loss': 0.48998287,\n",
      " 'learning_rate': 0.015175999}\n",
      "I0727 13:26:07.205102 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.45655447,\n",
      " 'Loss/localization_loss': 0.0035059573,\n",
      " 'Loss/regularization_loss': 0.029922444,\n",
      " 'Loss/total_loss': 0.48998287,\n",
      " 'learning_rate': 0.015175999}\n",
      "INFO:tensorflow:Step 1700 per-step time 0.840s\n",
      "I0727 13:27:31.141699 18776 model_lib_v2.py:705] Step 1700 per-step time 0.840s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.34078506,\n",
      " 'Loss/localization_loss': 0.00451407,\n",
      " 'Loss/regularization_loss': 0.029957255,\n",
      " 'Loss/total_loss': 0.3752564,\n",
      " 'learning_rate': 0.015712}\n",
      "I0727 13:27:31.141699 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.34078506,\n",
      " 'Loss/localization_loss': 0.00451407,\n",
      " 'Loss/regularization_loss': 0.029957255,\n",
      " 'Loss/total_loss': 0.3752564,\n",
      " 'learning_rate': 0.015712}\n",
      "INFO:tensorflow:Step 1800 per-step time 0.847s\n",
      "I0727 13:28:55.815216 18776 model_lib_v2.py:705] Step 1800 per-step time 0.847s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2894569,\n",
      " 'Loss/localization_loss': 0.0037300487,\n",
      " 'Loss/regularization_loss': 0.030005915,\n",
      " 'Loss/total_loss': 0.32319286,\n",
      " 'learning_rate': 0.016247999}\n",
      "I0727 13:28:55.815216 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.2894569,\n",
      " 'Loss/localization_loss': 0.0037300487,\n",
      " 'Loss/regularization_loss': 0.030005915,\n",
      " 'Loss/total_loss': 0.32319286,\n",
      " 'learning_rate': 0.016247999}\n",
      "INFO:tensorflow:Step 1900 per-step time 0.844s\n",
      "I0727 13:30:20.204626 18776 model_lib_v2.py:705] Step 1900 per-step time 0.844s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.28757983,\n",
      " 'Loss/localization_loss': 0.0026992094,\n",
      " 'Loss/regularization_loss': 0.030078568,\n",
      " 'Loss/total_loss': 0.3203576,\n",
      " 'learning_rate': 0.016784}\n",
      "I0727 13:30:20.205625 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.28757983,\n",
      " 'Loss/localization_loss': 0.0026992094,\n",
      " 'Loss/regularization_loss': 0.030078568,\n",
      " 'Loss/total_loss': 0.3203576,\n",
      " 'learning_rate': 0.016784}\n",
      "INFO:tensorflow:Step 2000 per-step time 0.832s\n",
      "I0727 13:31:43.462090 18776 model_lib_v2.py:705] Step 2000 per-step time 0.832s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.22810344,\n",
      " 'Loss/localization_loss': 0.0023810829,\n",
      " 'Loss/regularization_loss': 0.030122058,\n",
      " 'Loss/total_loss': 0.2606066,\n",
      " 'learning_rate': 0.01732}\n",
      "I0727 13:31:43.463093 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.22810344,\n",
      " 'Loss/localization_loss': 0.0023810829,\n",
      " 'Loss/regularization_loss': 0.030122058,\n",
      " 'Loss/total_loss': 0.2606066,\n",
      " 'learning_rate': 0.01732}\n",
      "INFO:tensorflow:Step 2100 per-step time 0.860s\n",
      "I0727 13:33:09.381642 18776 model_lib_v2.py:705] Step 2100 per-step time 0.860s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2684307,\n",
      " 'Loss/localization_loss': 0.0041333158,\n",
      " 'Loss/regularization_loss': 0.030156188,\n",
      " 'Loss/total_loss': 0.30272022,\n",
      " 'learning_rate': 0.017855998}\n",
      "I0727 13:33:09.381642 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.2684307,\n",
      " 'Loss/localization_loss': 0.0041333158,\n",
      " 'Loss/regularization_loss': 0.030156188,\n",
      " 'Loss/total_loss': 0.30272022,\n",
      " 'learning_rate': 0.017855998}\n",
      "INFO:tensorflow:Step 2200 per-step time 0.794s\n",
      "I0727 13:34:28.807924 18776 model_lib_v2.py:705] Step 2200 per-step time 0.794s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.4116957,\n",
      " 'Loss/localization_loss': 0.008569822,\n",
      " 'Loss/regularization_loss': 0.03020266,\n",
      " 'Loss/total_loss': 0.45046818,\n",
      " 'learning_rate': 0.018392}\n",
      "I0727 13:34:28.807924 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.4116957,\n",
      " 'Loss/localization_loss': 0.008569822,\n",
      " 'Loss/regularization_loss': 0.03020266,\n",
      " 'Loss/total_loss': 0.45046818,\n",
      " 'learning_rate': 0.018392}\n",
      "INFO:tensorflow:Step 2300 per-step time 0.803s\n",
      "I0727 13:35:49.139801 18776 model_lib_v2.py:705] Step 2300 per-step time 0.803s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.28659916,\n",
      " 'Loss/localization_loss': 0.002915258,\n",
      " 'Loss/regularization_loss': 0.030240284,\n",
      " 'Loss/total_loss': 0.31975472,\n",
      " 'learning_rate': 0.018927999}\n",
      "I0727 13:35:49.139801 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.28659916,\n",
      " 'Loss/localization_loss': 0.002915258,\n",
      " 'Loss/regularization_loss': 0.030240284,\n",
      " 'Loss/total_loss': 0.31975472,\n",
      " 'learning_rate': 0.018927999}\n",
      "INFO:tensorflow:Step 2400 per-step time 0.804s\n",
      "I0727 13:37:09.487167 18776 model_lib_v2.py:705] Step 2400 per-step time 0.804s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.37056783,\n",
      " 'Loss/localization_loss': 0.0031765855,\n",
      " 'Loss/regularization_loss': 0.030347286,\n",
      " 'Loss/total_loss': 0.40409172,\n",
      " 'learning_rate': 0.019464}\n",
      "I0727 13:37:09.487167 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.37056783,\n",
      " 'Loss/localization_loss': 0.0031765855,\n",
      " 'Loss/regularization_loss': 0.030347286,\n",
      " 'Loss/total_loss': 0.40409172,\n",
      " 'learning_rate': 0.019464}\n",
      "INFO:tensorflow:Step 2500 per-step time 0.806s\n",
      "I0727 13:38:30.135356 18776 model_lib_v2.py:705] Step 2500 per-step time 0.806s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.34730706,\n",
      " 'Loss/localization_loss': 0.002481241,\n",
      " 'Loss/regularization_loss': 0.030378845,\n",
      " 'Loss/total_loss': 0.38016716,\n",
      " 'learning_rate': 0.02}\n",
      "I0727 13:38:30.136355 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.34730706,\n",
      " 'Loss/localization_loss': 0.002481241,\n",
      " 'Loss/regularization_loss': 0.030378845,\n",
      " 'Loss/total_loss': 0.38016716,\n",
      " 'learning_rate': 0.02}\n",
      "INFO:tensorflow:Step 2600 per-step time 0.771s\n",
      "I0727 13:39:47.284457 18776 model_lib_v2.py:705] Step 2600 per-step time 0.771s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.40109426,\n",
      " 'Loss/localization_loss': 0.0068835597,\n",
      " 'Loss/regularization_loss': 0.030391168,\n",
      " 'Loss/total_loss': 0.43836898,\n",
      " 'learning_rate': 0.019999994}\n",
      "I0727 13:39:47.284457 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.40109426,\n",
      " 'Loss/localization_loss': 0.0068835597,\n",
      " 'Loss/regularization_loss': 0.030391168,\n",
      " 'Loss/total_loss': 0.43836898,\n",
      " 'learning_rate': 0.019999994}\n",
      "INFO:tensorflow:Step 2700 per-step time 0.801s\n",
      "I0727 13:41:07.294113 18776 model_lib_v2.py:705] Step 2700 per-step time 0.801s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.24732363,\n",
      " 'Loss/localization_loss': 0.0023542608,\n",
      " 'Loss/regularization_loss': 0.030401051,\n",
      " 'Loss/total_loss': 0.28007895,\n",
      " 'learning_rate': 0.019999977}\n",
      "I0727 13:41:07.294113 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.24732363,\n",
      " 'Loss/localization_loss': 0.0023542608,\n",
      " 'Loss/regularization_loss': 0.030401051,\n",
      " 'Loss/total_loss': 0.28007895,\n",
      " 'learning_rate': 0.019999977}\n",
      "INFO:tensorflow:Step 2800 per-step time 0.788s\n",
      "I0727 13:42:26.140614 18776 model_lib_v2.py:705] Step 2800 per-step time 0.788s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.25338972,\n",
      " 'Loss/localization_loss': 0.0028222587,\n",
      " 'Loss/regularization_loss': 0.030432565,\n",
      " 'Loss/total_loss': 0.28664452,\n",
      " 'learning_rate': 0.01999995}\n",
      "I0727 13:42:26.140614 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.25338972,\n",
      " 'Loss/localization_loss': 0.0028222587,\n",
      " 'Loss/regularization_loss': 0.030432565,\n",
      " 'Loss/total_loss': 0.28664452,\n",
      " 'learning_rate': 0.01999995}\n",
      "INFO:tensorflow:Step 2900 per-step time 0.766s\n",
      "I0727 13:43:42.802587 18776 model_lib_v2.py:705] Step 2900 per-step time 0.766s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2967389,\n",
      " 'Loss/localization_loss': 0.0049946834,\n",
      " 'Loss/regularization_loss': 0.03047901,\n",
      " 'Loss/total_loss': 0.3322126,\n",
      " 'learning_rate': 0.01999991}\n",
      "I0727 13:43:42.803589 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.2967389,\n",
      " 'Loss/localization_loss': 0.0049946834,\n",
      " 'Loss/regularization_loss': 0.03047901,\n",
      " 'Loss/total_loss': 0.3322126,\n",
      " 'learning_rate': 0.01999991}\n",
      "INFO:tensorflow:Step 3000 per-step time 0.755s\n",
      "I0727 13:44:58.205913 18776 model_lib_v2.py:705] Step 3000 per-step time 0.755s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.3530222,\n",
      " 'Loss/localization_loss': 0.0031169648,\n",
      " 'Loss/regularization_loss': 0.030565387,\n",
      " 'Loss/total_loss': 0.38670453,\n",
      " 'learning_rate': 0.01999986}\n",
      "I0727 13:44:58.205913 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.3530222,\n",
      " 'Loss/localization_loss': 0.0031169648,\n",
      " 'Loss/regularization_loss': 0.030565387,\n",
      " 'Loss/total_loss': 0.38670453,\n",
      " 'learning_rate': 0.01999986}\n",
      "INFO:tensorflow:Step 3100 per-step time 0.773s\n",
      "I0727 13:46:15.592850 18776 model_lib_v2.py:705] Step 3100 per-step time 0.773s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.28356236,\n",
      " 'Loss/localization_loss': 0.0022335027,\n",
      " 'Loss/regularization_loss': 0.030604389,\n",
      " 'Loss/total_loss': 0.31640026,\n",
      " 'learning_rate': 0.019999798}\n",
      "I0727 13:46:15.592850 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.28356236,\n",
      " 'Loss/localization_loss': 0.0022335027,\n",
      " 'Loss/regularization_loss': 0.030604389,\n",
      " 'Loss/total_loss': 0.31640026,\n",
      " 'learning_rate': 0.019999798}\n",
      "INFO:tensorflow:Step 3200 per-step time 0.755s\n",
      "I0727 13:47:31.065992 18776 model_lib_v2.py:705] Step 3200 per-step time 0.755s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.25974476,\n",
      " 'Loss/localization_loss': 0.0038079028,\n",
      " 'Loss/regularization_loss': 0.03064323,\n",
      " 'Loss/total_loss': 0.2941959,\n",
      " 'learning_rate': 0.019999726}\n",
      "I0727 13:47:31.065992 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.25974476,\n",
      " 'Loss/localization_loss': 0.0038079028,\n",
      " 'Loss/regularization_loss': 0.03064323,\n",
      " 'Loss/total_loss': 0.2941959,\n",
      " 'learning_rate': 0.019999726}\n",
      "INFO:tensorflow:Step 3300 per-step time 0.753s\n",
      "I0727 13:48:46.350871 18776 model_lib_v2.py:705] Step 3300 per-step time 0.753s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.3957628,\n",
      " 'Loss/localization_loss': 0.0031720074,\n",
      " 'Loss/regularization_loss': 0.030681524,\n",
      " 'Loss/total_loss': 0.42961633,\n",
      " 'learning_rate': 0.019999642}\n",
      "I0727 13:48:46.350871 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.3957628,\n",
      " 'Loss/localization_loss': 0.0031720074,\n",
      " 'Loss/regularization_loss': 0.030681524,\n",
      " 'Loss/total_loss': 0.42961633,\n",
      " 'learning_rate': 0.019999642}\n",
      "INFO:tensorflow:Step 3400 per-step time 0.758s\n",
      "I0727 13:50:02.266625 18776 model_lib_v2.py:705] Step 3400 per-step time 0.758s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.21285297,\n",
      " 'Loss/localization_loss': 0.0025958656,\n",
      " 'Loss/regularization_loss': 0.030743083,\n",
      " 'Loss/total_loss': 0.24619192,\n",
      " 'learning_rate': 0.019999547}\n",
      "I0727 13:50:02.266625 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.21285297,\n",
      " 'Loss/localization_loss': 0.0025958656,\n",
      " 'Loss/regularization_loss': 0.030743083,\n",
      " 'Loss/total_loss': 0.24619192,\n",
      " 'learning_rate': 0.019999547}\n",
      "INFO:tensorflow:Step 3500 per-step time 0.759s\n",
      "I0727 13:51:18.153470 18776 model_lib_v2.py:705] Step 3500 per-step time 0.759s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.31638935,\n",
      " 'Loss/localization_loss': 0.004171911,\n",
      " 'Loss/regularization_loss': 0.030817227,\n",
      " 'Loss/total_loss': 0.3513785,\n",
      " 'learning_rate': 0.01999944}\n",
      "I0727 13:51:18.153470 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.31638935,\n",
      " 'Loss/localization_loss': 0.004171911,\n",
      " 'Loss/regularization_loss': 0.030817227,\n",
      " 'Loss/total_loss': 0.3513785,\n",
      " 'learning_rate': 0.01999944}\n",
      "INFO:tensorflow:Step 3600 per-step time 0.761s\n",
      "I0727 13:52:34.186694 18776 model_lib_v2.py:705] Step 3600 per-step time 0.761s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.42464253,\n",
      " 'Loss/localization_loss': 0.00300098,\n",
      " 'Loss/regularization_loss': 0.030890368,\n",
      " 'Loss/total_loss': 0.45853388,\n",
      " 'learning_rate': 0.019999325}\n",
      "I0727 13:52:34.186694 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.42464253,\n",
      " 'Loss/localization_loss': 0.00300098,\n",
      " 'Loss/regularization_loss': 0.030890368,\n",
      " 'Loss/total_loss': 0.45853388,\n",
      " 'learning_rate': 0.019999325}\n",
      "INFO:tensorflow:Step 3700 per-step time 0.755s\n",
      "I0727 13:53:49.759813 18776 model_lib_v2.py:705] Step 3700 per-step time 0.755s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.30270606,\n",
      " 'Loss/localization_loss': 0.0028316714,\n",
      " 'Loss/regularization_loss': 0.03094616,\n",
      " 'Loss/total_loss': 0.3364839,\n",
      " 'learning_rate': 0.019999197}\n",
      "I0727 13:53:49.759813 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.30270606,\n",
      " 'Loss/localization_loss': 0.0028316714,\n",
      " 'Loss/regularization_loss': 0.03094616,\n",
      " 'Loss/total_loss': 0.3364839,\n",
      " 'learning_rate': 0.019999197}\n",
      "INFO:tensorflow:Step 3800 per-step time 0.754s\n",
      "I0727 13:55:05.114019 18776 model_lib_v2.py:705] Step 3800 per-step time 0.754s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.17464219,\n",
      " 'Loss/localization_loss': 0.003075483,\n",
      " 'Loss/regularization_loss': 0.030976426,\n",
      " 'Loss/total_loss': 0.2086941,\n",
      " 'learning_rate': 0.019999057}\n",
      "I0727 13:55:05.114019 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.17464219,\n",
      " 'Loss/localization_loss': 0.003075483,\n",
      " 'Loss/regularization_loss': 0.030976426,\n",
      " 'Loss/total_loss': 0.2086941,\n",
      " 'learning_rate': 0.019999057}\n",
      "INFO:tensorflow:Step 3900 per-step time 0.755s\n",
      "I0727 13:56:20.533384 18776 model_lib_v2.py:705] Step 3900 per-step time 0.755s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.15326071,\n",
      " 'Loss/localization_loss': 0.0015192833,\n",
      " 'Loss/regularization_loss': 0.030997878,\n",
      " 'Loss/total_loss': 0.18577786,\n",
      " 'learning_rate': 0.019998908}\n",
      "I0727 13:56:20.533384 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.15326071,\n",
      " 'Loss/localization_loss': 0.0015192833,\n",
      " 'Loss/regularization_loss': 0.030997878,\n",
      " 'Loss/total_loss': 0.18577786,\n",
      " 'learning_rate': 0.019998908}\n",
      "INFO:tensorflow:Step 4000 per-step time 0.754s\n",
      "I0727 13:57:35.936998 18776 model_lib_v2.py:705] Step 4000 per-step time 0.754s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.16086303,\n",
      " 'Loss/localization_loss': 0.0014721428,\n",
      " 'Loss/regularization_loss': 0.031033006,\n",
      " 'Loss/total_loss': 0.19336818,\n",
      " 'learning_rate': 0.019998746}\n",
      "I0727 13:57:35.937970 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.16086303,\n",
      " 'Loss/localization_loss': 0.0014721428,\n",
      " 'Loss/regularization_loss': 0.031033006,\n",
      " 'Loss/total_loss': 0.19336818,\n",
      " 'learning_rate': 0.019998746}\n",
      "INFO:tensorflow:Step 4100 per-step time 0.782s\n",
      "I0727 13:58:54.167879 18776 model_lib_v2.py:705] Step 4100 per-step time 0.782s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.28854743,\n",
      " 'Loss/localization_loss': 0.0016933294,\n",
      " 'Loss/regularization_loss': 0.031061342,\n",
      " 'Loss/total_loss': 0.32130212,\n",
      " 'learning_rate': 0.01999857}\n",
      "I0727 13:58:54.167879 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.28854743,\n",
      " 'Loss/localization_loss': 0.0016933294,\n",
      " 'Loss/regularization_loss': 0.031061342,\n",
      " 'Loss/total_loss': 0.32130212,\n",
      " 'learning_rate': 0.01999857}\n",
      "INFO:tensorflow:Step 4200 per-step time 0.752s\n",
      "I0727 14:00:09.355245 18776 model_lib_v2.py:705] Step 4200 per-step time 0.752s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.15856133,\n",
      " 'Loss/localization_loss': 0.0023352334,\n",
      " 'Loss/regularization_loss': 0.03110527,\n",
      " 'Loss/total_loss': 0.19200183,\n",
      " 'learning_rate': 0.019998388}\n",
      "I0727 14:00:09.355245 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.15856133,\n",
      " 'Loss/localization_loss': 0.0023352334,\n",
      " 'Loss/regularization_loss': 0.03110527,\n",
      " 'Loss/total_loss': 0.19200183,\n",
      " 'learning_rate': 0.019998388}\n",
      "INFO:tensorflow:Step 4300 per-step time 0.752s\n",
      "I0727 14:01:24.640922 18776 model_lib_v2.py:705] Step 4300 per-step time 0.752s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1809985,\n",
      " 'Loss/localization_loss': 0.002197423,\n",
      " 'Loss/regularization_loss': 0.031123491,\n",
      " 'Loss/total_loss': 0.21431942,\n",
      " 'learning_rate': 0.019998193}\n",
      "I0727 14:01:24.640922 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.1809985,\n",
      " 'Loss/localization_loss': 0.002197423,\n",
      " 'Loss/regularization_loss': 0.031123491,\n",
      " 'Loss/total_loss': 0.21431942,\n",
      " 'learning_rate': 0.019998193}\n",
      "INFO:tensorflow:Step 4400 per-step time 0.753s\n",
      "I0727 14:02:39.981271 18776 model_lib_v2.py:705] Step 4400 per-step time 0.753s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13921961,\n",
      " 'Loss/localization_loss': 0.0019779624,\n",
      " 'Loss/regularization_loss': 0.031167291,\n",
      " 'Loss/total_loss': 0.17236488,\n",
      " 'learning_rate': 0.019997988}\n",
      "I0727 14:02:39.981271 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.13921961,\n",
      " 'Loss/localization_loss': 0.0019779624,\n",
      " 'Loss/regularization_loss': 0.031167291,\n",
      " 'Loss/total_loss': 0.17236488,\n",
      " 'learning_rate': 0.019997988}\n",
      "INFO:tensorflow:Step 4500 per-step time 0.753s\n",
      "I0727 14:03:55.257011 18776 model_lib_v2.py:705] Step 4500 per-step time 0.753s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2051894,\n",
      " 'Loss/localization_loss': 0.0014984782,\n",
      " 'Loss/regularization_loss': 0.031205205,\n",
      " 'Loss/total_loss': 0.23789309,\n",
      " 'learning_rate': 0.01999777}\n",
      "I0727 14:03:55.257011 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.2051894,\n",
      " 'Loss/localization_loss': 0.0014984782,\n",
      " 'Loss/regularization_loss': 0.031205205,\n",
      " 'Loss/total_loss': 0.23789309,\n",
      " 'learning_rate': 0.01999777}\n",
      "INFO:tensorflow:Step 4600 per-step time 0.751s\n",
      "I0727 14:05:10.422190 18776 model_lib_v2.py:705] Step 4600 per-step time 0.751s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.19451551,\n",
      " 'Loss/localization_loss': 0.0021736703,\n",
      " 'Loss/regularization_loss': 0.03127243,\n",
      " 'Loss/total_loss': 0.22796161,\n",
      " 'learning_rate': 0.01999754}\n",
      "I0727 14:05:10.422190 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.19451551,\n",
      " 'Loss/localization_loss': 0.0021736703,\n",
      " 'Loss/regularization_loss': 0.03127243,\n",
      " 'Loss/total_loss': 0.22796161,\n",
      " 'learning_rate': 0.01999754}\n",
      "INFO:tensorflow:Step 4700 per-step time 0.757s\n",
      "I0727 14:06:26.067925 18776 model_lib_v2.py:705] Step 4700 per-step time 0.757s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.25023982,\n",
      " 'Loss/localization_loss': 0.004268363,\n",
      " 'Loss/regularization_loss': 0.031313557,\n",
      " 'Loss/total_loss': 0.28582174,\n",
      " 'learning_rate': 0.0199973}\n",
      "I0727 14:06:26.067925 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.25023982,\n",
      " 'Loss/localization_loss': 0.004268363,\n",
      " 'Loss/regularization_loss': 0.031313557,\n",
      " 'Loss/total_loss': 0.28582174,\n",
      " 'learning_rate': 0.0199973}\n",
      "INFO:tensorflow:Step 4800 per-step time 0.789s\n",
      "I0727 14:07:44.891362 18776 model_lib_v2.py:705] Step 4800 per-step time 0.789s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.17582962,\n",
      " 'Loss/localization_loss': 0.0024367634,\n",
      " 'Loss/regularization_loss': 0.03136514,\n",
      " 'Loss/total_loss': 0.20963152,\n",
      " 'learning_rate': 0.019997051}\n",
      "I0727 14:07:44.891362 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.17582962,\n",
      " 'Loss/localization_loss': 0.0024367634,\n",
      " 'Loss/regularization_loss': 0.03136514,\n",
      " 'Loss/total_loss': 0.20963152,\n",
      " 'learning_rate': 0.019997051}\n",
      "INFO:tensorflow:Step 4900 per-step time 0.763s\n",
      "I0727 14:09:01.239603 18776 model_lib_v2.py:705] Step 4900 per-step time 0.763s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.20378047,\n",
      " 'Loss/localization_loss': 0.0026319234,\n",
      " 'Loss/regularization_loss': 0.03139847,\n",
      " 'Loss/total_loss': 0.23781087,\n",
      " 'learning_rate': 0.019996788}\n",
      "I0727 14:09:01.239603 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.20378047,\n",
      " 'Loss/localization_loss': 0.0026319234,\n",
      " 'Loss/regularization_loss': 0.03139847,\n",
      " 'Loss/total_loss': 0.23781087,\n",
      " 'learning_rate': 0.019996788}\n",
      "INFO:tensorflow:Step 5000 per-step time 0.763s\n",
      "I0727 14:10:17.536950 18776 model_lib_v2.py:705] Step 5000 per-step time 0.763s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1869286,\n",
      " 'Loss/localization_loss': 0.0023817935,\n",
      " 'Loss/regularization_loss': 0.03147677,\n",
      " 'Loss/total_loss': 0.22078715,\n",
      " 'learning_rate': 0.019996515}\n",
      "I0727 14:10:17.536950 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.1869286,\n",
      " 'Loss/localization_loss': 0.0023817935,\n",
      " 'Loss/regularization_loss': 0.03147677,\n",
      " 'Loss/total_loss': 0.22078715,\n",
      " 'learning_rate': 0.019996515}\n",
      "INFO:tensorflow:Step 5100 per-step time 0.812s\n",
      "I0727 14:11:38.674649 18776 model_lib_v2.py:705] Step 5100 per-step time 0.812s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.28613007,\n",
      " 'Loss/localization_loss': 0.0024001386,\n",
      " 'Loss/regularization_loss': 0.031522974,\n",
      " 'Loss/total_loss': 0.3200532,\n",
      " 'learning_rate': 0.01999623}\n",
      "I0727 14:11:38.674649 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.28613007,\n",
      " 'Loss/localization_loss': 0.0024001386,\n",
      " 'Loss/regularization_loss': 0.031522974,\n",
      " 'Loss/total_loss': 0.3200532,\n",
      " 'learning_rate': 0.01999623}\n",
      "INFO:tensorflow:Step 5200 per-step time 0.782s\n",
      "I0727 14:12:56.943348 18776 model_lib_v2.py:705] Step 5200 per-step time 0.782s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.16749538,\n",
      " 'Loss/localization_loss': 0.0016384672,\n",
      " 'Loss/regularization_loss': 0.031582985,\n",
      " 'Loss/total_loss': 0.20071684,\n",
      " 'learning_rate': 0.019995935}\n",
      "I0727 14:12:56.944851 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.16749538,\n",
      " 'Loss/localization_loss': 0.0016384672,\n",
      " 'Loss/regularization_loss': 0.031582985,\n",
      " 'Loss/total_loss': 0.20071684,\n",
      " 'learning_rate': 0.019995935}\n",
      "INFO:tensorflow:Step 5300 per-step time 0.765s\n",
      "I0727 14:14:13.457623 18776 model_lib_v2.py:705] Step 5300 per-step time 0.765s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.12852168,\n",
      " 'Loss/localization_loss': 0.0013976586,\n",
      " 'Loss/regularization_loss': 0.031602122,\n",
      " 'Loss/total_loss': 0.16152145,\n",
      " 'learning_rate': 0.01999563}\n",
      "I0727 14:14:13.458622 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.12852168,\n",
      " 'Loss/localization_loss': 0.0013976586,\n",
      " 'Loss/regularization_loss': 0.031602122,\n",
      " 'Loss/total_loss': 0.16152145,\n",
      " 'learning_rate': 0.01999563}\n",
      "INFO:tensorflow:Step 5400 per-step time 0.778s\n",
      "I0727 14:15:31.214439 18776 model_lib_v2.py:705] Step 5400 per-step time 0.778s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.14414714,\n",
      " 'Loss/localization_loss': 0.0017237171,\n",
      " 'Loss/regularization_loss': 0.03164245,\n",
      " 'Loss/total_loss': 0.17751332,\n",
      " 'learning_rate': 0.019995311}\n",
      "I0727 14:15:31.215435 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.14414714,\n",
      " 'Loss/localization_loss': 0.0017237171,\n",
      " 'Loss/regularization_loss': 0.03164245,\n",
      " 'Loss/total_loss': 0.17751332,\n",
      " 'learning_rate': 0.019995311}\n",
      "INFO:tensorflow:Step 5500 per-step time 0.764s\n",
      "I0727 14:16:47.672651 18776 model_lib_v2.py:705] Step 5500 per-step time 0.764s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.21462442,\n",
      " 'Loss/localization_loss': 0.0018857846,\n",
      " 'Loss/regularization_loss': 0.03168543,\n",
      " 'Loss/total_loss': 0.24819563,\n",
      " 'learning_rate': 0.019994982}\n",
      "I0727 14:16:47.673675 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.21462442,\n",
      " 'Loss/localization_loss': 0.0018857846,\n",
      " 'Loss/regularization_loss': 0.03168543,\n",
      " 'Loss/total_loss': 0.24819563,\n",
      " 'learning_rate': 0.019994982}\n",
      "INFO:tensorflow:Step 5600 per-step time 0.769s\n",
      "I0727 14:18:04.577386 18776 model_lib_v2.py:705] Step 5600 per-step time 0.769s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.18425968,\n",
      " 'Loss/localization_loss': 0.002631228,\n",
      " 'Loss/regularization_loss': 0.031708106,\n",
      " 'Loss/total_loss': 0.21859902,\n",
      " 'learning_rate': 0.019994643}\n",
      "I0727 14:18:04.578361 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.18425968,\n",
      " 'Loss/localization_loss': 0.002631228,\n",
      " 'Loss/regularization_loss': 0.031708106,\n",
      " 'Loss/total_loss': 0.21859902,\n",
      " 'learning_rate': 0.019994643}\n",
      "INFO:tensorflow:Step 5700 per-step time 0.802s\n",
      "I0727 14:19:24.730076 18776 model_lib_v2.py:705] Step 5700 per-step time 0.802s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.12780213,\n",
      " 'Loss/localization_loss': 0.0012313039,\n",
      " 'Loss/regularization_loss': 0.031756625,\n",
      " 'Loss/total_loss': 0.16079006,\n",
      " 'learning_rate': 0.01999429}\n",
      "I0727 14:19:24.730076 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.12780213,\n",
      " 'Loss/localization_loss': 0.0012313039,\n",
      " 'Loss/regularization_loss': 0.031756625,\n",
      " 'Loss/total_loss': 0.16079006,\n",
      " 'learning_rate': 0.01999429}\n",
      "INFO:tensorflow:Step 5800 per-step time 0.801s\n",
      "I0727 14:20:44.911307 18776 model_lib_v2.py:705] Step 5800 per-step time 0.801s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.23476402,\n",
      " 'Loss/localization_loss': 0.0013078941,\n",
      " 'Loss/regularization_loss': 0.031788092,\n",
      " 'Loss/total_loss': 0.26786003,\n",
      " 'learning_rate': 0.019993927}\n",
      "I0727 14:20:44.911307 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.23476402,\n",
      " 'Loss/localization_loss': 0.0013078941,\n",
      " 'Loss/regularization_loss': 0.031788092,\n",
      " 'Loss/total_loss': 0.26786003,\n",
      " 'learning_rate': 0.019993927}\n",
      "INFO:tensorflow:Step 5900 per-step time 0.798s\n",
      "I0727 14:22:04.721982 18776 model_lib_v2.py:705] Step 5900 per-step time 0.798s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.11558583,\n",
      " 'Loss/localization_loss': 0.0011774328,\n",
      " 'Loss/regularization_loss': 0.031837005,\n",
      " 'Loss/total_loss': 0.14860027,\n",
      " 'learning_rate': 0.019993555}\n",
      "I0727 14:22:04.721982 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.11558583,\n",
      " 'Loss/localization_loss': 0.0011774328,\n",
      " 'Loss/regularization_loss': 0.031837005,\n",
      " 'Loss/total_loss': 0.14860027,\n",
      " 'learning_rate': 0.019993555}\n",
      "INFO:tensorflow:Step 6000 per-step time 0.788s\n",
      "I0727 14:23:23.511405 18776 model_lib_v2.py:705] Step 6000 per-step time 0.788s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.21000384,\n",
      " 'Loss/localization_loss': 0.0014537078,\n",
      " 'Loss/regularization_loss': 0.031876937,\n",
      " 'Loss/total_loss': 0.24333449,\n",
      " 'learning_rate': 0.01999317}\n",
      "I0727 14:23:23.511405 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.21000384,\n",
      " 'Loss/localization_loss': 0.0014537078,\n",
      " 'Loss/regularization_loss': 0.031876937,\n",
      " 'Loss/total_loss': 0.24333449,\n",
      " 'learning_rate': 0.01999317}\n",
      "INFO:tensorflow:Step 6100 per-step time 0.809s\n",
      "I0727 14:24:44.420718 18776 model_lib_v2.py:705] Step 6100 per-step time 0.809s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1605772,\n",
      " 'Loss/localization_loss': 0.0017582073,\n",
      " 'Loss/regularization_loss': 0.031899832,\n",
      " 'Loss/total_loss': 0.19423522,\n",
      " 'learning_rate': 0.019992774}\n",
      "I0727 14:24:44.420718 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.1605772,\n",
      " 'Loss/localization_loss': 0.0017582073,\n",
      " 'Loss/regularization_loss': 0.031899832,\n",
      " 'Loss/total_loss': 0.19423522,\n",
      " 'learning_rate': 0.019992774}\n",
      "INFO:tensorflow:Step 6200 per-step time 0.815s\n",
      "I0727 14:26:05.941011 18776 model_lib_v2.py:705] Step 6200 per-step time 0.815s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.149812,\n",
      " 'Loss/localization_loss': 0.0018045389,\n",
      " 'Loss/regularization_loss': 0.03192395,\n",
      " 'Loss/total_loss': 0.1835405,\n",
      " 'learning_rate': 0.019992368}\n",
      "I0727 14:26:05.942011 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.149812,\n",
      " 'Loss/localization_loss': 0.0018045389,\n",
      " 'Loss/regularization_loss': 0.03192395,\n",
      " 'Loss/total_loss': 0.1835405,\n",
      " 'learning_rate': 0.019992368}\n",
      "INFO:tensorflow:Step 6300 per-step time 0.799s\n",
      "I0727 14:27:25.771340 18776 model_lib_v2.py:705] Step 6300 per-step time 0.799s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.26210675,\n",
      " 'Loss/localization_loss': 0.002514701,\n",
      " 'Loss/regularization_loss': 0.031953443,\n",
      " 'Loss/total_loss': 0.2965749,\n",
      " 'learning_rate': 0.01999195}\n",
      "I0727 14:27:25.771340 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.26210675,\n",
      " 'Loss/localization_loss': 0.002514701,\n",
      " 'Loss/regularization_loss': 0.031953443,\n",
      " 'Loss/total_loss': 0.2965749,\n",
      " 'learning_rate': 0.01999195}\n",
      "INFO:tensorflow:Step 6400 per-step time 0.805s\n",
      "I0727 14:28:46.276764 18776 model_lib_v2.py:705] Step 6400 per-step time 0.805s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.12737039,\n",
      " 'Loss/localization_loss': 0.0017566002,\n",
      " 'Loss/regularization_loss': 0.03195798,\n",
      " 'Loss/total_loss': 0.16108496,\n",
      " 'learning_rate': 0.01999152}\n",
      "I0727 14:28:46.276764 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.12737039,\n",
      " 'Loss/localization_loss': 0.0017566002,\n",
      " 'Loss/regularization_loss': 0.03195798,\n",
      " 'Loss/total_loss': 0.16108496,\n",
      " 'learning_rate': 0.01999152}\n",
      "INFO:tensorflow:Step 6500 per-step time 0.801s\n",
      "I0727 14:30:06.394900 18776 model_lib_v2.py:705] Step 6500 per-step time 0.801s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.20779821,\n",
      " 'Loss/localization_loss': 0.0015073671,\n",
      " 'Loss/regularization_loss': 0.03197735,\n",
      " 'Loss/total_loss': 0.24128294,\n",
      " 'learning_rate': 0.019991081}\n",
      "I0727 14:30:06.395879 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.20779821,\n",
      " 'Loss/localization_loss': 0.0015073671,\n",
      " 'Loss/regularization_loss': 0.03197735,\n",
      " 'Loss/total_loss': 0.24128294,\n",
      " 'learning_rate': 0.019991081}\n",
      "INFO:tensorflow:Step 6600 per-step time 0.803s\n",
      "I0727 14:31:26.590745 18776 model_lib_v2.py:705] Step 6600 per-step time 0.803s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13747077,\n",
      " 'Loss/localization_loss': 0.0014739659,\n",
      " 'Loss/regularization_loss': 0.032006714,\n",
      " 'Loss/total_loss': 0.17095144,\n",
      " 'learning_rate': 0.019990629}\n",
      "I0727 14:31:26.590745 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.13747077,\n",
      " 'Loss/localization_loss': 0.0014739659,\n",
      " 'Loss/regularization_loss': 0.032006714,\n",
      " 'Loss/total_loss': 0.17095144,\n",
      " 'learning_rate': 0.019990629}\n",
      "INFO:tensorflow:Step 6700 per-step time 0.799s\n",
      "I0727 14:32:46.521213 18776 model_lib_v2.py:705] Step 6700 per-step time 0.799s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13126405,\n",
      " 'Loss/localization_loss': 0.0012191393,\n",
      " 'Loss/regularization_loss': 0.03202615,\n",
      " 'Loss/total_loss': 0.16450934,\n",
      " 'learning_rate': 0.019990167}\n",
      "I0727 14:32:46.521213 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.13126405,\n",
      " 'Loss/localization_loss': 0.0012191393,\n",
      " 'Loss/regularization_loss': 0.03202615,\n",
      " 'Loss/total_loss': 0.16450934,\n",
      " 'learning_rate': 0.019990167}\n",
      "INFO:tensorflow:Step 6800 per-step time 0.805s\n",
      "I0727 14:34:07.047862 18776 model_lib_v2.py:705] Step 6800 per-step time 0.805s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.3360749,\n",
      " 'Loss/localization_loss': 0.0015494458,\n",
      " 'Loss/regularization_loss': 0.032038,\n",
      " 'Loss/total_loss': 0.36966234,\n",
      " 'learning_rate': 0.019989694}\n",
      "I0727 14:34:07.047862 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.3360749,\n",
      " 'Loss/localization_loss': 0.0015494458,\n",
      " 'Loss/regularization_loss': 0.032038,\n",
      " 'Loss/total_loss': 0.36966234,\n",
      " 'learning_rate': 0.019989694}\n",
      "INFO:tensorflow:Step 6900 per-step time 0.812s\n",
      "I0727 14:35:28.235132 18776 model_lib_v2.py:705] Step 6900 per-step time 0.812s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13550612,\n",
      " 'Loss/localization_loss': 0.0014298656,\n",
      " 'Loss/regularization_loss': 0.032071017,\n",
      " 'Loss/total_loss': 0.16900702,\n",
      " 'learning_rate': 0.019989206}\n",
      "I0727 14:35:28.236096 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.13550612,\n",
      " 'Loss/localization_loss': 0.0014298656,\n",
      " 'Loss/regularization_loss': 0.032071017,\n",
      " 'Loss/total_loss': 0.16900702,\n",
      " 'learning_rate': 0.019989206}\n",
      "INFO:tensorflow:Step 7000 per-step time 0.809s\n",
      "I0727 14:36:49.162291 18776 model_lib_v2.py:705] Step 7000 per-step time 0.809s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.29425815,\n",
      " 'Loss/localization_loss': 0.002455831,\n",
      " 'Loss/regularization_loss': 0.032112096,\n",
      " 'Loss/total_loss': 0.32882607,\n",
      " 'learning_rate': 0.01998871}\n",
      "I0727 14:36:49.163600 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.29425815,\n",
      " 'Loss/localization_loss': 0.002455831,\n",
      " 'Loss/regularization_loss': 0.032112096,\n",
      " 'Loss/total_loss': 0.32882607,\n",
      " 'learning_rate': 0.01998871}\n",
      "INFO:tensorflow:Step 7100 per-step time 0.826s\n",
      "I0727 14:38:11.749742 18776 model_lib_v2.py:705] Step 7100 per-step time 0.826s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.12865588,\n",
      " 'Loss/localization_loss': 0.0014178925,\n",
      " 'Loss/regularization_loss': 0.03214537,\n",
      " 'Loss/total_loss': 0.16221914,\n",
      " 'learning_rate': 0.019988203}\n",
      "I0727 14:38:11.750743 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.12865588,\n",
      " 'Loss/localization_loss': 0.0014178925,\n",
      " 'Loss/regularization_loss': 0.03214537,\n",
      " 'Loss/total_loss': 0.16221914,\n",
      " 'learning_rate': 0.019988203}\n",
      "INFO:tensorflow:Step 7200 per-step time 0.821s\n",
      "I0727 14:39:33.779561 18776 model_lib_v2.py:705] Step 7200 per-step time 0.821s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1402966,\n",
      " 'Loss/localization_loss': 0.001635405,\n",
      " 'Loss/regularization_loss': 0.032173276,\n",
      " 'Loss/total_loss': 0.17410527,\n",
      " 'learning_rate': 0.019987686}\n",
      "I0727 14:39:33.779561 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.1402966,\n",
      " 'Loss/localization_loss': 0.001635405,\n",
      " 'Loss/regularization_loss': 0.032173276,\n",
      " 'Loss/total_loss': 0.17410527,\n",
      " 'learning_rate': 0.019987686}\n",
      "INFO:tensorflow:Step 7300 per-step time 0.803s\n",
      "I0727 14:40:54.139675 18776 model_lib_v2.py:705] Step 7300 per-step time 0.803s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.14912859,\n",
      " 'Loss/localization_loss': 0.0024454722,\n",
      " 'Loss/regularization_loss': 0.032214213,\n",
      " 'Loss/total_loss': 0.18378827,\n",
      " 'learning_rate': 0.019987157}\n",
      "I0727 14:40:54.139675 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.14912859,\n",
      " 'Loss/localization_loss': 0.0024454722,\n",
      " 'Loss/regularization_loss': 0.032214213,\n",
      " 'Loss/total_loss': 0.18378827,\n",
      " 'learning_rate': 0.019987157}\n",
      "INFO:tensorflow:Step 7400 per-step time 0.793s\n",
      "I0727 14:42:13.412692 18776 model_lib_v2.py:705] Step 7400 per-step time 0.793s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08391522,\n",
      " 'Loss/localization_loss': 0.000922228,\n",
      " 'Loss/regularization_loss': 0.032238025,\n",
      " 'Loss/total_loss': 0.117075466,\n",
      " 'learning_rate': 0.019986615}\n",
      "I0727 14:42:13.412692 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.08391522,\n",
      " 'Loss/localization_loss': 0.000922228,\n",
      " 'Loss/regularization_loss': 0.032238025,\n",
      " 'Loss/total_loss': 0.117075466,\n",
      " 'learning_rate': 0.019986615}\n",
      "INFO:tensorflow:Step 7500 per-step time 0.788s\n",
      "I0727 14:43:32.216535 18776 model_lib_v2.py:705] Step 7500 per-step time 0.788s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.15027937,\n",
      " 'Loss/localization_loss': 0.0011175656,\n",
      " 'Loss/regularization_loss': 0.032239687,\n",
      " 'Loss/total_loss': 0.18363664,\n",
      " 'learning_rate': 0.019986063}\n",
      "I0727 14:43:32.216535 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.15027937,\n",
      " 'Loss/localization_loss': 0.0011175656,\n",
      " 'Loss/regularization_loss': 0.032239687,\n",
      " 'Loss/total_loss': 0.18363664,\n",
      " 'learning_rate': 0.019986063}\n",
      "INFO:tensorflow:Step 7600 per-step time 0.790s\n",
      "I0727 14:44:51.201020 18776 model_lib_v2.py:705] Step 7600 per-step time 0.790s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.17255944,\n",
      " 'Loss/localization_loss': 0.0019830922,\n",
      " 'Loss/regularization_loss': 0.032267988,\n",
      " 'Loss/total_loss': 0.20681052,\n",
      " 'learning_rate': 0.0199855}\n",
      "I0727 14:44:51.201020 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.17255944,\n",
      " 'Loss/localization_loss': 0.0019830922,\n",
      " 'Loss/regularization_loss': 0.032267988,\n",
      " 'Loss/total_loss': 0.20681052,\n",
      " 'learning_rate': 0.0199855}\n",
      "INFO:tensorflow:Step 7700 per-step time 0.789s\n",
      "I0727 14:46:10.148578 18776 model_lib_v2.py:705] Step 7700 per-step time 0.789s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09893577,\n",
      " 'Loss/localization_loss': 0.0012587212,\n",
      " 'Loss/regularization_loss': 0.032275405,\n",
      " 'Loss/total_loss': 0.13246989,\n",
      " 'learning_rate': 0.019984927}\n",
      "I0727 14:46:10.148578 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.09893577,\n",
      " 'Loss/localization_loss': 0.0012587212,\n",
      " 'Loss/regularization_loss': 0.032275405,\n",
      " 'Loss/total_loss': 0.13246989,\n",
      " 'learning_rate': 0.019984927}\n",
      "INFO:tensorflow:Step 7800 per-step time 0.792s\n",
      "I0727 14:47:29.341842 18776 model_lib_v2.py:705] Step 7800 per-step time 0.792s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.27383575,\n",
      " 'Loss/localization_loss': 0.0023109557,\n",
      " 'Loss/regularization_loss': 0.032290377,\n",
      " 'Loss/total_loss': 0.30843708,\n",
      " 'learning_rate': 0.019984342}\n",
      "I0727 14:47:29.341842 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.27383575,\n",
      " 'Loss/localization_loss': 0.0023109557,\n",
      " 'Loss/regularization_loss': 0.032290377,\n",
      " 'Loss/total_loss': 0.30843708,\n",
      " 'learning_rate': 0.019984342}\n",
      "INFO:tensorflow:Step 7900 per-step time 0.792s\n",
      "I0727 14:48:48.541624 18776 model_lib_v2.py:705] Step 7900 per-step time 0.792s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1336526,\n",
      " 'Loss/localization_loss': 0.0014748694,\n",
      " 'Loss/regularization_loss': 0.032307766,\n",
      " 'Loss/total_loss': 0.16743524,\n",
      " 'learning_rate': 0.019983746}\n",
      "I0727 14:48:48.542623 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.1336526,\n",
      " 'Loss/localization_loss': 0.0014748694,\n",
      " 'Loss/regularization_loss': 0.032307766,\n",
      " 'Loss/total_loss': 0.16743524,\n",
      " 'learning_rate': 0.019983746}\n",
      "INFO:tensorflow:Step 8000 per-step time 0.792s\n",
      "I0727 14:50:07.742264 18776 model_lib_v2.py:705] Step 8000 per-step time 0.792s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.17945923,\n",
      " 'Loss/localization_loss': 0.0010310753,\n",
      " 'Loss/regularization_loss': 0.032325875,\n",
      " 'Loss/total_loss': 0.21281618,\n",
      " 'learning_rate': 0.019983139}\n",
      "I0727 14:50:07.743780 18776 model_lib_v2.py:708] {'Loss/classification_loss': 0.17945923,\n",
      " 'Loss/localization_loss': 0.0010310753,\n",
      " 'Loss/regularization_loss': 0.032325875,\n",
      " 'Loss/total_loss': 0.21281618,\n",
      " 'learning_rate': 0.019983139}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py \\\n",
    "    --pipeline_config_path={config_path} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_workers=4 \\\n",
    "    --num_train_steps={NUM_STEPS} \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --num_eval_steps={NUM_EVAL_STEPS}\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "log_model_info(\n",
    "    batch=BATCH_SIZE,\n",
    "    classes=NUM_CLASSES,\n",
    "    steps=NUM_STEPS,\n",
    "    eval_steps=NUM_EVAL_STEPS,\n",
    "    name=MODEL_NAME,\n",
    "    bfloat16=use_bfloat16,\n",
    "    pipeline=base_config_path,\n",
    "    checkpoint_path=base_checkpoint_path,\n",
    "    labelmap=labelmap_path,\n",
    "    train_record=train_record_path,\n",
    "    test_record=test_record_path,\n",
    "    config=config_path,\n",
    "    trained_model=model_dir,\n",
    "    train_time=training_time,\n",
    "    df_test=df_final_test,\n",
    "    df_train=df_final_train,\n",
    "    notes=NOTES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"SSD_8000_640_mobilenet_v1_fpn\"\n",
    "config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\model_main_tf2.py \\\n",
    "            --pipeline_config_path={config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={model_dir}\\\n",
    "            --num_workers={6}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "output, error = process.communicate()\n",
    "\n",
    "if process.returncode != 0:\n",
    "    print(error.decode(\"utf-8\"))\n",
    "else: \n",
    "    output_str = output.decode(\"utf-8\")\n",
    "    header = f\"Evaluation Results for: {MODEL_NAME}\\n\\n\"\n",
    "    with open(f\"./myModules/log/{MODEL_NAME}_evaluation_results.txt\", \"w\") as f:\n",
    "        f.write(header)\n",
    "        f.write(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"SSD_8000_640_mobilenet_v1_fpn_custom\" # Bugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"SSD_8000_640_mobilenet_v1_fpn\",\n",
    "    \"SSD_8000_640_mobilenet_v2_fpnlite_custom\",\n",
    "    \"SSD_8000_640_resnet101_v1_fpn\",\n",
    "    \"SSD_16000_640_mobilenet_v2_fpnlite\"\n",
    "]\n",
    "\n",
    "# Pfade und andere Konfigurationsparameter\n",
    "config_template = './myModules/configs/{name}_config.config'\n",
    "model_dir_template = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'\n",
    "log_dir = './myModules/log'\n",
    "num_workers = 6\n",
    "\n",
    "# Stelle sicher, dass das Log-Verzeichnis existiert\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Evaluierungsfunktion\n",
    "def evaluate_model(model_name):\n",
    "    config_path = config_template.format(name=model_name)\n",
    "    model_dir = model_dir_template.format(name=model_name)\n",
    "    \n",
    "    command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\model_main_tf2.py \\\n",
    "                --pipeline_config_path={config_path} \\\n",
    "                --model_dir={model_dir} \\\n",
    "                --checkpoint_dir={model_dir} \\\n",
    "                --num_workers={num_workers}\"\n",
    "    \n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        error_message = error.decode(\"utf-8\")\n",
    "        print(f\"Error evaluating model {model_name}: {error_message}\")\n",
    "        return error_message\n",
    "    else:\n",
    "        output_str = output.decode(\"utf-8\")\n",
    "        header = f\"Evaluation Results for: {model_name}\\n\\n\"\n",
    "        result_file = os.path.join(log_dir, f\"{model_name}_evaluation_results.txt\")\n",
    "        with open(result_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            f.write(output_str)\n",
    "        print(f\"Evaluation results for {model_name} saved to {result_file}\")\n",
    "        return output_str\n",
    "\n",
    "# Hauptschleife zur Evaluierung der Modelle\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    evaluate_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_checkpoints(config_path, model_dir, checkpoints, model_name):\n",
    "    \"\"\"Evaluates all checkpoints and logs the results.\"\"\"\n",
    "    for checkpoint in checkpoints:\n",
    "        eval_command = f\"python C:/Users/Alexej/Desktop/GTSRB/models/research/object_detection/model_main_tf2.py \\\n",
    "            --pipeline_config_path={config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={checkpoint} \\\n",
    "            --run_once\"\n",
    "\n",
    "        # Starte den Evaluierungsprozess\n",
    "        process = subprocess.Popen(eval_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "        output, error = process.communicate()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f\"Error evaluating checkpoint {checkpoint}:\")\n",
    "            print(error.decode(\"utf-8\"))\n",
    "        else:\n",
    "            output_str = output.decode(\"utf-8\")\n",
    "            checkpoint_name = os.path.basename(checkpoint)\n",
    "            header = f\"Evaluation Results for MODEL_NAME: {model_name} at Checkpoint: {checkpoint_name}\\n\\n\"\n",
    "            with open(f\"./myModules/log/{model_name}_evaluation_results_{checkpoint_name}.txt\", \"w\") as f:\n",
    "                f.write(header)\n",
    "                f.write(output_str)\n",
    "            print(f\"Evaluation for checkpoint {checkpoint} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = get_checkpoints(model_dir)\n",
    "\n",
    "evaluate_checkpoints(config_path, model_dir, checkpoints, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_multiple_images(model, image, min_score_threshold):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Filter out detections with a score below the threshold\n",
    "    scores = output_dict['detection_scores']\n",
    "    high_score_indices = scores >= min_score_threshold\n",
    "\n",
    "    output_dict['detection_boxes'] = output_dict['detection_boxes'][high_score_indices]\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'][high_score_indices]\n",
    "    output_dict['detection_scores'] = output_dict['detection_scores'][high_score_indices]\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path, width, height):\n",
    "    image = Image.open(path)\n",
    "    # Resize the image to a larger size\n",
    "    image = image.resize((width, height))  # Resize to 256x256, you can change this as needed\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes_with_color(image, boxes, classes, scores, category_index, box_color, line_thickness):\n",
    "    for i in range(boxes.shape[0]):\n",
    "        if scores is None or scores[i] > 0.5:\n",
    "            class_id = int(classes[i])\n",
    "            if class_id in category_index.keys():\n",
    "                class_name = category_index[class_id]['name']\n",
    "                display_str = str(class_name)\n",
    "                color = box_color\n",
    "                vis_util.draw_bounding_box_on_image_array(\n",
    "                    image,\n",
    "                    boxes[i][0],\n",
    "                    boxes[i][1],\n",
    "                    boxes[i][2],\n",
    "                    boxes[i][3],\n",
    "                    color=color,\n",
    "                    thickness=line_thickness,\n",
    "                    display_str_list=[display_str],\n",
    "                    use_normalized_coordinates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_legend(image, groundtruth_classes, category_index, iou):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    image_height = image.size[1]\n",
    "    font_size = max(int(image_height * 0.04), 12)  # Schriftgröße auf 4% der Bildhöhe begrenzt, aber mindestens 12\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "\n",
    "    y_offset = int(0.9 * image_height)  # Startoffset für die Legende, z.B. 90% der Bildhöhe\n",
    "    x_offset = int(0.05 * image.size[0])  # Startoffset für die Legende, z.B. 5% der Bildbreite\n",
    "\n",
    "    # Ground Truth Legende\n",
    "    for gt_class in groundtruth_classes:\n",
    "        if gt_class in category_index:\n",
    "            class_name = category_index[gt_class]['name']\n",
    "            draw.text((x_offset, y_offset), f\"GT: {class_name}\\nIoU: {iou:.2f}\", fill=\"yellow\", font=font)\n",
    "            y_offset += int(font_size * 1.5)  # Erhöhe den Offset basierend auf der aktuellen Schriftgröße"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxA, boxB):\n",
    "    yA = max(boxA[0], boxB[0])\n",
    "    xA = max(boxA[1], boxB[1])\n",
    "    yB = min(boxA[2], boxB[2])\n",
    "    xB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    \n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    \n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    \n",
    "    image_pil = Image.fromarray(np.uint8(image_np)).convert(\"RGB\")\n",
    "    add_legend(image_pil, groundtruth_classes, category_index, iou)\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, ax):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    ax.imshow(image_np)\n",
    "    ax.axis('off')  # Hide the axis\n",
    "\n",
    "    # Adding legend to the plot\n",
    "    label_text = category_index[groundtruth_classes[0]]['name']\n",
    "    ax.set_title(f'{label_text}\\nIoU: {iou:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_images_for_labels(df, model, category_index, discard_predictions_below_acc, num_images_to_display):\n",
    "    sorted_labels = sorted(df['Label'].unique())\n",
    "\n",
    "    num_rows = len(sorted_labels)\n",
    "    num_cols = num_images_to_display\n",
    "\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols + num_cols * 0.5, num_rows * 2))\n",
    "    \n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, num_cols)\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for idx, label in enumerate(sorted_labels):\n",
    "        label_df = df[df['Label'] == label]\n",
    "        \n",
    "        random_indices = random.sample(range(len(label_df)), min(num_images_to_display, len(label_df)))\n",
    "\n",
    "        for jdx, r_idx in enumerate(random_indices):\n",
    "            row = label_df.iloc[r_idx]\n",
    "            image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "            groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "            groundtruth_classes = [row['Label']]\n",
    "            \n",
    "            output_dict = run_inference_for_multiple_images(model, image_np, discard_predictions_below_acc)\n",
    "            \n",
    "            plot_idx = idx * num_images_to_display + jdx\n",
    "            \n",
    "            visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, axes[plot_idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_predictions_with_groundtruth (model, df, num_images_to_display, category_index):\n",
    "    \n",
    "    random_indices = random.sample(range(len(df)), num_images_to_display)\n",
    "\n",
    "    # Visualisiere zufällige ausgewählte Bilder\n",
    "    for idx in random_indices:\n",
    "        row = df.iloc[idx]\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_endless_predictions(model, df, category_index):\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_endless_predictions_from_path(model, path, category_index):\n",
    "    \n",
    "    for image_path in glob.glob(path):\n",
    "        image_np = load_image_into_numpy_array(image_path, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=1)\n",
    "        display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\exporter_main_v2.py \\\n",
    "    --trained_checkpoint_dir {model_dir} \\\n",
    "    --inference_path {inference_path} \\\n",
    "    --config_path {config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load(f'./{inference_path}/saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inference Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = './GTSRB/Final_Test/Images/*.ppm'\n",
    "min_score_threshold = 0.6\n",
    "number_of_images_to_display = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_endless_predictions_from_path(model, image_path, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_endless_predictions(model, df_final_test, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_predictions_with_groundtruth(model, df_final_test, number_of_images_to_display, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_images_for_labels(df_final_test, model, category_index, min_score_threshold, number_of_images_to_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation (discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_labels = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "for idx, row in df_test_raw.iterrows():\n",
    "    row = df_test_raw.iloc[idx]\n",
    "    image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "    groundtruth_classes = [row['Label']]\n",
    "    \n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "    \n",
    "    predicted_class = output_dict['detection_classes'][0]\n",
    "    all_predicted_labels.append(predicted_class)\n",
    "    all_true_labels.append(groundtruth_classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
    "recall = recall_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "precision = precision_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "f1 = f1_score(all_true_labels, all_predicted_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Vorhersage')\n",
    "plt.ylabel('Wahre Werte')\n",
    "plt.title('Konfusionsmatrix')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_true_labels, all_predicted_labels, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-Time detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(TRAINED_CHECKPOINT_PATH, 'ckpt-8')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load('./inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Laden des Modells\n",
    "def load_model(model_path):\n",
    "    saved_model = tf.saved_model.load(model_path)\n",
    "    return saved_model\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "model_path = './inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model'\n",
    "detection_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Erstellen des Detektionsmodells und Wiederherstellen des Checkpoints\n",
    "def load_model():\n",
    "    configs = tf.compat.v2.saved_model.load('./inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model')\n",
    "    return configs['model']\n",
    "\n",
    "detection_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=10,\n",
    "                min_score_thresh=.9,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 800)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video capture testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum Eingabevideo und zum Ausgabevideo\n",
    "input_video_path = 'path_to_your_video.mp4'\n",
    "output_video_path = 'path_to_output_video.mp4'\n",
    "\n",
    "# VideoCapture-Objekt erstellen\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Überprüfen, ob das Video geöffnet werden kann\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Video-Eigenschaften abrufen\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# VideoWriter-Objekt erstellen\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Beenden der Schleife, wenn das Video zu Ende ist\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes sollten ints sein.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=10,\n",
    "                min_score_thresh=.9,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    # Frame mit Detektionen in das Ausgabevideo schreiben\n",
    "    out.write(image_np_with_detections)\n",
    "\n",
    "    # Optional: Zeige das Video mit Detektionen in einem Fenster an\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 800)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Ressourcen freigeben\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video capture testing end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(frame, axis=0), dtype=tf.float32)\n",
    "    \n",
    "    # Durchführung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'],\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=100,  # Anzahl der maximal zu zeichnenden Boxen\n",
    "        min_score_thresh=0.5    # Minimale Vertrauensschwelle für die Anzeige\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    \n",
    "    # Stoppen der Erfassung bei Drücken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Laden und Konfigurieren des Modells\n",
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Wiederherstellen des Checkpoints\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(TRAINED_CHECKPOINT_PATH, 'ckpt-9')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)\n",
    "\n",
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Überprüfen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    \n",
    "    # Durchführung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.9,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', cv2.resize(image_np, (800, 800)))\n",
    "    \n",
    "    # Stoppen der Erfassung bei Drücken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Videoaufnahme von der Kamera starten (Kamera 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Überprüfen, ob die Kamera erfolgreich geöffnet wurde\n",
    "if not cap.isOpened():\n",
    "    print(\"Fehler beim Öffnen der Kamera\")\n",
    "    exit()\n",
    "\n",
    "# Unendlich Schleife, um Frames von der Kamera zu lesen und anzuzeigen\n",
    "while True:\n",
    "    # Lesen eines einzelnen Frames von der Kamera\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Überprüfen, ob das Frame erfolgreich gelesen wurde\n",
    "    if not ret:\n",
    "        print(\"Fehler beim Lesen des Frames\")\n",
    "        break\n",
    "    \n",
    "    # Anzeigen des Frames in einem Fenster\n",
    "    cv2.imshow('Videoaufnahme', frame)\n",
    "    \n",
    "    # Beenden der Schleife bei Drücken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Freigeben der Videoquelle und Schließen aller Fenster\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_CONFIG_PATH = \"./myModules/configs/Faster_RCNN_640_50_fixed_config.config\"\n",
    "TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\Faster_RCNN_640_50_fixed'\n",
    "TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Wiederherstellen des Checkpoints\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(TRAINED_CHECKPOINT_PATH, 'ckpt-9')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    #input_tensor = input_tensor[tf.newaxis,...]\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference(frame):\n",
    "    \n",
    "    image_np = np.array(frame)\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes']+1,\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=10,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "           \n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    cv2.imshow('Object Detection', frame_with_detections)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zugriff auf die Kamera (0 für die Standardkamera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Variablen zur FPS-Berechnung\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "fps = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Führe die Objekterkennung durch\n",
    "    frame_with_detections = show_inference(frame)\n",
    "\n",
    "    # Berechne die FPS\n",
    "    frame_count += 1\n",
    "    if (time.time() - start_time) > 1:  # Ein Sekundenintervall\n",
    "        fps = frame_count / (time.time() - start_time)\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    # Zeige die FPS auf dem Frame\n",
    "    cv2.putText(frame_with_detections, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Zeige den Frame mit den Erkennungsergebnissen\n",
    "    cv2.imshow('Object Detection', frame_with_detections)\n",
    "\n",
    "    # Beenden durch Drücken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release der Kamera und Schließen aller Fenster\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kamera-Kalibrierungsparameter (müssen angepasst werden)\n",
    "f = 800  # Brennweite in Pixeln\n",
    "objekt_breite = 0.5  # tatsächliche Breite des Objekts in Metern\n",
    "\n",
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Überprüfen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "    # Durchführung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Entfernung berechnen und Ergebnisse anpassen\n",
    "    for i in range(num_detections):\n",
    "        box = detections['detection_boxes'][i]\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        box_width = xmax - xmin\n",
    "        box_height = ymax - ymin\n",
    "\n",
    "        # Berechne die Größe des Objekts in Pixeln\n",
    "        box_width_pixels = box_width * frame.shape[1]\n",
    "        box_height_pixels = box_height * frame.shape[0]\n",
    "\n",
    "        # Berechne die Entfernung zum Objekt (angenommene Größe des Objekts)\n",
    "        entfernung = (objekt_breite * f) / box_width_pixels\n",
    "\n",
    "        # Hier kannst du die Erkennungsergebnisse basierend auf der Entfernung anpassen\n",
    "        # Beispiel: Du kannst die Boxen oder Scores basierend auf der Entfernung anpassen\n",
    "\n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'] + 1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.3,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Stoppen der Erfassung bei Drücken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Definiere Skalierungsfaktoren\n",
    "scales = [0.25, 1.0, 1.5]\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Überprüfen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Initialisiere eine Liste, um alle Erkennungen zu speichern\n",
    "    all_detections = []\n",
    "\n",
    "    for scale in scales:\n",
    "        # Skaliere das Bild\n",
    "        height, width, _ = frame.shape\n",
    "        new_size = (int(width * scale), int(height * scale))\n",
    "        resized_frame = cv2.resize(frame, new_size)\n",
    "\n",
    "        # Konvertieren des Frames zu einem Tensor\n",
    "        image_np = np.array(resized_frame)\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "        # Durchführung der Objekterkennung\n",
    "        detections = detect_fn(input_tensor)\n",
    "\n",
    "        # Verarbeitung der erkannten Objekte\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "        detections['num_detections'] = num_detections\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        # Berechne die Skalierungskorrektur für die Box-Koordinaten\n",
    "        height_scale = height / new_size[1]\n",
    "        width_scale = width / new_size[0]\n",
    "        detections['detection_boxes'][:, [0, 2]] *= height_scale\n",
    "        detections['detection_boxes'][:, [1, 3]] *= width_scale\n",
    "\n",
    "        # Speichere die Ergebnisse\n",
    "        all_detections.append(detections)\n",
    "\n",
    "    # Kombiniere alle Erkennungen\n",
    "    # Du kannst hier die Detections nach deinem Bedarf kombinieren oder zusammenführen\n",
    "\n",
    "    # Verwende die Detections von der letzten Skala für die Visualisierung (oder alle)\n",
    "    final_detections = all_detections[-1]  # Hier nehmen wir die Erkennung von der letzten Skala\n",
    "\n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        final_detections['detection_boxes'],\n",
    "        final_detections['detection_classes'] + 1,\n",
    "        final_detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.3,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Stoppen der Erfassung bei Drücken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Videoaufnahme starten\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[0;32m      4\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Überprüfen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    \n",
    "    # Durchführung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.3,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', cv2.resize(image_np, (640, 640)))\n",
    "    \n",
    "    # Stoppen der Erfassung bei Drücken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Skalieren des Frames auf 640x640\n",
    "    frame_resized = cv2.resize(frame, (640, 640))\n",
    "    \n",
    "    # Überprüfen der Form des Bildes\n",
    "    if len(frame_resized.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame_resized = cv2.cvtColor(frame_resized, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame_resized)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    \n",
    "    # Durchführung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.9,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', cv2.resize(image_np, (800, 800)))\n",
    "    \n",
    "    # Stoppen der Erfassung bei Drücken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Skalieren des Frames auf die Eingabegröße des Modells (640x640)\n",
    "    frame_resized = cv2.resize(frame, (640, 640))\n",
    "    \n",
    "    # Überprüfen der Form des Bildes\n",
    "    if len(frame_resized.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame_resized = cv2.cvtColor(frame_resized, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame_resized)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    \n",
    "    # Durchführung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Skalieren der Bounding Boxes auf die Originalgröße des Frames\n",
    "    h, w, _ = frame.shape\n",
    "    scale_x, scale_y = w / 640, h / 640\n",
    "    \n",
    "    detections['detection_boxes'][:, 0] *= h  # ymin\n",
    "    detections['detection_boxes'][:, 1] *= w  # xmin\n",
    "    detections['detection_boxes'][:, 2] *= h  # ymax\n",
    "    detections['detection_boxes'][:, 3] *= w  # xmax\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=False,  # Normalisierte Koordinaten nicht mehr verwenden\n",
    "        max_boxes_to_draw=10,\n",
    "        min_score_thresh=0.9,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', cv2.resize(image_np, (800, 800)))\n",
    "    \n",
    "    # Stoppen der Erfassung bei Drücken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TFLite for Jetson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 14:57:40.540812: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 14:57:40.640632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "I0724 14:57:54.295555 21352 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 14:57:58.380026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-07-24 14:57:58.412291: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "I0724 14:57:59.958792 21352 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 14:58:01.014157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "I0724 14:58:03.325321 21352 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 14:58:04.338228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001F2214C5610>, because it is not built.\n",
      "W0724 14:58:04.795488 21352 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001F2214C5610>, because it is not built.\n",
      "W0724 14:58:36.381130 21352 save.py:269] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 329). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\assets\n",
      "I0724 14:58:48.019352 21352 builder_impl.py:779] Assets written to: D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\export_tflite_graph_tf2.py \\\n",
    "    --pipeline_config_path={config_path} \\\n",
    "    --trained_checkpoint_dir={TRAINED_CHECKPOINT_PATH} \\\n",
    "    --output_directory={model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZEN_TFLITE_PATH = os.path.join(model_dir, 'saved_model')\n",
    "TFLITE_MODEL = os.path.join(model_dir, 'saved_model', 'detect.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\ D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\detect.tflite\n"
     ]
    }
   ],
   "source": [
    "print(saved_model_dir, TFLITE_MODEL)\n",
    "D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}\\\\saved_model\\\\'.format(name=MODEL_NAME)\n",
    "tflite_model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}\\\\saved_model\\\\detect.tflite'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"tflite_convert \\\n",
    "--saved_model_dir={} \\\n",
    "--output_file={} \\\n",
    "--input_shapes=1,300,300,3 \\\n",
    "--input_arrays=normalized_input_image_tensor \\\n",
    "--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
    "--inference_type=FLOAT \\\n",
    "--allow_custom_ops\".format(FROZEN_TFLITE_PATH, TFLITE_MODEL, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_convert --saved_model_dir=Tensorflow\\workspace\\models\\my_ssd_mobnet\\tfliteexport\\saved_model --output_file=Tensorflow\\workspace\\models\\my_ssd_mobnet\\tfliteexport\\saved_model\\detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --allow_custom_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\Scripts\\tflite_convert.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 692, in main\n",
      "    app.run(main=run_main, argv=sys.argv[:1])\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\absl\\app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\absl\\app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 675, in run_main\n",
      "    _convert_tf2_model(tflite_flags)\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 276, in _convert_tf2_model\n",
      "    converter = lite.TFLiteConverterV2.from_saved_model(\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1789, in from_saved_model\n",
      "    saved_model = _load(saved_model_dir, tags)\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 800, in load\n",
      "    result = load_partial(export_dir, None, tags, options)[\"root\"]\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 905, in load_partial\n",
      "    loader_impl.parse_saved_model_with_debug_info(export_dir))\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 57, in parse_saved_model_with_debug_info\n",
      "    saved_model = parse_saved_model(export_dir)\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 115, in parse_saved_model\n",
      "    raise IOError(\n",
      "OSError: SavedModel file does not exist at: saved_model_dir\\{saved_model.pbtxt|saved_model.pb}\n"
     ]
    }
   ],
   "source": [
    "!tflite_convert --saved_model_dir=saved_model_dir --output_file=tflite_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 15:28:29.020734: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 15:28:29.135414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "I0724 15:28:35.269696  8296 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 15:28:39.181165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-07-24 15:28:39.216993: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "I0724 15:28:40.493435  8296 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 15:28:41.579417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "I0724 15:28:43.717420  8296 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 15:28:44.665618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001FE91CD5700>, because it is not built.\n",
      "W0724 15:28:45.100864  8296 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001FE91CD5700>, because it is not built.\n",
      "W0724 15:29:14.464308  8296 save.py:269] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 329). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: tf_lite\\saved_model\\assets\n",
      "I0724 15:29:24.435737  8296 builder_impl.py:779] Assets written to: tf_lite\\saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "output_directory = 'tf_lite'\n",
    "\n",
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\export_tflite_graph_tf2.py \\\n",
    "    --trained_checkpoint_dir={TRAINED_CHECKPOINT_PATH} \\\n",
    "    --output_directory={output_directory} \\\n",
    "    --pipeline_config_path={config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 219.390 G  ops, equivalently 109.695 G  MACs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 15:33:24.241856: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 15:33:24.298424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-07-24 15:33:41.946710: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2024-07-24 15:33:41.947009: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2024-07-24 15:33:41.947927: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\\n",
      "2024-07-24 15:33:42.072791: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2024-07-24 15:33:42.073026: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\\n",
      "2024-07-24 15:33:42.412829: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2024-07-24 15:33:42.473342: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2024-07-24 15:33:44.054861: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\\n",
      "2024-07-24 15:33:44.559123: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 2611192 microseconds.\n",
      "2024-07-24 15:33:45.703183: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-24 15:33:47.845816: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1989] Estimated count of arithmetic ops: 219.390 G  ops, equivalently 109.695 G  MACs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tflite_convert --saved_model_dir=D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\ --output_file=D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\model.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\detect.tflite D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\\n"
     ]
    }
   ],
   "source": [
    "print(tflite_model_dir, saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tflite_convert --saved_model_dir=saved_model_dir --output_file=tflite_model_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
