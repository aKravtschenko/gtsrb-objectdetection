{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import cv2 \n",
    "import subprocess\n",
    "import time\n",
    "import uuid\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as minidom\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was successfully built\n"
     ]
    }
   ],
   "source": [
    "base_config_path = './pipeline.config'\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(base_config_path)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "print(\"Model was successfully built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "#Checking for gpu\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "   raise SystemError('GPU device not found')\n",
    "\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(21)\n",
    "tf.compat.v1.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id_mapping:\n",
    "    # 1:'Speed limit (30km/h)', \n",
    "    # 2:'Speed limit (50km/h)',\n",
    "    # 12:'Priority road', \n",
    "    # 14:'Stop', \n",
    "    # 17:'No entry',\n",
    "    # 41:'Ende des Ãœberholverbots',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE_TRAIN = 50 # Default: 50 Images of each class excluding values in :id_mapping. Used for Trainset\n",
    "SAMPLE_SIZE_TEST = 10 # Default: 10 Images of each class excluding values in :id_mapping . Used for Testset\n",
    "\n",
    "IMAGE_WIDTH = 280 # Image width in pixels to resize for visualization\n",
    "IMAGE_HEIGHT = 280 # Image height in pixels to resize for visualization\n",
    "\n",
    "train_path = './GTSRB/Final_Training/Images' # Location of the training images\n",
    "test_path = './GTSRB/Final_Test/Images' # Location of the test images\n",
    "\n",
    "id_mapping = {1: 1, 2: 2, 12: 3, 14: 4, 17: 5}\n",
    "unknown_label = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\"\n",
    "MODEL_NAME = \"SSD_8000_640_mobilenet_v2_fpnlite\"\n",
    "IS_SSD = True # True if SSD model is used. False otherwise \n",
    "\n",
    "BATCH_SIZE = 12\n",
    "NUM_CLASSES = 5\n",
    "NUM_STEPS = 8000\n",
    "NUM_EVAL_STEPS = 1000\n",
    "use_bfloat16 = False # Use bfloat16 True for TPU\n",
    "\n",
    "NOTES = \"\" # Training Notes for the log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for Evaluation\n",
    "if IS_SSD:\n",
    "    TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=MODEL_NAME)\n",
    "    TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)\n",
    "    TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short.pbtxt\"\n",
    "else:\n",
    "    TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=MODEL_NAME)\n",
    "    TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "    TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for Training and Inference\n",
    "if IS_SSD:\n",
    "    # Download from path \n",
    "    base_config_path = \"./base_models/ssd/{name}/pipeline.config\".format(name=BASE_MODEL)\n",
    "    base_checkpoint_path = \"./base_models/ssd/{name}/checkpoint/ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "    # Save to path\n",
    "    model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)\n",
    "    config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "    # Upload from path\n",
    "    labelmap_path = \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    train_record_path = \"./myModules/records/trainWoUnknown.record\"\n",
    "    test_record_path = \"./myModules/records/testWoUnknown.record\"\n",
    "    \n",
    "    inference_path = './inference/ssd/{name}'.format(name=MODEL_NAME)\n",
    "else:\n",
    "    base_config_path = \"D:\\\\Desktop-Short\\\\base_models\\\\faster_rcnn\\\\{name}\\\\pipeline.config\".format(name=BASE_MODEL)\n",
    "    base_checkpoint_path = \"D:\\\\Desktop-Short\\\\base_models\\\\{name}\\\\checkpoint\\\\ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "    # Save to path\n",
    "    model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "    config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "    # Upload from path\n",
    "    labelmap_path = \"./myModules/label_map_short.pbtxt\"\n",
    "    train_record_path = \"./myModules/records/train.record\"\n",
    "    test_record_path = \"./myModules/records/test.record\"\n",
    "    \n",
    "    inference_path = './inference/faster_rcnn/{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTrain(rootpath, cache_path):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "    Arguments:\n",
    "        rootpath: path to the traffic sign data, for example './GTSRB/Training'\n",
    "        cache_path: path to the cached DataFrame file, default is 'traffic_signs_data.pkl'\n",
    "    Returns:\n",
    "        DataFrame containing labels, image shapes, ROIs, and image paths\n",
    "    '''\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading: {cache_path}\")\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    data = []\n",
    "    # loop over all 43 classes\n",
    "    for c in range(0, 43):\n",
    "        prefix = f\"{rootpath}/{c:05d}/\"  # subdirectory for class\n",
    "        gtFile = open(prefix + f'GT-{c:05d}.csv')  # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';')  # csv parser for annotations file\n",
    "        next(gtReader)  # skip header\n",
    "\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            img_path = prefix + row[0]\n",
    "            img = plt.imread(img_path)  # load image\n",
    " \n",
    "            label = int(row[7])  # the 8th column is the label\n",
    "            height = img.shape[0]  # height of the image\n",
    "            width = img.shape[1]  # width of the image\n",
    "            #channels = img.shape[2] if len(img.shape) > 2 else 1  # channels of the image (default to 1 if grayscale)\n",
    "            roi_x1 = int(row[3])  # ROI X1 coordinate\n",
    "            roi_y1 = int(row[4])  # ROI Y1 coordinate\n",
    "            roi_x2 = int(row[5])  # ROI X2 coordinate\n",
    "            roi_y2 = int(row[6])  # ROI Y2 coordinate\n",
    "\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "\n",
    "        gtFile.close()\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    \n",
    "    df.to_pickle(cache_path)\n",
    "    print(f\"Dataframe saved in: {cache_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTest(rootpath, cache_path):\n",
    "    '''Reads the final test data for the German Traffic Sign Recognition Benchmark.\n",
    "    Arguments: path to the final test data, for example './GTSRB/Final_Test/Images'\n",
    "    Returns: DataFrame with image data, labels, image shapes, and ROI\n",
    "    '''\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading: {cache_path}\")\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    # Pfad zur CSV-Datei\n",
    "    csv_file = os.path.join(rootpath, 'GT-final_test_gt.csv')\n",
    "    \n",
    "    # Lade die CSV-Datei\n",
    "    df = pd.read_csv(csv_file, delimiter=';')\n",
    "    \n",
    "    # Liste zum Speichern der Daten\n",
    "    data = []\n",
    "    \n",
    "    # Schleife Ã¼ber alle Zeilen der CSV-Datei\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.join(rootpath, row['Filename'])\n",
    "        img = plt.imread(img_path)  # Lade das Bild\n",
    "        \n",
    "        if img is not None:\n",
    "            label = int(row['ClassId'])\n",
    "            height, width, channels = img.shape\n",
    "            roi_x1 = int(row['Roi.X1'])\n",
    "            roi_y1 = int(row['Roi.Y1'])\n",
    "            roi_x2 = int(row['Roi.X2'])\n",
    "            roi_y2 = int(row['Roi.Y2'])\n",
    "            \n",
    "            # FÃ¼ge die Daten als Zeile hinzu\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "        else:\n",
    "            print(f\"Bild {img_path} konnte nicht geladen werden.\")\n",
    "    \n",
    "    # Erstelle ein DataFrame aus der Liste\n",
    "    df_test = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    df_test.to_pickle(cache_path)\n",
    "    print(f\"Dataframe saved in: {cache_path}\")\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))  # Resize to 280x280\n",
    "    return np.array(image).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_labels(df, selected_labels, unknown_label, sample_size):\n",
    "    \"\"\"\n",
    "    Reassign labels in the dataframe according to selected_labels.\n",
    "    Keep all rows for the labels in selected_labels.\n",
    "    For remaining labels, sample a specified number of rows and reassign them to unknown_label.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with a column named 'Label'.\n",
    "    selected_labels (dict): A dictionary mapping old labels to new labels.\n",
    "    unknown_label (int): The label to assign to the remaining sampled rows.\n",
    "    sample_size (int): The number of rows to sample for each remaining label. Default is 15.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new dataframe with reassigned labels.\n",
    "    \"\"\"\n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    for old_label, new_label in selected_labels.items():\n",
    "        label_df = df[df['Label'] == old_label].copy()\n",
    "        label_df['Label'] = new_label\n",
    "        new_df = pd.concat([new_df, label_df])\n",
    "\n",
    "    unique_labels = df['Label'].unique()\n",
    "    remaining_labels = [label for label in unique_labels if label not in selected_labels]\n",
    "\n",
    "    for label in remaining_labels:\n",
    "        label_df = df[df['Label'] == label]\n",
    "        if len(label_df) > sample_size:\n",
    "            selected_rows = label_df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            selected_rows = label_df\n",
    "        selected_rows['Label'] = unknown_label\n",
    "        new_df = pd.concat([new_df, selected_rows])\n",
    "\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Dataset for YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN_DF = \"./myModules/data/df_train_raw.pkl\"\n",
    "PATH_TO_TEST_DF = \"./myModules/data/df_test_raw.pkl\"\n",
    "\n",
    "PATH_TO_TRAIN_ANNOTATIONS = './yoloNoUnkData/Train/Annotations/'\n",
    "PATH_TO_TRAIN_IMAGES = './yoloNoUnkData/Train/Images/'\n",
    "\n",
    "PATH_TO_TEST_ANNOTATIONS = './yoloNoUnkData/Test/Annotations/'\n",
    "PATH_TO_TEST_IMAGES = './yoloNoUnkData/Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_xml(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ET.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voc_annotation_for_trainset(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        annotation = ET.Element('annotation')\n",
    "        \n",
    "        # Extract subfolder name (e.g., '0000')\n",
    "        subfolder_name = os.path.basename(os.path.dirname(row['Path']))\n",
    "        \n",
    "        # Create unique filename with subfolder prefix (e.g., '0000_00000_00000.xml')\n",
    "        os.path.basename(row['Path']).replace('.ppm', '.jpg')\n",
    "        filename = f\"{subfolder_name}_{os.path.splitext(os.path.basename(row['Path']))[0]}.xml\"\n",
    "        \n",
    "        # Create annotation elements\n",
    "        ET.SubElement(annotation, 'folder').text = 'Images'\n",
    "        ET.SubElement(annotation, 'filename').text = filename.replace('.xml', '.jpg')\n",
    "        \n",
    "        size = ET.SubElement(annotation, 'size')\n",
    "        ET.SubElement(size, 'width').text = str(row['Width'])\n",
    "        ET.SubElement(size, 'height').text = str(row['Height'])\n",
    "        ET.SubElement(size, 'depth').text = '3'  # Assuming RGB images\n",
    "\n",
    "        segmented = ET.SubElement(annotation, 'segmented').text = '0'\n",
    "        \n",
    "        obj = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(obj, 'name').text = str(row['Label'])\n",
    "        ET.SubElement(obj, 'pose').text = 'Unspecified'\n",
    "        ET.SubElement(obj, 'truncated').text = '0'\n",
    "        ET.SubElement(obj, 'difficult').text = '0'\n",
    "        \n",
    "        bndbox = ET.SubElement(obj, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(row['Roi.X1'])\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(row['Roi.Y1'])\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(row['Roi.X2'])\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(row['Roi.Y2'])\n",
    "        \n",
    "        # Write XML file with pretty print\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(prettify_xml(annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voc_annotation_for_testset(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        annotation = ET.Element('annotation')\n",
    "        ET.SubElement(annotation, 'folder').text = 'Images'\n",
    "        ET.SubElement(annotation, 'filename').text = os.path.basename(row['Path']).replace('.ppm', '.jpg')\n",
    "        \n",
    "        size = ET.SubElement(annotation, 'size')\n",
    "        ET.SubElement(size, 'width').text = str(row['Width'])\n",
    "        ET.SubElement(size, 'height').text = str(row['Height'])\n",
    "        ET.SubElement(size, 'depth').text = '3'  # Assuming RGB images\n",
    "\n",
    "        segmented = ET.SubElement(annotation, 'segmented').text = '0'\n",
    "        \n",
    "        obj = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(obj, 'name').text = str(row['Label'])\n",
    "        ET.SubElement(obj, 'pose').text = 'Unspecified'\n",
    "        ET.SubElement(obj, 'truncated').text = '0'\n",
    "        ET.SubElement(obj, 'difficult').text = '0'\n",
    "        \n",
    "        bndbox = ET.SubElement(obj, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(row['Roi.X1'])\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(row['Roi.Y1'])\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(row['Roi.X2'])\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(row['Roi.Y2'])\n",
    "        \n",
    "        # Write XML file with pretty print\n",
    "        output_file = os.path.join(output_dir, os.path.basename(row['Path']).replace('.ppm', '.xml'))\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(prettify_xml(annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ppm_to_jpg_from_df_train(df, jpg_root):\n",
    "    if not os.path.exists(jpg_root):\n",
    "        os.makedirs(jpg_root)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        img_path = row['Path']\n",
    "        if img_path.endswith('.ppm'):\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Extrahiere den Dateinamen und den Subordner aus dem Pfad\n",
    "            folder_name = os.path.basename(os.path.dirname(img_path))  # Subordner\n",
    "            filename = os.path.basename(img_path).replace('.ppm', '.jpg')\n",
    "            jpg_filename = f\"{folder_name}_{filename}\"  # FÃ¼ge den Subordner-Namen vor dem Dateinamen hinzu\n",
    "            \n",
    "            # Erstelle den Ziel-JPEG-Pfad\n",
    "            jpg_path = os.path.join(jpg_root, jpg_filename)\n",
    "            \n",
    "            # Speichere das Bild im JPEG-Format\n",
    "            img.save(jpg_path, 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ppm_to_jpg_from_df(df, jpg_dir):\n",
    "    if not os.path.exists(jpg_dir):\n",
    "        os.makedirs(jpg_dir)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        img_path = row['Path']\n",
    "        if img_path.endswith('.ppm'):\n",
    "            img = Image.open(img_path)\n",
    "            # Extrahiere den Dateinamen und den Unterordner aus dem Pfad\n",
    "            subfolder = os.path.basename(os.path.dirname(img_path))\n",
    "            filename = os.path.basename(img_path).replace('.ppm', '.jpg')\n",
    "            # Erstelle den Zielpfad inklusive Unterordner\n",
    "            jpg_subdir = os.path.join(jpg_dir, subfolder)\n",
    "            if not os.path.exists(jpg_subdir):\n",
    "                os.makedirs(jpg_subdir)\n",
    "            jpg_path = os.path.join(jpg_subdir, filename)\n",
    "            img.save(jpg_path, 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = readTrafficSignsTrain(train_path, PATH_TO_TRAIN_DF)\n",
    "create_voc_annotation_for_trainset(df_final_train, PATH_TO_TRAIN_ANNOTATIONS)\n",
    "convert_ppm_to_jpg_from_df_train(df_final_train, PATH_TO_TRAIN_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = readTrafficSignsTest(test_path, PATH_TO_TEST_DF)\n",
    "create_voc_annotation_for_testset(df_final_test, PATH_TO_TEST_ANNOTATIONS)\n",
    "convert_ppm_to_jpg_from_df(df_final_test, PATH_TO_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./myModules/data/df_train_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "df_train_cache = readTrafficSignsTrain(train_path, \"./myModules/data/df_train_raw.pkl\")\n",
    "df_train = pd.DataFrame()\n",
    "df_train = df_train_cache.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./myModules/data/df_test_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "df_test_cache = readTrafficSignsTest(test_path, \"./myModules/data/df_test_raw.pkl\")\n",
    "df_test = pd.DataFrame()\n",
    "df_test = df_test_cache.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_train = reassign_labels(df_train, id_mapping, unknown_label, SAMPLE_SIZE_TRAIN)\n",
    "df_final_test = reassign_labels(df_test, id_mapping, unknown_label, SAMPLE_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to exlude the unknown Labels\n",
    "df_final_train = df_final_train[df_final_train['Label'] != 6]\n",
    "df_final_test = df_final_test[df_final_test['Label'] != 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2790 entries, 0 to 2789\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Width   2790 non-null   int64 \n",
      " 1   Height  2790 non-null   int64 \n",
      " 2   Roi.X1  2790 non-null   int64 \n",
      " 3   Roi.Y1  2790 non-null   int64 \n",
      " 4   Roi.X2  2790 non-null   int64 \n",
      " 5   Roi.Y2  2790 non-null   int64 \n",
      " 6   Path    2790 non-null   object\n",
      " 7   Label   2790 non-null   int64 \n",
      "dtypes: int64(7), object(1)\n",
      "memory usage: 196.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8460 entries, 0 to 8459\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Width   8460 non-null   int64 \n",
      " 1   Height  8460 non-null   int64 \n",
      " 2   Roi.X1  8460 non-null   int64 \n",
      " 3   Roi.Y1  8460 non-null   int64 \n",
      " 4   Roi.X2  8460 non-null   int64 \n",
      " 5   Roi.Y2  8460 non-null   int64 \n",
      " 6   Path    8460 non-null   object\n",
      " 7   Label   8460 non-null   int64 \n",
      "dtypes: int64(7), object(1)\n",
      "memory usage: 594.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Verfify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "short_classes = { 1: 'Geschwindigkeitsbegrenzung (30km/h)', \n",
    "                  2: 'Geschwindigkeitsbegrenzung (50km/h)', \n",
    "                  3: 'VorrangstraÃŸe', \n",
    "                  4: 'Stop', \n",
    "                  5: 'Einfahrt verboten',\n",
    "                  6: 'Unbekannt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_train['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_test['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_label_df = df_final_train[df_final_train['Label'] == 6]\n",
    "\n",
    "# WÃ¤hle zufÃ¤llig 100 Bilder aus\n",
    "image_paths_to_display = unknown_label_df['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "# Plotten der ausgewÃ¤hlten Bilder\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_final = df_final_train['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_final)\n",
    "\n",
    "label_counts_named_final = {short_classes[key]: value for key, value in label_counts_final.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_final.keys()), y=list(label_counts_named_final.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen fÃ¼r bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_test = df_final_test['Label'].value_counts().sort_index()\n",
    "label_counts_train = df_final_train['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_train = {short_classes[key]: value for key, value in label_counts_train.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_train.keys()), y=list(label_counts_named_train.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen fÃ¼r bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_test = {short_classes[key]: value for key, value in label_counts_test.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_test.keys()), y=list(label_counts_named_test.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Testdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen fÃ¼r bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Tf-records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(row):\n",
    "    img_path = row['Path']\n",
    "    # Lade das Bild und konvertiere es in ein kompatibles Format (z.B. JPEG)\n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    with io.BytesIO() as output:\n",
    "        image.save(output, format=\"JPEG\")\n",
    "        encoded_jpg = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = row['Path'].encode('utf8')\n",
    "    image_format = b'jpeg'  # Ã„ndere dies entsprechend des konvertierten Bildformats\n",
    "    xmins = [row['Roi.X1'] / width]\n",
    "    xmaxs = [row['Roi.X2'] / width]\n",
    "    ymins = [row['Roi.Y1'] / height]\n",
    "    ymaxs = [row['Roi.Y2'] / height]\n",
    "    classes_text = [str(row['Label']).encode('utf8')]\n",
    "    classes = [int(row['Label'])]\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_jpg])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n",
    "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n",
    "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n",
    "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n",
    "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n",
    "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(df, output_path):\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    for _, row in df.iterrows():\n",
    "        tf_example = create_tf_example(row)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tf_record(df_final_train, './myModules/records/trainWoUnknown.record')\n",
    "create_tf_record(df_final_test, './myModules/records/testWoUnknown.record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Verify Tensord records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_example(serialized_example):\n",
    "    feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/class/text': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(record_file):\n",
    "    raw_dataset = tf.data.TFRecordDataset(record_file)\n",
    "    parsed_dataset = raw_dataset.map(parse_tf_example)\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_at_index(parsed_dataset, index):\n",
    "    for i, tf_example in enumerate(parsed_dataset):\n",
    "        if i == index:\n",
    "            height = tf_example['image/height'].numpy()\n",
    "            width = tf_example['image/width'].numpy()\n",
    "            encoded_image = tf_example['image/encoded'].numpy()\n",
    "            xmin = tf_example['image/object/bbox/xmin'].numpy()\n",
    "            xmax = tf_example['image/object/bbox/xmax'].numpy()\n",
    "            ymin = tf_example['image/object/bbox/ymin'].numpy()\n",
    "            ymax = tf_example['image/object/bbox/ymax'].numpy()\n",
    "            label = tf_example['image/object/class/label'].numpy()\n",
    "\n",
    "            image = tf.image.decode_jpeg(encoded_image)\n",
    "            image_np = image.numpy()\n",
    "\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(image_np)\n",
    "            plt.title(f'Label: {label}')\n",
    "\n",
    "            # Draw the bounding box\n",
    "            plt.gca().add_patch(plt.Rectangle((xmin * width, ymin * height), \n",
    "                                              (xmax - xmin) * width, (ymax - ymin) * height,\n",
    "                                              edgecolor='green', facecolor='none', linewidth=2))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record = read_tfrecord('./myModules/records/trainWoUnknown.record')\n",
    "test_record = read_tfrecord('./myModules/records/testWoUnknown.record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_at_index(test_record, 888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image_at_index(train_record, 8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_length(df):\n",
    "    count_images = len(df)\n",
    "    labels_count = df['Label'].value_counts()\n",
    "\n",
    "    sorted_labels_count = labels_count.sort_index()\n",
    "    return sorted_labels_count.tolist(), count_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_info(batch, classes, steps, eval_steps, name, bfloat16, pipeline, \n",
    "                   checkpoint_path, labelmap, train_record, test_record, config, \n",
    "                   trained_model, train_time, df_train, df_test, notes):\n",
    "    \n",
    "    \n",
    "    test_count, test_len = count_length(df_test)\n",
    "    train_count, train_len = count_length(df_train)   \n",
    "    log_contents = f\"\"\"\n",
    "Trainingparameters:\n",
    "    BATCH_SIZE = {batch} # Cannot be higher than 2-4 (lack of ressources)\n",
    "    NUM_CLASSES = {classes} # Total number of classes to train is 43 - used for training are only 6-7\n",
    "    NUM_STEPS = {steps}\n",
    "    NUM_EVAL_STEPS = {eval_steps}\n",
    "    MODEL_NAME = {name}\n",
    "    use_bfloat16 = {bfloat16} # Use bfloat16 = True for trainign with TPU\n",
    "    \n",
    "Locations: \n",
    "    Path to Pipeline-Config = {pipeline}\n",
    "    Checkpoint from transfermodel  = {checkpoint_path}\n",
    "\n",
    "    Path to the trainedmodel = {trained_model}\n",
    "    Config of the trainedmodel = {config}\n",
    "    \n",
    "Used records: \n",
    "    Used labelmap = {labelmap}\n",
    "    Used train_record = {train_record}\n",
    "    Used test_record = {test_record}\n",
    "\n",
    "Generell Information: \n",
    "    Time needed for modeltraining = {train_time}\n",
    "    Length of traindataset = {train_len}\n",
    "    Counted values for each Label form Traindataset = {train_count}\n",
    "    Length of testdataset = {test_len}\n",
    "    Counted values for each Label form Testdataset = {test_count}\n",
    "\"\"\"\n",
    "\n",
    "    log_id = uuid.uuid4()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_entry = f\"ID: {log_id}\\nTimestamp: {timestamp}\\n\\n{log_contents}\\nNotes: {notes}\\n\"\n",
    "\n",
    "    filename = f\"./myModules/log/{name}_{log_id}.txt\"\n",
    "    with open(filename, \"a\") as file:\n",
    "        file.write(log_entry)\n",
    "        file.write(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_config_path) as f:\n",
    "    config = f.read()\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "  \n",
    "  # Set labelmap path\n",
    "  config = re.sub('label_map_path: \".*?\"', \n",
    "             'label_map_path: \"{}\"'.format(labelmap_path), config)\n",
    "  \n",
    "  # Set fine_tune_checkpoint path\n",
    "  config = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                  'fine_tune_checkpoint: \"{}\"'.format(base_checkpoint_path), config)\n",
    "  \n",
    "  # Set train tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(train_record_path), config)\n",
    "  \n",
    "  # Set test tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(test_record_path), config)\n",
    "  \n",
    "  # Set number of classes.\n",
    "  config = re.sub('num_classes: [0-9]+',\n",
    "                  'num_classes: {}'.format(NUM_CLASSES), config)\n",
    "  \n",
    "  # Set batch size\n",
    "  config = re.sub('batch_size: [0-9]+',\n",
    "                  'batch_size: {}'.format(BATCH_SIZE), config)\n",
    "  \n",
    "  # Set training steps\n",
    "  config = re.sub('num_steps: [0-9]+',\n",
    "                  'num_steps: {}'.format(NUM_STEPS), config)\n",
    "  \n",
    "    # Set use_bfloat16\n",
    "  config = re.sub('use_bfloat16: (true|false)',\n",
    "                  'use_bfloat16: {}'.format(str(use_bfloat16).lower()), config)\n",
    "  \n",
    "  # Set fine-tune checkpoint type to detection\n",
    "  config = re.sub('fine_tune_checkpoint_type: \"classification\"', \n",
    "             'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n",
    "  \n",
    "  f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model {\n",
      "  ssd {\n",
      "    num_classes: 5\n",
      "    image_resizer {\n",
      "      fixed_shape_resizer {\n",
      "        height: 640\n",
      "        width: 640\n",
      "      }\n",
      "    }\n",
      "    feature_extractor {\n",
      "      type: \"ssd_mobilenet_v2_fpn_keras\"\n",
      "      depth_multiplier: 1.0\n",
      "      min_depth: 16\n",
      "      conv_hyperparams {\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 3.9999998989515007e-05\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          random_normal_initializer {\n",
      "            mean: 0.0\n",
      "            stddev: 0.009999999776482582\n",
      "          }\n",
      "        }\n",
      "        activation: RELU_6\n",
      "        batch_norm {\n",
      "          decay: 0.996999979019165\n",
      "          scale: true\n",
      "          epsilon: 0.0010000000474974513\n",
      "        }\n",
      "      }\n",
      "      use_depthwise: true\n",
      "      override_base_feature_extractor_hyperparams: true\n",
      "      fpn {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "        additional_layer_depth: 128\n",
      "      }\n",
      "    }\n",
      "    box_coder {\n",
      "      faster_rcnn_box_coder {\n",
      "        y_scale: 10.0\n",
      "        x_scale: 10.0\n",
      "        height_scale: 5.0\n",
      "        width_scale: 5.0\n",
      "      }\n",
      "    }\n",
      "    matcher {\n",
      "      argmax_matcher {\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.5\n",
      "        ignore_thresholds: false\n",
      "        negatives_lower_than_unmatched: true\n",
      "        force_match_for_each_row: true\n",
      "        use_matmul_gather: true\n",
      "      }\n",
      "    }\n",
      "    similarity_calculator {\n",
      "      iou_similarity {\n",
      "      }\n",
      "    }\n",
      "    box_predictor {\n",
      "      weight_shared_convolutional_box_predictor {\n",
      "        conv_hyperparams {\n",
      "          regularizer {\n",
      "            l2_regularizer {\n",
      "              weight: 3.9999998989515007e-05\n",
      "            }\n",
      "          }\n",
      "          initializer {\n",
      "            random_normal_initializer {\n",
      "              mean: 0.0\n",
      "              stddev: 0.009999999776482582\n",
      "            }\n",
      "          }\n",
      "          activation: RELU_6\n",
      "          batch_norm {\n",
      "            decay: 0.996999979019165\n",
      "            scale: true\n",
      "            epsilon: 0.0010000000474974513\n",
      "          }\n",
      "        }\n",
      "        depth: 128\n",
      "        num_layers_before_predictor: 4\n",
      "        kernel_size: 3\n",
      "        class_prediction_bias_init: -4.599999904632568\n",
      "        share_prediction_tower: true\n",
      "        use_depthwise: true\n",
      "      }\n",
      "    }\n",
      "    anchor_generator {\n",
      "      multiscale_anchor_generator {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "        anchor_scale: 4.0\n",
      "        aspect_ratios: 1.0\n",
      "        aspect_ratios: 2.0\n",
      "        aspect_ratios: 0.5\n",
      "        scales_per_octave: 2\n",
      "      }\n",
      "    }\n",
      "    post_processing {\n",
      "      batch_non_max_suppression {\n",
      "        score_threshold: 9.99999993922529e-09\n",
      "        iou_threshold: 0.6000000238418579\n",
      "        max_detections_per_class: 100\n",
      "        max_total_detections: 100\n",
      "        use_static_shapes: false\n",
      "      }\n",
      "      score_converter: SIGMOID\n",
      "    }\n",
      "    normalize_loss_by_num_matches: true\n",
      "    loss {\n",
      "      localization_loss {\n",
      "        weighted_smooth_l1 {\n",
      "        }\n",
      "      }\n",
      "      classification_loss {\n",
      "        weighted_sigmoid_focal {\n",
      "          gamma: 2.0\n",
      "          alpha: 0.25\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 1.0\n",
      "    }\n",
      "    encode_background_as_zeros: true\n",
      "    normalize_loc_loss_by_codesize: true\n",
      "    inplace_batchnorm_update: true\n",
      "    freeze_batchnorm: false\n",
      "  }\n",
      "}\n",
      "train_config {\n",
      "  batch_size: 12\n",
      "  data_augmentation_options {\n",
      "    random_horizontal_flip {\n",
      "    }\n",
      "  }\n",
      "  data_augmentation_options {\n",
      "    random_crop_image {\n",
      "      min_object_covered: 0.0\n",
      "      min_aspect_ratio: 0.75\n",
      "      max_aspect_ratio: 3.0\n",
      "      min_area: 0.75\n",
      "      max_area: 1.0\n",
      "      overlap_thresh: 0.0\n",
      "    }\n",
      "  }\n",
      "  sync_replicas: true\n",
      "  optimizer {\n",
      "    momentum_optimizer {\n",
      "      learning_rate {\n",
      "        cosine_decay_learning_rate {\n",
      "          learning_rate_base: 0.02\n",
      "          total_steps: 50000\n",
      "          warmup_learning_rate: 0.006666000485420227\n",
      "          warmup_steps: 1000\n",
      "        }\n",
      "      }\n",
      "      momentum_optimizer_value: 0.8999999761581421\n",
      "    }\n",
      "    use_moving_average: false\n",
      "  }\n",
      "  fine_tune_checkpoint: \"./base_models/ssd/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
      "  num_steps: 8000\n",
      "  startup_delay_steps: 0.0\n",
      "  replicas_to_aggregate: 8\n",
      "  max_number_of_boxes: 100\n",
      "  unpad_groundtruth_tensors: false\n",
      "  fine_tune_checkpoint_type: \"detection\"\n",
      "  fine_tune_checkpoint_version: V2\n",
      "}\n",
      "train_input_reader {\n",
      "  label_map_path: \"./myModules/label_map_short_woUnk.pbtxt\"\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/trainWoUnknown.record\"\n",
      "  }\n",
      "}\n",
      "eval_config {\n",
      "  metrics_set: \"coco_detection_metrics\"\n",
      "  use_moving_averages: false\n",
      "}\n",
      "eval_input_reader {\n",
      "  label_map_path: \"./myModules/label_map_short_woUnk.pbtxt\"\n",
      "  shuffle: false\n",
      "  num_epochs: 1\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/trainWoUnknown.record\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(config_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "WARNING:tensorflow:From C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py:100: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "W0727 21:54:20.522220 21236 deprecation.py:350] From C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py:100: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "2024-07-27 21:54:20.525064: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-27 21:54:20.598435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0',), communication = CommunicationImplementation.AUTO\n",
      "I0727 21:54:21.029438 21236 collective_all_reduce_strategy.py:446] Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0',), communication = CommunicationImplementation.AUTO\n",
      "INFO:tensorflow:Maybe overwriting train_steps: 8000\n",
      "I0727 21:54:21.034441 21236 config_util.py:552] Maybe overwriting train_steps: 8000\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0727 21:54:21.034441 21236 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W0727 21:54:21.071485 21236 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['./myModules/records/trainWoUnknown.record']\n",
      "I0727 21:54:21.087541 21236 dataset_builder.py:162] Reading unweighted datasets: ['./myModules/records/trainWoUnknown.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['./myModules/records/trainWoUnknown.record']\n",
      "I0727 21:54:21.088545 21236 dataset_builder.py:79] Reading record datasets for input file: ['./myModules/records/trainWoUnknown.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I0727 21:54:21.088545 21236 dataset_builder.py:80] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0727 21:54:21.088545 21236 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W0727 21:54:21.102537 21236 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0727 21:54:21.130542 21236 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W0727 21:54:27.826973 21236 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W0727 21:54:30.414403 21236 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0727 21:54:32.222920 21236 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n",
      "I0727 21:54:40.904922 17200 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0727 21:54:51.419914  8496 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-27 21:54:58.814065: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8907\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0727 21:55:03.034081 20580 deprecation.py:554] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "I0727 21:55:03.929023 20580 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0727 21:55:10.502773 18924 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0727 21:55:16.374821 12488 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0727 21:55:22.628941 19564 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-27 21:55:40.734157: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 21:55:40.748388: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.24GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 21:55:40.947186: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.36GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 21:55:40.954746: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 21:55:40.963687: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 21:55:40.970216: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 21:55:40.998447: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.28GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 21:55:41.095610: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.80GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-27 21:55:41.616328: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.57GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "INFO:tensorflow:Step 100 per-step time 1.235s\n",
      "I0727 21:57:06.263780 21236 model_lib_v2.py:705] Step 100 per-step time 1.235s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.79273754,\n",
      " 'Loss/localization_loss': 0.42055207,\n",
      " 'Loss/regularization_loss': 0.1514082,\n",
      " 'Loss/total_loss': 1.3646978,\n",
      " 'learning_rate': 0.007999401}\n",
      "I0727 21:57:06.263780 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.79273754,\n",
      " 'Loss/localization_loss': 0.42055207,\n",
      " 'Loss/regularization_loss': 0.1514082,\n",
      " 'Loss/total_loss': 1.3646978,\n",
      " 'learning_rate': 0.007999401}\n",
      "INFO:tensorflow:Step 200 per-step time 0.870s\n",
      "I0727 21:58:33.224002 21236 model_lib_v2.py:705] Step 200 per-step time 0.870s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.33836284,\n",
      " 'Loss/localization_loss': 0.12131888,\n",
      " 'Loss/regularization_loss': 0.15135317,\n",
      " 'Loss/total_loss': 0.61103487,\n",
      " 'learning_rate': 0.0093328}\n",
      "I0727 21:58:33.225008 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.33836284,\n",
      " 'Loss/localization_loss': 0.12131888,\n",
      " 'Loss/regularization_loss': 0.15135317,\n",
      " 'Loss/total_loss': 0.61103487,\n",
      " 'learning_rate': 0.0093328}\n",
      "INFO:tensorflow:Step 300 per-step time 0.863s\n",
      "I0727 21:59:59.454319 21236 model_lib_v2.py:705] Step 300 per-step time 0.863s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.31137496,\n",
      " 'Loss/localization_loss': 0.085200585,\n",
      " 'Loss/regularization_loss': 0.1512669,\n",
      " 'Loss/total_loss': 0.54784244,\n",
      " 'learning_rate': 0.0106662}\n",
      "I0727 21:59:59.454319 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.31137496,\n",
      " 'Loss/localization_loss': 0.085200585,\n",
      " 'Loss/regularization_loss': 0.1512669,\n",
      " 'Loss/total_loss': 0.54784244,\n",
      " 'learning_rate': 0.0106662}\n",
      "INFO:tensorflow:Step 400 per-step time 0.862s\n",
      "I0727 22:01:25.710414 21236 model_lib_v2.py:705] Step 400 per-step time 0.862s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2812184,\n",
      " 'Loss/localization_loss': 0.054858126,\n",
      " 'Loss/regularization_loss': 0.15117158,\n",
      " 'Loss/total_loss': 0.48724812,\n",
      " 'learning_rate': 0.0119996}\n",
      "I0727 22:01:25.710414 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.2812184,\n",
      " 'Loss/localization_loss': 0.054858126,\n",
      " 'Loss/regularization_loss': 0.15117158,\n",
      " 'Loss/total_loss': 0.48724812,\n",
      " 'learning_rate': 0.0119996}\n",
      "INFO:tensorflow:Step 500 per-step time 0.917s\n",
      "I0727 22:02:57.411242 21236 model_lib_v2.py:705] Step 500 per-step time 0.917s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.24351658,\n",
      " 'Loss/localization_loss': 0.0709576,\n",
      " 'Loss/regularization_loss': 0.1510622,\n",
      " 'Loss/total_loss': 0.4655364,\n",
      " 'learning_rate': 0.013333}\n",
      "I0727 22:02:57.411242 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.24351658,\n",
      " 'Loss/localization_loss': 0.0709576,\n",
      " 'Loss/regularization_loss': 0.1510622,\n",
      " 'Loss/total_loss': 0.4655364,\n",
      " 'learning_rate': 0.013333}\n",
      "INFO:tensorflow:Step 600 per-step time 0.908s\n",
      "I0727 22:04:28.213482 21236 model_lib_v2.py:705] Step 600 per-step time 0.908s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.19555123,\n",
      " 'Loss/localization_loss': 0.04643235,\n",
      " 'Loss/regularization_loss': 0.15094629,\n",
      " 'Loss/total_loss': 0.39292985,\n",
      " 'learning_rate': 0.014666399}\n",
      "I0727 22:04:28.213482 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.19555123,\n",
      " 'Loss/localization_loss': 0.04643235,\n",
      " 'Loss/regularization_loss': 0.15094629,\n",
      " 'Loss/total_loss': 0.39292985,\n",
      " 'learning_rate': 0.014666399}\n",
      "INFO:tensorflow:Step 700 per-step time 0.900s\n",
      "I0727 22:05:58.232443 21236 model_lib_v2.py:705] Step 700 per-step time 0.900s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.16414608,\n",
      " 'Loss/localization_loss': 0.03918678,\n",
      " 'Loss/regularization_loss': 0.15081102,\n",
      " 'Loss/total_loss': 0.35414386,\n",
      " 'learning_rate': 0.0159998}\n",
      "I0727 22:05:58.233444 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.16414608,\n",
      " 'Loss/localization_loss': 0.03918678,\n",
      " 'Loss/regularization_loss': 0.15081102,\n",
      " 'Loss/total_loss': 0.35414386,\n",
      " 'learning_rate': 0.0159998}\n",
      "INFO:tensorflow:Step 800 per-step time 0.934s\n",
      "I0727 22:07:31.629143 21236 model_lib_v2.py:705] Step 800 per-step time 0.934s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.17636679,\n",
      " 'Loss/localization_loss': 0.035584576,\n",
      " 'Loss/regularization_loss': 0.15066132,\n",
      " 'Loss/total_loss': 0.3626127,\n",
      " 'learning_rate': 0.0173332}\n",
      "I0727 22:07:31.629143 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.17636679,\n",
      " 'Loss/localization_loss': 0.035584576,\n",
      " 'Loss/regularization_loss': 0.15066132,\n",
      " 'Loss/total_loss': 0.3626127,\n",
      " 'learning_rate': 0.0173332}\n",
      "INFO:tensorflow:Step 900 per-step time 0.957s\n",
      "I0727 22:09:07.277001 21236 model_lib_v2.py:705] Step 900 per-step time 0.957s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.12775834,\n",
      " 'Loss/localization_loss': 0.0384371,\n",
      " 'Loss/regularization_loss': 0.15050094,\n",
      " 'Loss/total_loss': 0.31669638,\n",
      " 'learning_rate': 0.018666599}\n",
      "I0727 22:09:07.277001 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.12775834,\n",
      " 'Loss/localization_loss': 0.0384371,\n",
      " 'Loss/regularization_loss': 0.15050094,\n",
      " 'Loss/total_loss': 0.31669638,\n",
      " 'learning_rate': 0.018666599}\n",
      "INFO:tensorflow:Step 1000 per-step time 0.986s\n",
      "I0727 22:10:45.867971 21236 model_lib_v2.py:705] Step 1000 per-step time 0.986s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.10377909,\n",
      " 'Loss/localization_loss': 0.035532016,\n",
      " 'Loss/regularization_loss': 0.15032074,\n",
      " 'Loss/total_loss': 0.28963187,\n",
      " 'learning_rate': 0.02}\n",
      "I0727 22:10:45.867971 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.10377909,\n",
      " 'Loss/localization_loss': 0.035532016,\n",
      " 'Loss/regularization_loss': 0.15032074,\n",
      " 'Loss/total_loss': 0.28963187,\n",
      " 'learning_rate': 0.02}\n",
      "INFO:tensorflow:Step 1100 per-step time 0.942s\n",
      "I0727 22:12:20.137331 21236 model_lib_v2.py:705] Step 1100 per-step time 0.942s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.16180974,\n",
      " 'Loss/localization_loss': 0.03589073,\n",
      " 'Loss/regularization_loss': 0.15012981,\n",
      " 'Loss/total_loss': 0.34783027,\n",
      " 'learning_rate': 0.019999795}\n",
      "I0727 22:12:20.138261 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.16180974,\n",
      " 'Loss/localization_loss': 0.03589073,\n",
      " 'Loss/regularization_loss': 0.15012981,\n",
      " 'Loss/total_loss': 0.34783027,\n",
      " 'learning_rate': 0.019999795}\n",
      "INFO:tensorflow:Step 1200 per-step time 0.909s\n",
      "I0727 22:13:50.987010 21236 model_lib_v2.py:705] Step 1200 per-step time 0.909s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.11685168,\n",
      " 'Loss/localization_loss': 0.03138153,\n",
      " 'Loss/regularization_loss': 0.14993395,\n",
      " 'Loss/total_loss': 0.29816714,\n",
      " 'learning_rate': 0.019999176}\n",
      "I0727 22:13:50.987010 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.11685168,\n",
      " 'Loss/localization_loss': 0.03138153,\n",
      " 'Loss/regularization_loss': 0.14993395,\n",
      " 'Loss/total_loss': 0.29816714,\n",
      " 'learning_rate': 0.019999176}\n",
      "INFO:tensorflow:Step 1300 per-step time 0.931s\n",
      "I0727 22:15:24.101431 21236 model_lib_v2.py:705] Step 1300 per-step time 0.931s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.118057914,\n",
      " 'Loss/localization_loss': 0.034261253,\n",
      " 'Loss/regularization_loss': 0.14974332,\n",
      " 'Loss/total_loss': 0.30206248,\n",
      " 'learning_rate': 0.01999815}\n",
      "I0727 22:15:24.101431 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.118057914,\n",
      " 'Loss/localization_loss': 0.034261253,\n",
      " 'Loss/regularization_loss': 0.14974332,\n",
      " 'Loss/total_loss': 0.30206248,\n",
      " 'learning_rate': 0.01999815}\n",
      "INFO:tensorflow:Step 1400 per-step time 0.898s\n",
      "I0727 22:16:53.840221 21236 model_lib_v2.py:705] Step 1400 per-step time 0.898s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.14476337,\n",
      " 'Loss/localization_loss': 0.046607476,\n",
      " 'Loss/regularization_loss': 0.14954795,\n",
      " 'Loss/total_loss': 0.34091878,\n",
      " 'learning_rate': 0.019996712}\n",
      "I0727 22:16:53.840221 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.14476337,\n",
      " 'Loss/localization_loss': 0.046607476,\n",
      " 'Loss/regularization_loss': 0.14954795,\n",
      " 'Loss/total_loss': 0.34091878,\n",
      " 'learning_rate': 0.019996712}\n",
      "INFO:tensorflow:Step 1500 per-step time 0.919s\n",
      "I0727 22:18:25.687494 21236 model_lib_v2.py:705] Step 1500 per-step time 0.919s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.133967,\n",
      " 'Loss/localization_loss': 0.03813086,\n",
      " 'Loss/regularization_loss': 0.14935097,\n",
      " 'Loss/total_loss': 0.32144883,\n",
      " 'learning_rate': 0.019994862}\n",
      "I0727 22:18:25.688491 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.133967,\n",
      " 'Loss/localization_loss': 0.03813086,\n",
      " 'Loss/regularization_loss': 0.14935097,\n",
      " 'Loss/total_loss': 0.32144883,\n",
      " 'learning_rate': 0.019994862}\n",
      "INFO:tensorflow:Step 1600 per-step time 0.945s\n",
      "I0727 22:20:00.253588 21236 model_lib_v2.py:705] Step 1600 per-step time 0.945s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.15978405,\n",
      " 'Loss/localization_loss': 0.054466523,\n",
      " 'Loss/regularization_loss': 0.14917566,\n",
      " 'Loss/total_loss': 0.36342624,\n",
      " 'learning_rate': 0.019992601}\n",
      "I0727 22:20:00.253588 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.15978405,\n",
      " 'Loss/localization_loss': 0.054466523,\n",
      " 'Loss/regularization_loss': 0.14917566,\n",
      " 'Loss/total_loss': 0.36342624,\n",
      " 'learning_rate': 0.019992601}\n",
      "INFO:tensorflow:Step 1700 per-step time 0.964s\n",
      "I0727 22:21:36.656229 21236 model_lib_v2.py:705] Step 1700 per-step time 0.964s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09141595,\n",
      " 'Loss/localization_loss': 0.019088108,\n",
      " 'Loss/regularization_loss': 0.14897463,\n",
      " 'Loss/total_loss': 0.2594787,\n",
      " 'learning_rate': 0.01998993}\n",
      "I0727 22:21:36.656229 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.09141595,\n",
      " 'Loss/localization_loss': 0.019088108,\n",
      " 'Loss/regularization_loss': 0.14897463,\n",
      " 'Loss/total_loss': 0.2594787,\n",
      " 'learning_rate': 0.01998993}\n",
      "INFO:tensorflow:Step 1800 per-step time 0.891s\n",
      "I0727 22:23:05.663414 21236 model_lib_v2.py:705] Step 1800 per-step time 0.891s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.082418226,\n",
      " 'Loss/localization_loss': 0.028918754,\n",
      " 'Loss/regularization_loss': 0.14876647,\n",
      " 'Loss/total_loss': 0.26010343,\n",
      " 'learning_rate': 0.01998685}\n",
      "I0727 22:23:05.663414 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.082418226,\n",
      " 'Loss/localization_loss': 0.028918754,\n",
      " 'Loss/regularization_loss': 0.14876647,\n",
      " 'Loss/total_loss': 0.26010343,\n",
      " 'learning_rate': 0.01998685}\n",
      "INFO:tensorflow:Step 1900 per-step time 0.893s\n",
      "I0727 22:24:35.029323 21236 model_lib_v2.py:705] Step 1900 per-step time 0.893s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.10118934,\n",
      " 'Loss/localization_loss': 0.027723711,\n",
      " 'Loss/regularization_loss': 0.14855945,\n",
      " 'Loss/total_loss': 0.2774725,\n",
      " 'learning_rate': 0.019983355}\n",
      "I0727 22:24:35.029323 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.10118934,\n",
      " 'Loss/localization_loss': 0.027723711,\n",
      " 'Loss/regularization_loss': 0.14855945,\n",
      " 'Loss/total_loss': 0.2774725,\n",
      " 'learning_rate': 0.019983355}\n",
      "INFO:tensorflow:Step 2000 per-step time 0.903s\n",
      "I0727 22:26:05.299388 21236 model_lib_v2.py:705] Step 2000 per-step time 0.903s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.090890795,\n",
      " 'Loss/localization_loss': 0.022754312,\n",
      " 'Loss/regularization_loss': 0.14834912,\n",
      " 'Loss/total_loss': 0.26199424,\n",
      " 'learning_rate': 0.019979453}\n",
      "I0727 22:26:05.299388 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.090890795,\n",
      " 'Loss/localization_loss': 0.022754312,\n",
      " 'Loss/regularization_loss': 0.14834912,\n",
      " 'Loss/total_loss': 0.26199424,\n",
      " 'learning_rate': 0.019979453}\n",
      "INFO:tensorflow:Step 2100 per-step time 0.915s\n",
      "I0727 22:27:36.809551 21236 model_lib_v2.py:705] Step 2100 per-step time 0.915s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.11810975,\n",
      " 'Loss/localization_loss': 0.029603105,\n",
      " 'Loss/regularization_loss': 0.14814135,\n",
      " 'Loss/total_loss': 0.29585418,\n",
      " 'learning_rate': 0.01997514}\n",
      "I0727 22:27:36.809551 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.11810975,\n",
      " 'Loss/localization_loss': 0.029603105,\n",
      " 'Loss/regularization_loss': 0.14814135,\n",
      " 'Loss/total_loss': 0.29585418,\n",
      " 'learning_rate': 0.01997514}\n",
      "INFO:tensorflow:Step 2200 per-step time 0.893s\n",
      "I0727 22:29:06.188834 21236 model_lib_v2.py:705] Step 2200 per-step time 0.893s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.11143779,\n",
      " 'Loss/localization_loss': 0.026659487,\n",
      " 'Loss/regularization_loss': 0.14794543,\n",
      " 'Loss/total_loss': 0.28604272,\n",
      " 'learning_rate': 0.019970417}\n",
      "I0727 22:29:06.189763 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.11143779,\n",
      " 'Loss/localization_loss': 0.026659487,\n",
      " 'Loss/regularization_loss': 0.14794543,\n",
      " 'Loss/total_loss': 0.28604272,\n",
      " 'learning_rate': 0.019970417}\n",
      "INFO:tensorflow:Step 2300 per-step time 0.881s\n",
      "I0727 22:30:34.258693 21236 model_lib_v2.py:705] Step 2300 per-step time 0.881s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09733808,\n",
      " 'Loss/localization_loss': 0.025003187,\n",
      " 'Loss/regularization_loss': 0.14777446,\n",
      " 'Loss/total_loss': 0.27011573,\n",
      " 'learning_rate': 0.019965285}\n",
      "I0727 22:30:34.258693 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.09733808,\n",
      " 'Loss/localization_loss': 0.025003187,\n",
      " 'Loss/regularization_loss': 0.14777446,\n",
      " 'Loss/total_loss': 0.27011573,\n",
      " 'learning_rate': 0.019965285}\n",
      "INFO:tensorflow:Step 2400 per-step time 0.891s\n",
      "I0727 22:32:03.353961 21236 model_lib_v2.py:705] Step 2400 per-step time 0.891s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09027126,\n",
      " 'Loss/localization_loss': 0.023689529,\n",
      " 'Loss/regularization_loss': 0.14756985,\n",
      " 'Loss/total_loss': 0.26153064,\n",
      " 'learning_rate': 0.019959742}\n",
      "I0727 22:32:03.353961 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.09027126,\n",
      " 'Loss/localization_loss': 0.023689529,\n",
      " 'Loss/regularization_loss': 0.14756985,\n",
      " 'Loss/total_loss': 0.26153064,\n",
      " 'learning_rate': 0.019959742}\n",
      "INFO:tensorflow:Step 2500 per-step time 0.898s\n",
      "I0727 22:33:33.164768 21236 model_lib_v2.py:705] Step 2500 per-step time 0.898s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.092964865,\n",
      " 'Loss/localization_loss': 0.04124611,\n",
      " 'Loss/regularization_loss': 0.14736679,\n",
      " 'Loss/total_loss': 0.28157777,\n",
      " 'learning_rate': 0.019953791}\n",
      "I0727 22:33:33.164768 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.092964865,\n",
      " 'Loss/localization_loss': 0.04124611,\n",
      " 'Loss/regularization_loss': 0.14736679,\n",
      " 'Loss/total_loss': 0.28157777,\n",
      " 'learning_rate': 0.019953791}\n",
      "INFO:tensorflow:Step 2600 per-step time 0.900s\n",
      "I0727 22:35:03.144442 21236 model_lib_v2.py:705] Step 2600 per-step time 0.900s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08897904,\n",
      " 'Loss/localization_loss': 0.049592447,\n",
      " 'Loss/regularization_loss': 0.14716463,\n",
      " 'Loss/total_loss': 0.2857361,\n",
      " 'learning_rate': 0.01994743}\n",
      "I0727 22:35:03.145441 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08897904,\n",
      " 'Loss/localization_loss': 0.049592447,\n",
      " 'Loss/regularization_loss': 0.14716463,\n",
      " 'Loss/total_loss': 0.2857361,\n",
      " 'learning_rate': 0.01994743}\n",
      "INFO:tensorflow:Step 2700 per-step time 0.896s\n",
      "I0727 22:36:32.685376 21236 model_lib_v2.py:705] Step 2700 per-step time 0.896s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.070308834,\n",
      " 'Loss/localization_loss': 0.0188964,\n",
      " 'Loss/regularization_loss': 0.14695989,\n",
      " 'Loss/total_loss': 0.23616512,\n",
      " 'learning_rate': 0.01994066}\n",
      "I0727 22:36:32.686377 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.070308834,\n",
      " 'Loss/localization_loss': 0.0188964,\n",
      " 'Loss/regularization_loss': 0.14695989,\n",
      " 'Loss/total_loss': 0.23616512,\n",
      " 'learning_rate': 0.01994066}\n",
      "INFO:tensorflow:Step 2800 per-step time 0.904s\n",
      "I0727 22:38:03.109666 21236 model_lib_v2.py:705] Step 2800 per-step time 0.904s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08581184,\n",
      " 'Loss/localization_loss': 0.026318407,\n",
      " 'Loss/regularization_loss': 0.14675416,\n",
      " 'Loss/total_loss': 0.2588844,\n",
      " 'learning_rate': 0.01993348}\n",
      "I0727 22:38:03.110661 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08581184,\n",
      " 'Loss/localization_loss': 0.026318407,\n",
      " 'Loss/regularization_loss': 0.14675416,\n",
      " 'Loss/total_loss': 0.2588844,\n",
      " 'learning_rate': 0.01993348}\n",
      "INFO:tensorflow:Step 2900 per-step time 0.909s\n",
      "I0727 22:39:33.995588 21236 model_lib_v2.py:705] Step 2900 per-step time 0.909s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.091321126,\n",
      " 'Loss/localization_loss': 0.0253436,\n",
      " 'Loss/regularization_loss': 0.14655477,\n",
      " 'Loss/total_loss': 0.2632195,\n",
      " 'learning_rate': 0.019925894}\n",
      "I0727 22:39:33.995588 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.091321126,\n",
      " 'Loss/localization_loss': 0.0253436,\n",
      " 'Loss/regularization_loss': 0.14655477,\n",
      " 'Loss/total_loss': 0.2632195,\n",
      " 'learning_rate': 0.019925894}\n",
      "INFO:tensorflow:Step 3000 per-step time 0.862s\n",
      "I0727 22:41:00.182584 21236 model_lib_v2.py:705] Step 3000 per-step time 0.862s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.07449074,\n",
      " 'Loss/localization_loss': 0.027950287,\n",
      " 'Loss/regularization_loss': 0.14637364,\n",
      " 'Loss/total_loss': 0.24881467,\n",
      " 'learning_rate': 0.0199179}\n",
      "I0727 22:41:00.182584 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.07449074,\n",
      " 'Loss/localization_loss': 0.027950287,\n",
      " 'Loss/regularization_loss': 0.14637364,\n",
      " 'Loss/total_loss': 0.24881467,\n",
      " 'learning_rate': 0.0199179}\n",
      "INFO:tensorflow:Step 3100 per-step time 0.878s\n",
      "I0727 22:42:27.984719 21236 model_lib_v2.py:705] Step 3100 per-step time 0.878s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08512057,\n",
      " 'Loss/localization_loss': 0.021677054,\n",
      " 'Loss/regularization_loss': 0.14617378,\n",
      " 'Loss/total_loss': 0.2529714,\n",
      " 'learning_rate': 0.019909497}\n",
      "I0727 22:42:27.984719 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08512057,\n",
      " 'Loss/localization_loss': 0.021677054,\n",
      " 'Loss/regularization_loss': 0.14617378,\n",
      " 'Loss/total_loss': 0.2529714,\n",
      " 'learning_rate': 0.019909497}\n",
      "INFO:tensorflow:Step 3200 per-step time 0.873s\n",
      "I0727 22:43:55.312601 21236 model_lib_v2.py:705] Step 3200 per-step time 0.873s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.0934145,\n",
      " 'Loss/localization_loss': 0.027539697,\n",
      " 'Loss/regularization_loss': 0.14596878,\n",
      " 'Loss/total_loss': 0.26692298,\n",
      " 'learning_rate': 0.019900687}\n",
      "I0727 22:43:55.313529 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.0934145,\n",
      " 'Loss/localization_loss': 0.027539697,\n",
      " 'Loss/regularization_loss': 0.14596878,\n",
      " 'Loss/total_loss': 0.26692298,\n",
      " 'learning_rate': 0.019900687}\n",
      "INFO:tensorflow:Step 3300 per-step time 0.884s\n",
      "I0727 22:45:23.703398 21236 model_lib_v2.py:705] Step 3300 per-step time 0.884s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08545942,\n",
      " 'Loss/localization_loss': 0.022733208,\n",
      " 'Loss/regularization_loss': 0.14575955,\n",
      " 'Loss/total_loss': 0.25395218,\n",
      " 'learning_rate': 0.01989147}\n",
      "I0727 22:45:23.703398 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08545942,\n",
      " 'Loss/localization_loss': 0.022733208,\n",
      " 'Loss/regularization_loss': 0.14575955,\n",
      " 'Loss/total_loss': 0.25395218,\n",
      " 'learning_rate': 0.01989147}\n",
      "INFO:tensorflow:Step 3400 per-step time 0.889s\n",
      "I0727 22:46:52.661476 21236 model_lib_v2.py:705] Step 3400 per-step time 0.889s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08226542,\n",
      " 'Loss/localization_loss': 0.029866995,\n",
      " 'Loss/regularization_loss': 0.14555266,\n",
      " 'Loss/total_loss': 0.2576851,\n",
      " 'learning_rate': 0.019881846}\n",
      "I0727 22:46:52.662483 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08226542,\n",
      " 'Loss/localization_loss': 0.029866995,\n",
      " 'Loss/regularization_loss': 0.14555266,\n",
      " 'Loss/total_loss': 0.2576851,\n",
      " 'learning_rate': 0.019881846}\n",
      "INFO:tensorflow:Step 3500 per-step time 0.890s\n",
      "I0727 22:48:21.656954 21236 model_lib_v2.py:705] Step 3500 per-step time 0.890s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.107706726,\n",
      " 'Loss/localization_loss': 0.038123243,\n",
      " 'Loss/regularization_loss': 0.14534475,\n",
      " 'Loss/total_loss': 0.2911747,\n",
      " 'learning_rate': 0.019871818}\n",
      "I0727 22:48:21.657958 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.107706726,\n",
      " 'Loss/localization_loss': 0.038123243,\n",
      " 'Loss/regularization_loss': 0.14534475,\n",
      " 'Loss/total_loss': 0.2911747,\n",
      " 'learning_rate': 0.019871818}\n",
      "INFO:tensorflow:Step 3600 per-step time 0.896s\n",
      "I0727 22:49:51.246282 21236 model_lib_v2.py:705] Step 3600 per-step time 0.896s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.0727129,\n",
      " 'Loss/localization_loss': 0.029033588,\n",
      " 'Loss/regularization_loss': 0.1451402,\n",
      " 'Loss/total_loss': 0.24688669,\n",
      " 'learning_rate': 0.019861382}\n",
      "I0727 22:49:51.246282 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.0727129,\n",
      " 'Loss/localization_loss': 0.029033588,\n",
      " 'Loss/regularization_loss': 0.1451402,\n",
      " 'Loss/total_loss': 0.24688669,\n",
      " 'learning_rate': 0.019861382}\n",
      "INFO:tensorflow:Step 3700 per-step time 0.895s\n",
      "I0727 22:51:20.753928 21236 model_lib_v2.py:705] Step 3700 per-step time 0.895s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.100682065,\n",
      " 'Loss/localization_loss': 0.025720611,\n",
      " 'Loss/regularization_loss': 0.14493373,\n",
      " 'Loss/total_loss': 0.2713364,\n",
      " 'learning_rate': 0.01985054}\n",
      "I0727 22:51:20.753928 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.100682065,\n",
      " 'Loss/localization_loss': 0.025720611,\n",
      " 'Loss/regularization_loss': 0.14493373,\n",
      " 'Loss/total_loss': 0.2713364,\n",
      " 'learning_rate': 0.01985054}\n",
      "INFO:tensorflow:Step 3800 per-step time 0.892s\n",
      "I0727 22:52:49.957802 21236 model_lib_v2.py:705] Step 3800 per-step time 0.892s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08272069,\n",
      " 'Loss/localization_loss': 0.021386277,\n",
      " 'Loss/regularization_loss': 0.14473024,\n",
      " 'Loss/total_loss': 0.24883722,\n",
      " 'learning_rate': 0.019839296}\n",
      "I0727 22:52:49.957802 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08272069,\n",
      " 'Loss/localization_loss': 0.021386277,\n",
      " 'Loss/regularization_loss': 0.14473024,\n",
      " 'Loss/total_loss': 0.24883722,\n",
      " 'learning_rate': 0.019839296}\n",
      "INFO:tensorflow:Step 3900 per-step time 0.882s\n",
      "I0727 22:54:18.120759 21236 model_lib_v2.py:705] Step 3900 per-step time 0.882s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08476197,\n",
      " 'Loss/localization_loss': 0.02711374,\n",
      " 'Loss/regularization_loss': 0.14453061,\n",
      " 'Loss/total_loss': 0.2564063,\n",
      " 'learning_rate': 0.019827645}\n",
      "I0727 22:54:18.120759 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08476197,\n",
      " 'Loss/localization_loss': 0.02711374,\n",
      " 'Loss/regularization_loss': 0.14453061,\n",
      " 'Loss/total_loss': 0.2564063,\n",
      " 'learning_rate': 0.019827645}\n",
      "INFO:tensorflow:Step 4000 per-step time 0.881s\n",
      "I0727 22:55:46.255744 21236 model_lib_v2.py:705] Step 4000 per-step time 0.881s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.06583286,\n",
      " 'Loss/localization_loss': 0.01729674,\n",
      " 'Loss/regularization_loss': 0.14432341,\n",
      " 'Loss/total_loss': 0.22745301,\n",
      " 'learning_rate': 0.01981559}\n",
      "I0727 22:55:46.255744 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.06583286,\n",
      " 'Loss/localization_loss': 0.01729674,\n",
      " 'Loss/regularization_loss': 0.14432341,\n",
      " 'Loss/total_loss': 0.22745301,\n",
      " 'learning_rate': 0.01981559}\n",
      "INFO:tensorflow:Step 4100 per-step time 0.885s\n",
      "I0727 22:57:14.725939 21236 model_lib_v2.py:705] Step 4100 per-step time 0.885s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.11581183,\n",
      " 'Loss/localization_loss': 0.044892766,\n",
      " 'Loss/regularization_loss': 0.14411598,\n",
      " 'Loss/total_loss': 0.30482057,\n",
      " 'learning_rate': 0.019803133}\n",
      "I0727 22:57:14.725939 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.11581183,\n",
      " 'Loss/localization_loss': 0.044892766,\n",
      " 'Loss/regularization_loss': 0.14411598,\n",
      " 'Loss/total_loss': 0.30482057,\n",
      " 'learning_rate': 0.019803133}\n",
      "INFO:tensorflow:Step 4200 per-step time 0.896s\n",
      "I0727 22:58:44.310746 21236 model_lib_v2.py:705] Step 4200 per-step time 0.896s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.11333718,\n",
      " 'Loss/localization_loss': 0.041133787,\n",
      " 'Loss/regularization_loss': 0.1439094,\n",
      " 'Loss/total_loss': 0.29838037,\n",
      " 'learning_rate': 0.019790273}\n",
      "I0727 22:58:44.311742 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.11333718,\n",
      " 'Loss/localization_loss': 0.041133787,\n",
      " 'Loss/regularization_loss': 0.1439094,\n",
      " 'Loss/total_loss': 0.29838037,\n",
      " 'learning_rate': 0.019790273}\n",
      "INFO:tensorflow:Step 4300 per-step time 0.892s\n",
      "I0727 23:00:13.547776 21236 model_lib_v2.py:705] Step 4300 per-step time 0.892s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08548263,\n",
      " 'Loss/localization_loss': 0.017384287,\n",
      " 'Loss/regularization_loss': 0.14371844,\n",
      " 'Loss/total_loss': 0.24658535,\n",
      " 'learning_rate': 0.01977701}\n",
      "I0727 23:00:13.547776 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08548263,\n",
      " 'Loss/localization_loss': 0.017384287,\n",
      " 'Loss/regularization_loss': 0.14371844,\n",
      " 'Loss/total_loss': 0.24658535,\n",
      " 'learning_rate': 0.01977701}\n",
      "INFO:tensorflow:Step 4400 per-step time 0.895s\n",
      "I0727 23:01:43.004130 21236 model_lib_v2.py:705] Step 4400 per-step time 0.895s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.15492834,\n",
      " 'Loss/localization_loss': 0.035469428,\n",
      " 'Loss/regularization_loss': 0.14351536,\n",
      " 'Loss/total_loss': 0.33391315,\n",
      " 'learning_rate': 0.019763345}\n",
      "I0727 23:01:43.004130 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.15492834,\n",
      " 'Loss/localization_loss': 0.035469428,\n",
      " 'Loss/regularization_loss': 0.14351536,\n",
      " 'Loss/total_loss': 0.33391315,\n",
      " 'learning_rate': 0.019763345}\n",
      "INFO:tensorflow:Step 4500 per-step time 0.900s\n",
      "I0727 23:03:13.021592 21236 model_lib_v2.py:705] Step 4500 per-step time 0.900s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.10363286,\n",
      " 'Loss/localization_loss': 0.044080704,\n",
      " 'Loss/regularization_loss': 0.14333247,\n",
      " 'Loss/total_loss': 0.29104602,\n",
      " 'learning_rate': 0.019749278}\n",
      "I0727 23:03:13.022593 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.10363286,\n",
      " 'Loss/localization_loss': 0.044080704,\n",
      " 'Loss/regularization_loss': 0.14333247,\n",
      " 'Loss/total_loss': 0.29104602,\n",
      " 'learning_rate': 0.019749278}\n",
      "INFO:tensorflow:Step 4600 per-step time 0.929s\n",
      "I0727 23:04:46.003468 21236 model_lib_v2.py:705] Step 4600 per-step time 0.929s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.107196614,\n",
      " 'Loss/localization_loss': 0.021320494,\n",
      " 'Loss/regularization_loss': 0.14312977,\n",
      " 'Loss/total_loss': 0.2716469,\n",
      " 'learning_rate': 0.019734811}\n",
      "I0727 23:04:46.003468 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.107196614,\n",
      " 'Loss/localization_loss': 0.021320494,\n",
      " 'Loss/regularization_loss': 0.14312977,\n",
      " 'Loss/total_loss': 0.2716469,\n",
      " 'learning_rate': 0.019734811}\n",
      "INFO:tensorflow:Step 4700 per-step time 0.899s\n",
      "I0727 23:06:15.879028 21236 model_lib_v2.py:705] Step 4700 per-step time 0.899s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.06489,\n",
      " 'Loss/localization_loss': 0.019667512,\n",
      " 'Loss/regularization_loss': 0.14292884,\n",
      " 'Loss/total_loss': 0.22748634,\n",
      " 'learning_rate': 0.019719945}\n",
      "I0727 23:06:15.879028 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.06489,\n",
      " 'Loss/localization_loss': 0.019667512,\n",
      " 'Loss/regularization_loss': 0.14292884,\n",
      " 'Loss/total_loss': 0.22748634,\n",
      " 'learning_rate': 0.019719945}\n",
      "INFO:tensorflow:Step 4800 per-step time 0.890s\n",
      "I0727 23:07:44.879219 21236 model_lib_v2.py:705] Step 4800 per-step time 0.890s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.097974665,\n",
      " 'Loss/localization_loss': 0.030736899,\n",
      " 'Loss/regularization_loss': 0.14274423,\n",
      " 'Loss/total_loss': 0.2714558,\n",
      " 'learning_rate': 0.019704677}\n",
      "I0727 23:07:44.879219 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.097974665,\n",
      " 'Loss/localization_loss': 0.030736899,\n",
      " 'Loss/regularization_loss': 0.14274423,\n",
      " 'Loss/total_loss': 0.2714558,\n",
      " 'learning_rate': 0.019704677}\n",
      "INFO:tensorflow:Step 4900 per-step time 0.889s\n",
      "I0727 23:09:13.807991 21236 model_lib_v2.py:705] Step 4900 per-step time 0.889s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09892092,\n",
      " 'Loss/localization_loss': 0.025833601,\n",
      " 'Loss/regularization_loss': 0.14254472,\n",
      " 'Loss/total_loss': 0.26729923,\n",
      " 'learning_rate': 0.019689012}\n",
      "I0727 23:09:13.807991 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.09892092,\n",
      " 'Loss/localization_loss': 0.025833601,\n",
      " 'Loss/regularization_loss': 0.14254472,\n",
      " 'Loss/total_loss': 0.26729923,\n",
      " 'learning_rate': 0.019689012}\n",
      "INFO:tensorflow:Step 5000 per-step time 0.902s\n",
      "I0727 23:10:44.039795 21236 model_lib_v2.py:705] Step 5000 per-step time 0.902s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08117097,\n",
      " 'Loss/localization_loss': 0.025635833,\n",
      " 'Loss/regularization_loss': 0.1423559,\n",
      " 'Loss/total_loss': 0.2491627,\n",
      " 'learning_rate': 0.019672949}\n",
      "I0727 23:10:44.039795 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08117097,\n",
      " 'Loss/localization_loss': 0.025635833,\n",
      " 'Loss/regularization_loss': 0.1423559,\n",
      " 'Loss/total_loss': 0.2491627,\n",
      " 'learning_rate': 0.019672949}\n",
      "INFO:tensorflow:Step 5100 per-step time 0.903s\n",
      "I0727 23:12:14.282886 21236 model_lib_v2.py:705] Step 5100 per-step time 0.903s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09242217,\n",
      " 'Loss/localization_loss': 0.01965417,\n",
      " 'Loss/regularization_loss': 0.14219673,\n",
      " 'Loss/total_loss': 0.25427306,\n",
      " 'learning_rate': 0.019656487}\n",
      "I0727 23:12:14.283899 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.09242217,\n",
      " 'Loss/localization_loss': 0.01965417,\n",
      " 'Loss/regularization_loss': 0.14219673,\n",
      " 'Loss/total_loss': 0.25427306,\n",
      " 'learning_rate': 0.019656487}\n",
      "INFO:tensorflow:Step 5200 per-step time 0.889s\n",
      "I0727 23:13:43.239473 21236 model_lib_v2.py:705] Step 5200 per-step time 0.889s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.0640409,\n",
      " 'Loss/localization_loss': 0.01972643,\n",
      " 'Loss/regularization_loss': 0.14199367,\n",
      " 'Loss/total_loss': 0.225761,\n",
      " 'learning_rate': 0.019639628}\n",
      "I0727 23:13:43.239473 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.0640409,\n",
      " 'Loss/localization_loss': 0.01972643,\n",
      " 'Loss/regularization_loss': 0.14199367,\n",
      " 'Loss/total_loss': 0.225761,\n",
      " 'learning_rate': 0.019639628}\n",
      "INFO:tensorflow:Step 5300 per-step time 0.889s\n",
      "I0727 23:15:12.173192 21236 model_lib_v2.py:705] Step 5300 per-step time 0.889s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.07347813,\n",
      " 'Loss/localization_loss': 0.016260492,\n",
      " 'Loss/regularization_loss': 0.14179723,\n",
      " 'Loss/total_loss': 0.23153585,\n",
      " 'learning_rate': 0.019622372}\n",
      "I0727 23:15:12.173192 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.07347813,\n",
      " 'Loss/localization_loss': 0.016260492,\n",
      " 'Loss/regularization_loss': 0.14179723,\n",
      " 'Loss/total_loss': 0.23153585,\n",
      " 'learning_rate': 0.019622372}\n",
      "INFO:tensorflow:Step 5400 per-step time 0.895s\n",
      "I0727 23:16:41.653719 21236 model_lib_v2.py:705] Step 5400 per-step time 0.895s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.11468906,\n",
      " 'Loss/localization_loss': 0.025131894,\n",
      " 'Loss/regularization_loss': 0.14159705,\n",
      " 'Loss/total_loss': 0.281418,\n",
      " 'learning_rate': 0.019604724}\n",
      "I0727 23:16:41.653719 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.11468906,\n",
      " 'Loss/localization_loss': 0.025131894,\n",
      " 'Loss/regularization_loss': 0.14159705,\n",
      " 'Loss/total_loss': 0.281418,\n",
      " 'learning_rate': 0.019604724}\n",
      "INFO:tensorflow:Step 5500 per-step time 0.895s\n",
      "I0727 23:18:11.229498 21236 model_lib_v2.py:705] Step 5500 per-step time 0.895s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08660129,\n",
      " 'Loss/localization_loss': 0.029468888,\n",
      " 'Loss/regularization_loss': 0.14139451,\n",
      " 'Loss/total_loss': 0.25746468,\n",
      " 'learning_rate': 0.019586679}\n",
      "I0727 23:18:11.229498 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08660129,\n",
      " 'Loss/localization_loss': 0.029468888,\n",
      " 'Loss/regularization_loss': 0.14139451,\n",
      " 'Loss/total_loss': 0.25746468,\n",
      " 'learning_rate': 0.019586679}\n",
      "INFO:tensorflow:Step 5600 per-step time 0.903s\n",
      "I0727 23:19:41.566318 21236 model_lib_v2.py:705] Step 5600 per-step time 0.903s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.07785738,\n",
      " 'Loss/localization_loss': 0.024445262,\n",
      " 'Loss/regularization_loss': 0.1411946,\n",
      " 'Loss/total_loss': 0.24349725,\n",
      " 'learning_rate': 0.019568238}\n",
      "I0727 23:19:41.567308 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.07785738,\n",
      " 'Loss/localization_loss': 0.024445262,\n",
      " 'Loss/regularization_loss': 0.1411946,\n",
      " 'Loss/total_loss': 0.24349725,\n",
      " 'learning_rate': 0.019568238}\n",
      "INFO:tensorflow:Step 5700 per-step time 0.888s\n",
      "I0727 23:21:10.324529 21236 model_lib_v2.py:705] Step 5700 per-step time 0.888s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.0766236,\n",
      " 'Loss/localization_loss': 0.021601731,\n",
      " 'Loss/regularization_loss': 0.14099634,\n",
      " 'Loss/total_loss': 0.23922168,\n",
      " 'learning_rate': 0.019549407}\n",
      "I0727 23:21:10.324529 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.0766236,\n",
      " 'Loss/localization_loss': 0.021601731,\n",
      " 'Loss/regularization_loss': 0.14099634,\n",
      " 'Loss/total_loss': 0.23922168,\n",
      " 'learning_rate': 0.019549407}\n",
      "INFO:tensorflow:Step 5800 per-step time 0.876s\n",
      "I0727 23:22:37.988358 21236 model_lib_v2.py:705] Step 5800 per-step time 0.876s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08036955,\n",
      " 'Loss/localization_loss': 0.041013952,\n",
      " 'Loss/regularization_loss': 0.14080371,\n",
      " 'Loss/total_loss': 0.2621872,\n",
      " 'learning_rate': 0.01953018}\n",
      "I0727 23:22:37.988358 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08036955,\n",
      " 'Loss/localization_loss': 0.041013952,\n",
      " 'Loss/regularization_loss': 0.14080371,\n",
      " 'Loss/total_loss': 0.2621872,\n",
      " 'learning_rate': 0.01953018}\n",
      "INFO:tensorflow:Step 5900 per-step time 0.896s\n",
      "I0727 23:24:07.626118 21236 model_lib_v2.py:705] Step 5900 per-step time 0.896s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.076179914,\n",
      " 'Loss/localization_loss': 0.012876758,\n",
      " 'Loss/regularization_loss': 0.14060587,\n",
      " 'Loss/total_loss': 0.22966255,\n",
      " 'learning_rate': 0.019510563}\n",
      "I0727 23:24:07.626118 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.076179914,\n",
      " 'Loss/localization_loss': 0.012876758,\n",
      " 'Loss/regularization_loss': 0.14060587,\n",
      " 'Loss/total_loss': 0.22966255,\n",
      " 'learning_rate': 0.019510563}\n",
      "INFO:tensorflow:Step 6000 per-step time 0.913s\n",
      "I0727 23:25:38.894113 21236 model_lib_v2.py:705] Step 6000 per-step time 0.913s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09632756,\n",
      " 'Loss/localization_loss': 0.01805138,\n",
      " 'Loss/regularization_loss': 0.14041635,\n",
      " 'Loss/total_loss': 0.25479528,\n",
      " 'learning_rate': 0.019490557}\n",
      "I0727 23:25:38.894113 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.09632756,\n",
      " 'Loss/localization_loss': 0.01805138,\n",
      " 'Loss/regularization_loss': 0.14041635,\n",
      " 'Loss/total_loss': 0.25479528,\n",
      " 'learning_rate': 0.019490557}\n",
      "INFO:tensorflow:Step 6100 per-step time 0.911s\n",
      "I0727 23:27:09.998881 21236 model_lib_v2.py:705] Step 6100 per-step time 0.911s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.0649751,\n",
      " 'Loss/localization_loss': 0.028078802,\n",
      " 'Loss/regularization_loss': 0.14021993,\n",
      " 'Loss/total_loss': 0.23327383,\n",
      " 'learning_rate': 0.019470159}\n",
      "I0727 23:27:09.998881 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.0649751,\n",
      " 'Loss/localization_loss': 0.028078802,\n",
      " 'Loss/regularization_loss': 0.14021993,\n",
      " 'Loss/total_loss': 0.23327383,\n",
      " 'learning_rate': 0.019470159}\n",
      "INFO:tensorflow:Step 6200 per-step time 0.895s\n",
      "I0727 23:28:39.459794 21236 model_lib_v2.py:705] Step 6200 per-step time 0.895s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.068967804,\n",
      " 'Loss/localization_loss': 0.01696116,\n",
      " 'Loss/regularization_loss': 0.14002365,\n",
      " 'Loss/total_loss': 0.22595261,\n",
      " 'learning_rate': 0.019449372}\n",
      "I0727 23:28:39.460794 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.068967804,\n",
      " 'Loss/localization_loss': 0.01696116,\n",
      " 'Loss/regularization_loss': 0.14002365,\n",
      " 'Loss/total_loss': 0.22595261,\n",
      " 'learning_rate': 0.019449372}\n",
      "INFO:tensorflow:Step 6300 per-step time 0.902s\n",
      "I0727 23:30:09.602252 21236 model_lib_v2.py:705] Step 6300 per-step time 0.902s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.060828537,\n",
      " 'Loss/localization_loss': 0.016531661,\n",
      " 'Loss/regularization_loss': 0.13982786,\n",
      " 'Loss/total_loss': 0.21718806,\n",
      " 'learning_rate': 0.019428197}\n",
      "I0727 23:30:09.603252 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.060828537,\n",
      " 'Loss/localization_loss': 0.016531661,\n",
      " 'Loss/regularization_loss': 0.13982786,\n",
      " 'Loss/total_loss': 0.21718806,\n",
      " 'learning_rate': 0.019428197}\n",
      "INFO:tensorflow:Step 6400 per-step time 0.878s\n",
      "I0727 23:31:37.475615 21236 model_lib_v2.py:705] Step 6400 per-step time 0.878s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.06468485,\n",
      " 'Loss/localization_loss': 0.024401113,\n",
      " 'Loss/regularization_loss': 0.13963254,\n",
      " 'Loss/total_loss': 0.2287185,\n",
      " 'learning_rate': 0.019406633}\n",
      "I0727 23:31:37.475615 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.06468485,\n",
      " 'Loss/localization_loss': 0.024401113,\n",
      " 'Loss/regularization_loss': 0.13963254,\n",
      " 'Loss/total_loss': 0.2287185,\n",
      " 'learning_rate': 0.019406633}\n",
      "INFO:tensorflow:Step 6500 per-step time 0.888s\n",
      "I0727 23:33:06.278628 21236 model_lib_v2.py:705] Step 6500 per-step time 0.888s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09291338,\n",
      " 'Loss/localization_loss': 0.03838594,\n",
      " 'Loss/regularization_loss': 0.13947268,\n",
      " 'Loss/total_loss': 0.27077198,\n",
      " 'learning_rate': 0.019384684}\n",
      "I0727 23:33:06.278628 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.09291338,\n",
      " 'Loss/localization_loss': 0.03838594,\n",
      " 'Loss/regularization_loss': 0.13947268,\n",
      " 'Loss/total_loss': 0.27077198,\n",
      " 'learning_rate': 0.019384684}\n",
      "INFO:tensorflow:Step 6600 per-step time 0.899s\n",
      "I0727 23:34:36.146461 21236 model_lib_v2.py:705] Step 6600 per-step time 0.899s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08942098,\n",
      " 'Loss/localization_loss': 0.02589336,\n",
      " 'Loss/regularization_loss': 0.139319,\n",
      " 'Loss/total_loss': 0.25463334,\n",
      " 'learning_rate': 0.019362349}\n",
      "I0727 23:34:36.146461 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08942098,\n",
      " 'Loss/localization_loss': 0.02589336,\n",
      " 'Loss/regularization_loss': 0.139319,\n",
      " 'Loss/total_loss': 0.25463334,\n",
      " 'learning_rate': 0.019362349}\n",
      "INFO:tensorflow:Step 6700 per-step time 0.903s\n",
      "I0727 23:36:06.442182 21236 model_lib_v2.py:705] Step 6700 per-step time 0.903s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.06466985,\n",
      " 'Loss/localization_loss': 0.01733929,\n",
      " 'Loss/regularization_loss': 0.13912493,\n",
      " 'Loss/total_loss': 0.22113407,\n",
      " 'learning_rate': 0.019339629}\n",
      "I0727 23:36:06.442182 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.06466985,\n",
      " 'Loss/localization_loss': 0.01733929,\n",
      " 'Loss/regularization_loss': 0.13912493,\n",
      " 'Loss/total_loss': 0.22113407,\n",
      " 'learning_rate': 0.019339629}\n",
      "INFO:tensorflow:Step 6800 per-step time 0.888s\n",
      "I0727 23:37:35.221733 21236 model_lib_v2.py:705] Step 6800 per-step time 0.888s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.11244618,\n",
      " 'Loss/localization_loss': 0.032591812,\n",
      " 'Loss/regularization_loss': 0.13892886,\n",
      " 'Loss/total_loss': 0.28396684,\n",
      " 'learning_rate': 0.019316524}\n",
      "I0727 23:37:35.221733 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.11244618,\n",
      " 'Loss/localization_loss': 0.032591812,\n",
      " 'Loss/regularization_loss': 0.13892886,\n",
      " 'Loss/total_loss': 0.28396684,\n",
      " 'learning_rate': 0.019316524}\n",
      "INFO:tensorflow:Step 6900 per-step time 0.897s\n",
      "I0727 23:39:04.866489 21236 model_lib_v2.py:705] Step 6900 per-step time 0.897s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.069446914,\n",
      " 'Loss/localization_loss': 0.020940168,\n",
      " 'Loss/regularization_loss': 0.13873327,\n",
      " 'Loss/total_loss': 0.22912036,\n",
      " 'learning_rate': 0.019293036}\n",
      "I0727 23:39:04.866489 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.069446914,\n",
      " 'Loss/localization_loss': 0.020940168,\n",
      " 'Loss/regularization_loss': 0.13873327,\n",
      " 'Loss/total_loss': 0.22912036,\n",
      " 'learning_rate': 0.019293036}\n",
      "INFO:tensorflow:Step 7000 per-step time 0.894s\n",
      "I0727 23:40:34.319694 21236 model_lib_v2.py:705] Step 7000 per-step time 0.894s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09282067,\n",
      " 'Loss/localization_loss': 0.018957354,\n",
      " 'Loss/regularization_loss': 0.13854061,\n",
      " 'Loss/total_loss': 0.25031862,\n",
      " 'learning_rate': 0.019269168}\n",
      "I0727 23:40:34.320693 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.09282067,\n",
      " 'Loss/localization_loss': 0.018957354,\n",
      " 'Loss/regularization_loss': 0.13854061,\n",
      " 'Loss/total_loss': 0.25031862,\n",
      " 'learning_rate': 0.019269168}\n",
      "INFO:tensorflow:Step 7100 per-step time 0.903s\n",
      "I0727 23:42:04.614541 21236 model_lib_v2.py:705] Step 7100 per-step time 0.903s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.094891846,\n",
      " 'Loss/localization_loss': 0.022671653,\n",
      " 'Loss/regularization_loss': 0.13834544,\n",
      " 'Loss/total_loss': 0.25590894,\n",
      " 'learning_rate': 0.019244917}\n",
      "I0727 23:42:04.614541 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.094891846,\n",
      " 'Loss/localization_loss': 0.022671653,\n",
      " 'Loss/regularization_loss': 0.13834544,\n",
      " 'Loss/total_loss': 0.25590894,\n",
      " 'learning_rate': 0.019244917}\n",
      "INFO:tensorflow:Step 7200 per-step time 0.896s\n",
      "I0727 23:43:34.136807 21236 model_lib_v2.py:705] Step 7200 per-step time 0.896s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.06898942,\n",
      " 'Loss/localization_loss': 0.022796853,\n",
      " 'Loss/regularization_loss': 0.13816598,\n",
      " 'Loss/total_loss': 0.22995226,\n",
      " 'learning_rate': 0.019220287}\n",
      "I0727 23:43:34.136807 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.06898942,\n",
      " 'Loss/localization_loss': 0.022796853,\n",
      " 'Loss/regularization_loss': 0.13816598,\n",
      " 'Loss/total_loss': 0.22995226,\n",
      " 'learning_rate': 0.019220287}\n",
      "INFO:tensorflow:Step 7300 per-step time 0.898s\n",
      "I0727 23:45:03.980797 21236 model_lib_v2.py:705] Step 7300 per-step time 0.898s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.062186528,\n",
      " 'Loss/localization_loss': 0.0127644725,\n",
      " 'Loss/regularization_loss': 0.13797665,\n",
      " 'Loss/total_loss': 0.21292764,\n",
      " 'learning_rate': 0.019195277}\n",
      "I0727 23:45:03.980797 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.062186528,\n",
      " 'Loss/localization_loss': 0.0127644725,\n",
      " 'Loss/regularization_loss': 0.13797665,\n",
      " 'Loss/total_loss': 0.21292764,\n",
      " 'learning_rate': 0.019195277}\n",
      "INFO:tensorflow:Step 7400 per-step time 0.872s\n",
      "I0727 23:46:31.233505 21236 model_lib_v2.py:705] Step 7400 per-step time 0.872s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08061973,\n",
      " 'Loss/localization_loss': 0.031709865,\n",
      " 'Loss/regularization_loss': 0.13778637,\n",
      " 'Loss/total_loss': 0.250116,\n",
      " 'learning_rate': 0.019169891}\n",
      "I0727 23:46:31.233505 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08061973,\n",
      " 'Loss/localization_loss': 0.031709865,\n",
      " 'Loss/regularization_loss': 0.13778637,\n",
      " 'Loss/total_loss': 0.250116,\n",
      " 'learning_rate': 0.019169891}\n",
      "INFO:tensorflow:Step 7500 per-step time 0.884s\n",
      "I0727 23:47:59.557513 21236 model_lib_v2.py:705] Step 7500 per-step time 0.884s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.05710353,\n",
      " 'Loss/localization_loss': 0.016521946,\n",
      " 'Loss/regularization_loss': 0.13759357,\n",
      " 'Loss/total_loss': 0.21121904,\n",
      " 'learning_rate': 0.019144125}\n",
      "I0727 23:47:59.558514 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.05710353,\n",
      " 'Loss/localization_loss': 0.016521946,\n",
      " 'Loss/regularization_loss': 0.13759357,\n",
      " 'Loss/total_loss': 0.21121904,\n",
      " 'learning_rate': 0.019144125}\n",
      "INFO:tensorflow:Step 7600 per-step time 0.895s\n",
      "I0727 23:49:29.129867 21236 model_lib_v2.py:705] Step 7600 per-step time 0.895s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.071371146,\n",
      " 'Loss/localization_loss': 0.027018875,\n",
      " 'Loss/regularization_loss': 0.1373991,\n",
      " 'Loss/total_loss': 0.23578912,\n",
      " 'learning_rate': 0.019117985}\n",
      "I0727 23:49:29.129867 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.071371146,\n",
      " 'Loss/localization_loss': 0.027018875,\n",
      " 'Loss/regularization_loss': 0.1373991,\n",
      " 'Loss/total_loss': 0.23578912,\n",
      " 'learning_rate': 0.019117985}\n",
      "INFO:tensorflow:Step 7700 per-step time 0.892s\n",
      "I0727 23:50:58.292175 21236 model_lib_v2.py:705] Step 7700 per-step time 0.892s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09097831,\n",
      " 'Loss/localization_loss': 0.01761197,\n",
      " 'Loss/regularization_loss': 0.1372074,\n",
      " 'Loss/total_loss': 0.24579768,\n",
      " 'learning_rate': 0.01909147}\n",
      "I0727 23:50:58.292175 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.09097831,\n",
      " 'Loss/localization_loss': 0.01761197,\n",
      " 'Loss/regularization_loss': 0.1372074,\n",
      " 'Loss/total_loss': 0.24579768,\n",
      " 'learning_rate': 0.01909147}\n",
      "INFO:tensorflow:Step 7800 per-step time 0.876s\n",
      "I0727 23:52:25.828147 21236 model_lib_v2.py:705] Step 7800 per-step time 0.876s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.08729589,\n",
      " 'Loss/localization_loss': 0.017927416,\n",
      " 'Loss/regularization_loss': 0.13701555,\n",
      " 'Loss/total_loss': 0.24223885,\n",
      " 'learning_rate': 0.019064583}\n",
      "I0727 23:52:25.828147 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.08729589,\n",
      " 'Loss/localization_loss': 0.017927416,\n",
      " 'Loss/regularization_loss': 0.13701555,\n",
      " 'Loss/total_loss': 0.24223885,\n",
      " 'learning_rate': 0.019064583}\n",
      "INFO:tensorflow:Step 7900 per-step time 0.895s\n",
      "I0727 23:53:55.375047 21236 model_lib_v2.py:705] Step 7900 per-step time 0.895s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.06041146,\n",
      " 'Loss/localization_loss': 0.014585681,\n",
      " 'Loss/regularization_loss': 0.13682473,\n",
      " 'Loss/total_loss': 0.21182185,\n",
      " 'learning_rate': 0.01903732}\n",
      "I0727 23:53:55.375047 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.06041146,\n",
      " 'Loss/localization_loss': 0.014585681,\n",
      " 'Loss/regularization_loss': 0.13682473,\n",
      " 'Loss/total_loss': 0.21182185,\n",
      " 'learning_rate': 0.01903732}\n",
      "INFO:tensorflow:Step 8000 per-step time 0.906s\n",
      "I0727 23:55:26.004516 21236 model_lib_v2.py:705] Step 8000 per-step time 0.906s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.05763397,\n",
      " 'Loss/localization_loss': 0.018362245,\n",
      " 'Loss/regularization_loss': 0.13663441,\n",
      " 'Loss/total_loss': 0.21263061,\n",
      " 'learning_rate': 0.019009687}\n",
      "I0727 23:55:26.004516 21236 model_lib_v2.py:708] {'Loss/classification_loss': 0.05763397,\n",
      " 'Loss/localization_loss': 0.018362245,\n",
      " 'Loss/regularization_loss': 0.13663441,\n",
      " 'Loss/total_loss': 0.21263061,\n",
      " 'learning_rate': 0.019009687}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py \\\n",
    "    --pipeline_config_path={config_path} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_workers=4 \\\n",
    "    --num_train_steps={NUM_STEPS} \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --num_eval_steps={NUM_EVAL_STEPS}\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "log_model_info(\n",
    "    batch=BATCH_SIZE,\n",
    "    classes=NUM_CLASSES,\n",
    "    steps=NUM_STEPS,\n",
    "    eval_steps=NUM_EVAL_STEPS,\n",
    "    name=MODEL_NAME,\n",
    "    bfloat16=use_bfloat16,\n",
    "    pipeline=base_config_path,\n",
    "    checkpoint_path=base_checkpoint_path,\n",
    "    labelmap=labelmap_path,\n",
    "    train_record=train_record_path,\n",
    "    test_record=test_record_path,\n",
    "    config=config_path,\n",
    "    trained_model=model_dir,\n",
    "    train_time=training_time,\n",
    "    df_test=df_final_test,\n",
    "    df_train=df_final_train,\n",
    "    notes=NOTES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"SSD_16000_640_mobilenet_v2_fpnlite\"\n",
    "config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\model_main_tf2.py \\\n",
    "            --pipeline_config_path={config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={model_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: SSD_8000_640_mobilenet_v2_fpnlite\n",
      "Done Evaluating: SSD_8000_640_mobilenet_v2_fpnlite\n",
      "No Error: SSD_8000_640_mobilenet_v2_fpnlite\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating model: {MODEL_NAME}\")\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "output, error = process.communicate()\n",
    "print(f\"Done Evaluating: {MODEL_NAME}\")\n",
    "if process.returncode != 0:\n",
    "    print(error.decode(\"utf-8\"))\n",
    "else:\n",
    "    print(f\"No Error: {MODEL_NAME}\")\n",
    "    output_str = output.decode(\"utf-8\")\n",
    "    header = f\"Evaluation Results for: {MODEL_NAME}\\n\\n\"\n",
    "    with open(f\"./myModules/log/{MODEL_NAME}_evaluation_results.txt\", \"w\") as f:\n",
    "        f.write(header)\n",
    "        f.write(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"SSD_8000_640_mobilenet_v1_fpn_custom\" # Bugged\n",
    "\"SSD_8000_640_mobilenet_v1_fpn\"\n",
    "\"SSD_8000_640_mobilenet_v2_fpnlite_custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: SSD_8000_640_resnet101_v1_fpn\n"
     ]
    }
   ],
   "source": [
    "model_names = [\n",
    "    \"SSD_8000_640_resnet101_v1_fpn\",\n",
    "    \"SSD_16000_640_mobilenet_v2_fpnlite\"\n",
    "]\n",
    "\n",
    "config_template = './myModules/configs/{name}_config.config'\n",
    "model_dir_template = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'\n",
    "log_dir = './myModules/log'\n",
    "\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def evaluate_model(model_name):\n",
    "    config_path = config_template.format(name=model_name)\n",
    "    model_dir = model_dir_template.format(name=model_name)\n",
    "    \n",
    "    command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\model_main_tf2.py \\\n",
    "                --pipeline_config_path={config_path} \\\n",
    "                --model_dir={model_dir} \\\n",
    "                --checkpoint_dir={model_dir}\"\n",
    "    \n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        error_message = error.decode(\"utf-8\")\n",
    "        print(f\"Error evaluating model {model_name}: {error_message}\")\n",
    "        return error_message\n",
    "    else:\n",
    "        output_str = output.decode(\"utf-8\")\n",
    "        header = f\"Evaluation Results for: {model_name}\\n\\n\"\n",
    "        result_file = os.path.join(log_dir, f\"{model_name}_evaluation_results.txt\")\n",
    "        with open(result_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            f.write(output_str)\n",
    "        print(f\"Evaluation results for {model_name} saved to {result_file}\")\n",
    "        return output_str\n",
    "\n",
    "# Hauptschleife zur Evaluierung der Modelle\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    evaluate_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_record_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Lade die Evaluierungsdaten\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTFRecordDataset(\u001b[43mtest_record_path\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ZÃ¤hle die Anzahl der Elemente (Frames) im Dataset\u001b[39;00m\n\u001b[0;32m      5\u001b[0m num_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m eval_dataset)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_record_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Lade die Evaluierungsdaten\n",
    "eval_dataset = tf.data.TFRecordDataset(test_record_path)\n",
    "\n",
    "# ZÃ¤hle die Anzahl der Elemente (Frames) im Dataset\n",
    "num_frames = sum(1 for _ in eval_dataset)\n",
    "print(f\"Anzahl der Frames: {num_frames}\")\n",
    "\n",
    "total_time = 17.09 + 10.86  # Gesamtzeit in Sekunden\n",
    "\n",
    "# Durchschnittliche Zeit pro Frame\n",
    "time_per_frame = total_time / num_frames\n",
    "\n",
    "# FPS\n",
    "fps = 1 / time_per_frame\n",
    "print(f\"FPS: {fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_checkpoints(config_path, model_dir, checkpoints, model_name):\n",
    "    \"\"\"Evaluates all checkpoints and logs the results.\"\"\"\n",
    "    for checkpoint in checkpoints:\n",
    "        eval_command = f\"python C:/Users/Alexej/Desktop/GTSRB/models/research/object_detection/model_main_tf2.py \\\n",
    "            --pipeline_config_path={config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={checkpoint} \\\n",
    "            --run_once\"\n",
    "\n",
    "        # Starte den Evaluierungsprozess\n",
    "        process = subprocess.Popen(eval_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "        output, error = process.communicate()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f\"Error evaluating checkpoint {checkpoint}:\")\n",
    "            print(error.decode(\"utf-8\"))\n",
    "        else:\n",
    "            output_str = output.decode(\"utf-8\")\n",
    "            checkpoint_name = os.path.basename(checkpoint)\n",
    "            header = f\"Evaluation Results for MODEL_NAME: {model_name} at Checkpoint: {checkpoint_name}\\n\\n\"\n",
    "            with open(f\"./myModules/log/{model_name}_evaluation_results_{checkpoint_name}.txt\", \"w\") as f:\n",
    "                f.write(header)\n",
    "                f.write(output_str)\n",
    "            print(f\"Evaluation for checkpoint {checkpoint} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = get_checkpoints(model_dir)\n",
    "\n",
    "evaluate_checkpoints(config_path, model_dir, checkpoints, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_multiple_images(model, image, min_score_threshold):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Filter out detections with a score below the threshold\n",
    "    scores = output_dict['detection_scores']\n",
    "    high_score_indices = scores >= min_score_threshold\n",
    "\n",
    "    output_dict['detection_boxes'] = output_dict['detection_boxes'][high_score_indices]\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'][high_score_indices]\n",
    "    output_dict['detection_scores'] = output_dict['detection_scores'][high_score_indices]\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path, width, height):\n",
    "    image = Image.open(path)\n",
    "    # Resize the image to a larger size\n",
    "    image = image.resize((width, height))  # Resize to 256x256, you can change this as needed\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes_with_color(image, boxes, classes, scores, category_index, box_color, line_thickness):\n",
    "    for i in range(boxes.shape[0]):\n",
    "        if scores is None or scores[i] > 0.5:\n",
    "            class_id = int(classes[i])\n",
    "            if class_id in category_index.keys():\n",
    "                class_name = category_index[class_id]['name']\n",
    "                display_str = str(class_name)\n",
    "                color = box_color\n",
    "                vis_util.draw_bounding_box_on_image_array(\n",
    "                    image,\n",
    "                    boxes[i][0],\n",
    "                    boxes[i][1],\n",
    "                    boxes[i][2],\n",
    "                    boxes[i][3],\n",
    "                    color=color,\n",
    "                    thickness=line_thickness,\n",
    "                    display_str_list=[display_str],\n",
    "                    use_normalized_coordinates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_legend(image, groundtruth_classes, category_index, iou):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    image_height = image.size[1]\n",
    "    font_size = max(int(image_height * 0.04), 12)  # SchriftgrÃ¶ÃŸe auf 4% der BildhÃ¶he begrenzt, aber mindestens 12\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "\n",
    "    y_offset = int(0.9 * image_height)  # Startoffset fÃ¼r die Legende, z.B. 90% der BildhÃ¶he\n",
    "    x_offset = int(0.05 * image.size[0])  # Startoffset fÃ¼r die Legende, z.B. 5% der Bildbreite\n",
    "\n",
    "    # Ground Truth Legende\n",
    "    for gt_class in groundtruth_classes:\n",
    "        if gt_class in category_index:\n",
    "            class_name = category_index[gt_class]['name']\n",
    "            draw.text((x_offset, y_offset), f\"GT: {class_name}\\nIoU: {iou:.2f}\", fill=\"yellow\", font=font)\n",
    "            y_offset += int(font_size * 1.5)  # ErhÃ¶he den Offset basierend auf der aktuellen SchriftgrÃ¶ÃŸe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxA, boxB):\n",
    "    yA = max(boxA[0], boxB[0])\n",
    "    xA = max(boxA[1], boxB[1])\n",
    "    yB = min(boxA[2], boxB[2])\n",
    "    xB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    \n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    \n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    \n",
    "    image_pil = Image.fromarray(np.uint8(image_np)).convert(\"RGB\")\n",
    "    add_legend(image_pil, groundtruth_classes, category_index, iou)\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, ax):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    ax.imshow(image_np)\n",
    "    ax.axis('off')  # Hide the axis\n",
    "\n",
    "    # Adding legend to the plot\n",
    "    label_text = category_index[groundtruth_classes[0]]['name']\n",
    "    ax.set_title(f'{label_text}\\nIoU: {iou:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_images_for_labels(df, model, category_index, discard_predictions_below_acc, num_images_to_display):\n",
    "    sorted_labels = sorted(df['Label'].unique())\n",
    "\n",
    "    num_rows = len(sorted_labels)\n",
    "    num_cols = num_images_to_display\n",
    "\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols + num_cols * 0.5, num_rows * 2))\n",
    "    \n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, num_cols)\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for idx, label in enumerate(sorted_labels):\n",
    "        label_df = df[df['Label'] == label]\n",
    "        \n",
    "        random_indices = random.sample(range(len(label_df)), min(num_images_to_display, len(label_df)))\n",
    "\n",
    "        for jdx, r_idx in enumerate(random_indices):\n",
    "            row = label_df.iloc[r_idx]\n",
    "            image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "            groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "            groundtruth_classes = [row['Label']]\n",
    "            \n",
    "            output_dict = run_inference_for_multiple_images(model, image_np, discard_predictions_below_acc)\n",
    "            \n",
    "            plot_idx = idx * num_images_to_display + jdx\n",
    "            \n",
    "            visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, axes[plot_idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_predictions_with_groundtruth (model, df, num_images_to_display, category_index):\n",
    "    \n",
    "    random_indices = random.sample(range(len(df)), num_images_to_display)\n",
    "\n",
    "    # Visualisiere zufÃ¤llige ausgewÃ¤hlte Bilder\n",
    "    for idx in random_indices:\n",
    "        row = df.iloc[idx]\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_endless_predictions(model, df, category_index):\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_endless_predictions_from_path(model, path, category_index):\n",
    "    \n",
    "    for image_path in glob.glob(path):\n",
    "        image_np = load_image_into_numpy_array(image_path, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=1)\n",
    "        display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\exporter_main_v2.py \\\n",
    "    --trained_checkpoint_dir {model_dir} \\\n",
    "    --inference_path {inference_path} \\\n",
    "    --config_path {config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load(f'./{inference_path}/saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inference Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = './GTSRB/Final_Test/Images/*.ppm'\n",
    "min_score_threshold = 0.6\n",
    "number_of_images_to_display = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_endless_predictions_from_path(model, image_path, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_endless_predictions(model, df_final_test, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_predictions_with_groundtruth(model, df_final_test, number_of_images_to_display, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_images_for_labels(df_final_test, model, category_index, min_score_threshold, number_of_images_to_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation (discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_labels = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "for idx, row in df_test_raw.iterrows():\n",
    "    row = df_test_raw.iloc[idx]\n",
    "    image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "    groundtruth_classes = [row['Label']]\n",
    "    \n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "    \n",
    "    predicted_class = output_dict['detection_classes'][0]\n",
    "    all_predicted_labels.append(predicted_class)\n",
    "    all_true_labels.append(groundtruth_classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
    "recall = recall_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "precision = precision_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "f1 = f1_score(all_true_labels, all_predicted_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Vorhersage')\n",
    "plt.ylabel('Wahre Werte')\n",
    "plt.title('Konfusionsmatrix')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_true_labels, all_predicted_labels, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-Time detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(TRAINED_CHECKPOINT_PATH, 'ckpt-8')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load('./inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Laden des Modells\n",
    "def load_model(model_path):\n",
    "    saved_model = tf.saved_model.load(model_path)\n",
    "    return saved_model\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "model_path = './inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model'\n",
    "detection_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Erstellen des Detektionsmodells und Wiederherstellen des Checkpoints\n",
    "def load_model():\n",
    "    configs = tf.compat.v2.saved_model.load('./inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model')\n",
    "    return configs['model']\n",
    "\n",
    "detection_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=10,\n",
    "                min_score_thresh=.9,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 800)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video capture testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum Eingabevideo und zum Ausgabevideo\n",
    "input_video_path = 'path_to_your_video.mp4'\n",
    "output_video_path = 'path_to_output_video.mp4'\n",
    "\n",
    "# VideoCapture-Objekt erstellen\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# ÃœberprÃ¼fen, ob das Video geÃ¶ffnet werden kann\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Video-Eigenschaften abrufen\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# VideoWriter-Objekt erstellen\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Beenden der Schleife, wenn das Video zu Ende ist\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes sollten ints sein.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=10,\n",
    "                min_score_thresh=.9,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    # Frame mit Detektionen in das Ausgabevideo schreiben\n",
    "    out.write(image_np_with_detections)\n",
    "\n",
    "    # Optional: Zeige das Video mit Detektionen in einem Fenster an\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 800)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Ressourcen freigeben\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video capture testing end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(frame, axis=0), dtype=tf.float32)\n",
    "    \n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'],\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=100,  # Anzahl der maximal zu zeichnenden Boxen\n",
    "        min_score_thresh=0.5    # Minimale Vertrauensschwelle fÃ¼r die Anzeige\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    \n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Laden und Konfigurieren des Modells\n",
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Wiederherstellen des Checkpoints\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(TRAINED_CHECKPOINT_PATH, 'ckpt-9')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)\n",
    "\n",
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    \n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.9,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', cv2.resize(image_np, (800, 800)))\n",
    "    \n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Videoaufnahme von der Kamera starten (Kamera 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# ÃœberprÃ¼fen, ob die Kamera erfolgreich geÃ¶ffnet wurde\n",
    "if not cap.isOpened():\n",
    "    print(\"Fehler beim Ã–ffnen der Kamera\")\n",
    "    exit()\n",
    "\n",
    "# Unendlich Schleife, um Frames von der Kamera zu lesen und anzuzeigen\n",
    "while True:\n",
    "    # Lesen eines einzelnen Frames von der Kamera\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # ÃœberprÃ¼fen, ob das Frame erfolgreich gelesen wurde\n",
    "    if not ret:\n",
    "        print(\"Fehler beim Lesen des Frames\")\n",
    "        break\n",
    "    \n",
    "    # Anzeigen des Frames in einem Fenster\n",
    "    cv2.imshow('Videoaufnahme', frame)\n",
    "    \n",
    "    # Beenden der Schleife bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Freigeben der Videoquelle und SchlieÃŸen aller Fenster\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_CONFIG_PATH = \"./myModules/configs/Faster_RCNN_640_50_fixed_config.config\"\n",
    "TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\Faster_RCNN_640_50_fixed'\n",
    "TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Wiederherstellen des Checkpoints\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(TRAINED_CHECKPOINT_PATH, 'ckpt-9')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    #input_tensor = input_tensor[tf.newaxis,...]\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference(frame):\n",
    "    \n",
    "    image_np = np.array(frame)\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes']+1,\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=10,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "           \n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    cv2.imshow('Object Detection', frame_with_detections)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zugriff auf die Kamera (0 fÃ¼r die Standardkamera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Variablen zur FPS-Berechnung\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "fps = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # FÃ¼hre die Objekterkennung durch\n",
    "    frame_with_detections = show_inference(frame)\n",
    "\n",
    "    # Berechne die FPS\n",
    "    frame_count += 1\n",
    "    if (time.time() - start_time) > 1:  # Ein Sekundenintervall\n",
    "        fps = frame_count / (time.time() - start_time)\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    # Zeige die FPS auf dem Frame\n",
    "    cv2.putText(frame_with_detections, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Zeige den Frame mit den Erkennungsergebnissen\n",
    "    cv2.imshow('Object Detection', frame_with_detections)\n",
    "\n",
    "    # Beenden durch DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release der Kamera und SchlieÃŸen aller Fenster\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kamera-Kalibrierungsparameter (mÃ¼ssen angepasst werden)\n",
    "f = 800  # Brennweite in Pixeln\n",
    "objekt_breite = 0.5  # tatsÃ¤chliche Breite des Objekts in Metern\n",
    "\n",
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Entfernung berechnen und Ergebnisse anpassen\n",
    "    for i in range(num_detections):\n",
    "        box = detections['detection_boxes'][i]\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        box_width = xmax - xmin\n",
    "        box_height = ymax - ymin\n",
    "\n",
    "        # Berechne die GrÃ¶ÃŸe des Objekts in Pixeln\n",
    "        box_width_pixels = box_width * frame.shape[1]\n",
    "        box_height_pixels = box_height * frame.shape[0]\n",
    "\n",
    "        # Berechne die Entfernung zum Objekt (angenommene GrÃ¶ÃŸe des Objekts)\n",
    "        entfernung = (objekt_breite * f) / box_width_pixels\n",
    "\n",
    "        # Hier kannst du die Erkennungsergebnisse basierend auf der Entfernung anpassen\n",
    "        # Beispiel: Du kannst die Boxen oder Scores basierend auf der Entfernung anpassen\n",
    "\n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'] + 1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.3,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Definiere Skalierungsfaktoren\n",
    "scales = [0.25, 1.0, 1.5]\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Initialisiere eine Liste, um alle Erkennungen zu speichern\n",
    "    all_detections = []\n",
    "\n",
    "    for scale in scales:\n",
    "        # Skaliere das Bild\n",
    "        height, width, _ = frame.shape\n",
    "        new_size = (int(width * scale), int(height * scale))\n",
    "        resized_frame = cv2.resize(frame, new_size)\n",
    "\n",
    "        # Konvertieren des Frames zu einem Tensor\n",
    "        image_np = np.array(resized_frame)\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "        # DurchfÃ¼hrung der Objekterkennung\n",
    "        detections = detect_fn(input_tensor)\n",
    "\n",
    "        # Verarbeitung der erkannten Objekte\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "        detections['num_detections'] = num_detections\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        # Berechne die Skalierungskorrektur fÃ¼r die Box-Koordinaten\n",
    "        height_scale = height / new_size[1]\n",
    "        width_scale = width / new_size[0]\n",
    "        detections['detection_boxes'][:, [0, 2]] *= height_scale\n",
    "        detections['detection_boxes'][:, [1, 3]] *= width_scale\n",
    "\n",
    "        # Speichere die Ergebnisse\n",
    "        all_detections.append(detections)\n",
    "\n",
    "    # Kombiniere alle Erkennungen\n",
    "    # Du kannst hier die Detections nach deinem Bedarf kombinieren oder zusammenfÃ¼hren\n",
    "\n",
    "    # Verwende die Detections von der letzten Skala fÃ¼r die Visualisierung (oder alle)\n",
    "    final_detections = all_detections[-1]  # Hier nehmen wir die Erkennung von der letzten Skala\n",
    "\n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        final_detections['detection_boxes'],\n",
    "        final_detections['detection_classes'] + 1,\n",
    "        final_detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.3,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Videoaufnahme starten\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[0;32m      4\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    \n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.3,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', cv2.resize(image_np, (640, 640)))\n",
    "    \n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Skalieren des Frames auf 640x640\n",
    "    frame_resized = cv2.resize(frame, (640, 640))\n",
    "    \n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame_resized.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame_resized = cv2.cvtColor(frame_resized, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame_resized)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    \n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.9,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', cv2.resize(image_np, (800, 800)))\n",
    "    \n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Skalieren des Frames auf die EingabegrÃ¶ÃŸe des Modells (640x640)\n",
    "    frame_resized = cv2.resize(frame, (640, 640))\n",
    "    \n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame_resized.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame_resized = cv2.cvtColor(frame_resized, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame_resized)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    \n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Skalieren der Bounding Boxes auf die OriginalgrÃ¶ÃŸe des Frames\n",
    "    h, w, _ = frame.shape\n",
    "    scale_x, scale_y = w / 640, h / 640\n",
    "    \n",
    "    detections['detection_boxes'][:, 0] *= h  # ymin\n",
    "    detections['detection_boxes'][:, 1] *= w  # xmin\n",
    "    detections['detection_boxes'][:, 2] *= h  # ymax\n",
    "    detections['detection_boxes'][:, 3] *= w  # xmax\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=False,  # Normalisierte Koordinaten nicht mehr verwenden\n",
    "        max_boxes_to_draw=10,\n",
    "        min_score_thresh=0.9,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', cv2.resize(image_np, (800, 800)))\n",
    "    \n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TFLite for Jetson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 14:57:40.540812: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 14:57:40.640632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "I0724 14:57:54.295555 21352 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 14:57:58.380026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-07-24 14:57:58.412291: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "I0724 14:57:59.958792 21352 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 14:58:01.014157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "I0724 14:58:03.325321 21352 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 14:58:04.338228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001F2214C5610>, because it is not built.\n",
      "W0724 14:58:04.795488 21352 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001F2214C5610>, because it is not built.\n",
      "W0724 14:58:36.381130 21352 save.py:269] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 329). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\assets\n",
      "I0724 14:58:48.019352 21352 builder_impl.py:779] Assets written to: D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\export_tflite_graph_tf2.py \\\n",
    "    --pipeline_config_path={config_path} \\\n",
    "    --trained_checkpoint_dir={TRAINED_CHECKPOINT_PATH} \\\n",
    "    --output_directory={model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZEN_TFLITE_PATH = os.path.join(model_dir, 'saved_model')\n",
    "TFLITE_MODEL = os.path.join(model_dir, 'saved_model', 'detect.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\ D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\detect.tflite\n"
     ]
    }
   ],
   "source": [
    "print(saved_model_dir, TFLITE_MODEL)\n",
    "D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}\\\\saved_model\\\\'.format(name=MODEL_NAME)\n",
    "tflite_model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}\\\\saved_model\\\\detect.tflite'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"tflite_convert \\\n",
    "--saved_model_dir={} \\\n",
    "--output_file={} \\\n",
    "--input_shapes=1,300,300,3 \\\n",
    "--input_arrays=normalized_input_image_tensor \\\n",
    "--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
    "--inference_type=FLOAT \\\n",
    "--allow_custom_ops\".format(FROZEN_TFLITE_PATH, TFLITE_MODEL, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_convert --saved_model_dir=Tensorflow\\workspace\\models\\my_ssd_mobnet\\tfliteexport\\saved_model --output_file=Tensorflow\\workspace\\models\\my_ssd_mobnet\\tfliteexport\\saved_model\\detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --allow_custom_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\Scripts\\tflite_convert.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 692, in main\n",
      "    app.run(main=run_main, argv=sys.argv[:1])\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\absl\\app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\absl\\app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 675, in run_main\n",
      "    _convert_tf2_model(tflite_flags)\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 276, in _convert_tf2_model\n",
      "    converter = lite.TFLiteConverterV2.from_saved_model(\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1789, in from_saved_model\n",
      "    saved_model = _load(saved_model_dir, tags)\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 800, in load\n",
      "    result = load_partial(export_dir, None, tags, options)[\"root\"]\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 905, in load_partial\n",
      "    loader_impl.parse_saved_model_with_debug_info(export_dir))\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 57, in parse_saved_model_with_debug_info\n",
      "    saved_model = parse_saved_model(export_dir)\n",
      "  File \"C:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 115, in parse_saved_model\n",
      "    raise IOError(\n",
      "OSError: SavedModel file does not exist at: saved_model_dir\\{saved_model.pbtxt|saved_model.pb}\n"
     ]
    }
   ],
   "source": [
    "!tflite_convert --saved_model_dir=saved_model_dir --output_file=tflite_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 15:28:29.020734: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 15:28:29.135414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "I0724 15:28:35.269696  8296 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 15:28:39.181165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-07-24 15:28:39.216993: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "I0724 15:28:40.493435  8296 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 15:28:41.579417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "I0724 15:28:43.717420  8296 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-24 15:28:44.665618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001FE91CD5700>, because it is not built.\n",
      "W0724 15:28:45.100864  8296 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001FE91CD5700>, because it is not built.\n",
      "W0724 15:29:14.464308  8296 save.py:269] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 329). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: tf_lite\\saved_model\\assets\n",
      "I0724 15:29:24.435737  8296 builder_impl.py:779] Assets written to: tf_lite\\saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "output_directory = 'tf_lite'\n",
    "\n",
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\export_tflite_graph_tf2.py \\\n",
    "    --trained_checkpoint_dir={TRAINED_CHECKPOINT_PATH} \\\n",
    "    --output_directory={output_directory} \\\n",
    "    --pipeline_config_path={config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 219.390 G  ops, equivalently 109.695 G  MACs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 15:33:24.241856: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 15:33:24.298424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-07-24 15:33:41.946710: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2024-07-24 15:33:41.947009: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2024-07-24 15:33:41.947927: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\\n",
      "2024-07-24 15:33:42.072791: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2024-07-24 15:33:42.073026: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\\n",
      "2024-07-24 15:33:42.412829: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2024-07-24 15:33:42.473342: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2024-07-24 15:33:44.054861: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\\n",
      "2024-07-24 15:33:44.559123: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 2611192 microseconds.\n",
      "2024-07-24 15:33:45.703183: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-24 15:33:47.845816: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1989] Estimated count of arithmetic ops: 219.390 G  ops, equivalently 109.695 G  MACs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tflite_convert --saved_model_dir=D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\ --output_file=D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\model.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\detect.tflite D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\\n"
     ]
    }
   ],
   "source": [
    "print(tflite_model_dir, saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tflite_convert --saved_model_dir=saved_model_dir --output_file=tflite_model_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
