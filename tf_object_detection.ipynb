{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import cv2 \n",
    "import subprocess\n",
    "import time\n",
    "import uuid\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as minidom\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config_path = './pipeline.config'\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(base_config_path)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "print(\"Model was successfully built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for gpu\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "   raise SystemError('GPU device not found')\n",
    "\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(21)\n",
    "tf.compat.v1.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id_mapping:\n",
    "    # 1:'Speed limit (30km/h)', \n",
    "    # 2:'Speed limit (50km/h)',\n",
    "    # 12:'Priority road', \n",
    "    # 14:'Stop', \n",
    "    # 17:'No entry',\n",
    "    # 41:'Ende des Ãœberholverbots',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE_TRAIN = 50 # Default: 50 Images of each class excluding values in :id_mapping. Used for Trainset\n",
    "SAMPLE_SIZE_TEST = 10 # Default: 10 Images of each class excluding values in :id_mapping . Used for Testset\n",
    "\n",
    "IMAGE_WIDTH = 280 # Image width in pixels to resize for visualization\n",
    "IMAGE_HEIGHT = 280 # Image height in pixels to resize for visualization\n",
    "\n",
    "train_path = './GTSRB/Final_Training/Images' # Location of the training images\n",
    "test_path = './GTSRB/Final_Test/Images' # Location of the test images\n",
    "\n",
    "id_mapping = {1: 1, 2: 2, 12: 3, 14: 4, 17: 5}\n",
    "unknown_label = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"efficientdet_d4_coco17_tpu-32\"\n",
    "MODEL_NAME = \"Efficientdet_12000_1024_d4\"\n",
    "IS_SSD = True # True if SSD model is used. False otherwise \n",
    "TRANSERLEARNING = False\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "NUM_CLASSES = 5\n",
    "NUM_STEPS = 13000\n",
    "NUM_EVAL_STEPS = 1000\n",
    "use_bfloat16 = False # Use bfloat16 True for TPU\n",
    "\n",
    "NOTES = \"\" # Training Notes for the log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for Evaluation\n",
    "if IS_SSD:\n",
    "    TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=MODEL_NAME)\n",
    "    TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)\n",
    "    TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    MODEL_TYPE = \"ssd\"\n",
    "else:\n",
    "    TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=MODEL_NAME)\n",
    "    TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "    TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    MODEL_TYPE = \"faster_rcnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSERLEARNING:\n",
    "        base_config_path = \"./inference/faster_rcnn/{name}/pipeline.config\".format(name=BASE_MODEL)\n",
    "        base_checkpoint_path = \"./inference/faster_rcnn/{name}/checkpoint/ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "        # Save to path\n",
    "        model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "        config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "        # Upload from path\n",
    "        labelmap_path = \"./myModules/label_map_short.pbtxt\"\n",
    "        train_record_path = \"./myModules/records/train.record\"\n",
    "        test_record_path = \"./myModules/records/test.record\"\n",
    "        \n",
    "        inference_path = './inference/faster_rcnn/{name}'.format(name=MODEL_NAME)\n",
    "else:\n",
    "    if IS_SSD:\n",
    "        # Download from path \n",
    "        base_config_path = \"./base_models/ssd/{name}/pipeline.config\".format(name=BASE_MODEL)\n",
    "        base_checkpoint_path = \"./base_models/ssd/{name}/checkpoint/ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "        # Save to path\n",
    "        model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)\n",
    "        config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "        # Upload from path\n",
    "        labelmap_path = \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "        train_record_path = \"./myModules/records/trainWoUnknown.record\"\n",
    "        test_record_path = \"./myModules/records/testWoUnknown.record\"\n",
    "        \n",
    "        inference_path = './inference/ssd/{name}'.format(name=MODEL_NAME)\n",
    "    else:\n",
    "        base_config_path = \"D:\\\\Desktop-Short\\\\base_models\\\\faster_rcnn\\\\{name}\\\\pipeline.config\".format(name=BASE_MODEL)\n",
    "        base_checkpoint_path = \"D:\\\\Desktop-Short\\\\base_models\\\\{name}\\\\checkpoint\\\\ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "        # Save to path\n",
    "        model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "        config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "        # Upload from path\n",
    "        labelmap_path = \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "        train_record_path = \"./myModules/records/trainWoUnknown.record\"\n",
    "        test_record_path = \"./myModules/records/testWoUnknown.record\"\n",
    "        \n",
    "        inference_path = './inference/faster_rcnn/{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTrain(rootpath, cache_path):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "    Arguments:\n",
    "        rootpath: path to the traffic sign data, for example './GTSRB/Training'\n",
    "        cache_path: path to the cached DataFrame file, default is 'traffic_signs_data.pkl'\n",
    "    Returns:\n",
    "        DataFrame containing labels, image shapes, ROIs, and image paths\n",
    "    '''\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading: {cache_path}\")\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    data = []\n",
    "    # loop over all 43 classes\n",
    "    for c in range(0, 43):\n",
    "        prefix = f\"{rootpath}/{c:05d}/\"  # subdirectory for class\n",
    "        gtFile = open(prefix + f'GT-{c:05d}.csv')  # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';')  # csv parser for annotations file\n",
    "        next(gtReader)  # skip header\n",
    "\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            img_path = prefix + row[0]\n",
    "            img = plt.imread(img_path)  # load image\n",
    " \n",
    "            label = int(row[7])  # the 8th column is the label\n",
    "            height = img.shape[0]  # height of the image\n",
    "            width = img.shape[1]  # width of the image\n",
    "            #channels = img.shape[2] if len(img.shape) > 2 else 1  # channels of the image (default to 1 if grayscale)\n",
    "            roi_x1 = int(row[3])  # ROI X1 coordinate\n",
    "            roi_y1 = int(row[4])  # ROI Y1 coordinate\n",
    "            roi_x2 = int(row[5])  # ROI X2 coordinate\n",
    "            roi_y2 = int(row[6])  # ROI Y2 coordinate\n",
    "\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "\n",
    "        gtFile.close()\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    \n",
    "    df.to_pickle(cache_path)\n",
    "    print(f\"Dataframe saved in: {cache_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTest(rootpath, cache_path):\n",
    "    '''Reads the final test data for the German Traffic Sign Recognition Benchmark.\n",
    "    Arguments: path to the final test data, for example './GTSRB/Final_Test/Images'\n",
    "    Returns: DataFrame with image data, labels, image shapes, and ROI\n",
    "    '''\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading: {cache_path}\")\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    # Pfad zur CSV-Datei\n",
    "    csv_file = os.path.join(rootpath, 'GT-final_test_gt.csv')\n",
    "    \n",
    "    # Lade die CSV-Datei\n",
    "    df = pd.read_csv(csv_file, delimiter=';')\n",
    "    \n",
    "    # Liste zum Speichern der Daten\n",
    "    data = []\n",
    "    \n",
    "    # Schleife Ã¼ber alle Zeilen der CSV-Datei\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.join(rootpath, row['Filename'])\n",
    "        img = plt.imread(img_path)  # Lade das Bild\n",
    "        \n",
    "        if img is not None:\n",
    "            label = int(row['ClassId'])\n",
    "            height, width, channels = img.shape\n",
    "            roi_x1 = int(row['Roi.X1'])\n",
    "            roi_y1 = int(row['Roi.Y1'])\n",
    "            roi_x2 = int(row['Roi.X2'])\n",
    "            roi_y2 = int(row['Roi.Y2'])\n",
    "            \n",
    "            # FÃ¼ge die Daten als Zeile hinzu\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "        else:\n",
    "            print(f\"Bild {img_path} konnte nicht geladen werden.\")\n",
    "    \n",
    "    # Erstelle ein DataFrame aus der Liste\n",
    "    df_test = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    df_test.to_pickle(cache_path)\n",
    "    print(f\"Dataframe saved in: {cache_path}\")\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))  # Resize to 280x280\n",
    "    return np.array(image).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_labels(df, selected_labels, unknown_label, sample_size):\n",
    "    \"\"\"\n",
    "    Reassign labels in the dataframe according to selected_labels.\n",
    "    Keep all rows for the labels in selected_labels.\n",
    "    For remaining labels, sample a specified number of rows and reassign them to unknown_label.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with a column named 'Label'.\n",
    "    selected_labels (dict): A dictionary mapping old labels to new labels.\n",
    "    unknown_label (int): The label to assign to the remaining sampled rows.\n",
    "    sample_size (int): The number of rows to sample for each remaining label. Default is 15.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new dataframe with reassigned labels.\n",
    "    \"\"\"\n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    for old_label, new_label in selected_labels.items():\n",
    "        label_df = df[df['Label'] == old_label].copy()\n",
    "        label_df['Label'] = new_label\n",
    "        new_df = pd.concat([new_df, label_df])\n",
    "\n",
    "    unique_labels = df['Label'].unique()\n",
    "    remaining_labels = [label for label in unique_labels if label not in selected_labels]\n",
    "\n",
    "    for label in remaining_labels:\n",
    "        label_df = df[df['Label'] == label]\n",
    "        if len(label_df) > sample_size:\n",
    "            selected_rows = label_df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            selected_rows = label_df\n",
    "        selected_rows['Label'] = unknown_label\n",
    "        new_df = pd.concat([new_df, selected_rows])\n",
    "\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Dataset for YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN_DF = \"./myModules/data/df_train_raw.pkl\"\n",
    "PATH_TO_TEST_DF = \"./myModules/data/df_test_raw.pkl\"\n",
    "\n",
    "PATH_TO_TRAIN_ANNOTATIONS = './yoloNoUnkData/Train/Annotations/'\n",
    "PATH_TO_TRAIN_IMAGES = './yoloNoUnkData/Train/Images/'\n",
    "\n",
    "PATH_TO_TEST_ANNOTATIONS = './yoloNoUnkData/Test/Annotations/'\n",
    "PATH_TO_TEST_IMAGES = './yoloNoUnkData/Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_xml(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ET.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voc_annotation_for_trainset(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        annotation = ET.Element('annotation')\n",
    "        \n",
    "        # Extract subfolder name (e.g., '0000')\n",
    "        subfolder_name = os.path.basename(os.path.dirname(row['Path']))\n",
    "        \n",
    "        # Create unique filename with subfolder prefix (e.g., '0000_00000_00000.xml')\n",
    "        os.path.basename(row['Path']).replace('.ppm', '.jpg')\n",
    "        filename = f\"{subfolder_name}_{os.path.splitext(os.path.basename(row['Path']))[0]}.xml\"\n",
    "        \n",
    "        # Create annotation elements\n",
    "        ET.SubElement(annotation, 'folder').text = 'Images'\n",
    "        ET.SubElement(annotation, 'filename').text = filename.replace('.xml', '.jpg')\n",
    "        \n",
    "        size = ET.SubElement(annotation, 'size')\n",
    "        ET.SubElement(size, 'width').text = str(row['Width'])\n",
    "        ET.SubElement(size, 'height').text = str(row['Height'])\n",
    "        ET.SubElement(size, 'depth').text = '3'  # Assuming RGB images\n",
    "\n",
    "        segmented = ET.SubElement(annotation, 'segmented').text = '0'\n",
    "        \n",
    "        obj = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(obj, 'name').text = str(row['Label'])\n",
    "        ET.SubElement(obj, 'pose').text = 'Unspecified'\n",
    "        ET.SubElement(obj, 'truncated').text = '0'\n",
    "        ET.SubElement(obj, 'difficult').text = '0'\n",
    "        \n",
    "        bndbox = ET.SubElement(obj, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(row['Roi.X1'])\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(row['Roi.Y1'])\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(row['Roi.X2'])\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(row['Roi.Y2'])\n",
    "        \n",
    "        # Write XML file with pretty print\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(prettify_xml(annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voc_annotation_for_testset(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        annotation = ET.Element('annotation')\n",
    "        ET.SubElement(annotation, 'folder').text = 'Images'\n",
    "        ET.SubElement(annotation, 'filename').text = os.path.basename(row['Path']).replace('.ppm', '.jpg')\n",
    "        \n",
    "        size = ET.SubElement(annotation, 'size')\n",
    "        ET.SubElement(size, 'width').text = str(row['Width'])\n",
    "        ET.SubElement(size, 'height').text = str(row['Height'])\n",
    "        ET.SubElement(size, 'depth').text = '3'  # Assuming RGB images\n",
    "\n",
    "        segmented = ET.SubElement(annotation, 'segmented').text = '0'\n",
    "        \n",
    "        obj = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(obj, 'name').text = str(row['Label'])\n",
    "        ET.SubElement(obj, 'pose').text = 'Unspecified'\n",
    "        ET.SubElement(obj, 'truncated').text = '0'\n",
    "        ET.SubElement(obj, 'difficult').text = '0'\n",
    "        \n",
    "        bndbox = ET.SubElement(obj, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(row['Roi.X1'])\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(row['Roi.Y1'])\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(row['Roi.X2'])\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(row['Roi.Y2'])\n",
    "        \n",
    "        # Write XML file with pretty print\n",
    "        output_file = os.path.join(output_dir, os.path.basename(row['Path']).replace('.ppm', '.xml'))\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(prettify_xml(annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ppm_to_jpg_from_df_train(df, jpg_root):\n",
    "    if not os.path.exists(jpg_root):\n",
    "        os.makedirs(jpg_root)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        img_path = row['Path']\n",
    "        if img_path.endswith('.ppm'):\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            folder_name = os.path.basename(os.path.dirname(img_path))  # Subordner\n",
    "            filename = os.path.basename(img_path).replace('.ppm', '.jpg')\n",
    "            jpg_filename = f\"{folder_name}_{filename}\"  \n",
    "            \n",
    "            jpg_path = os.path.join(jpg_root, jpg_filename)\n",
    "\n",
    "            img.save(jpg_path, 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ppm_to_jpg_from_df(df, jpg_dir):\n",
    "    if not os.path.exists(jpg_dir):\n",
    "        os.makedirs(jpg_dir)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        img_path = row['Path']\n",
    "        if img_path.endswith('.ppm'):\n",
    "            img = Image.open(img_path)\n",
    "            # Extrahiere den Dateinamen und den Unterordner aus dem Pfad\n",
    "            subfolder = os.path.basename(os.path.dirname(img_path))\n",
    "            filename = os.path.basename(img_path).replace('.ppm', '.jpg')\n",
    "            # Erstelle den Zielpfad inklusive Unterordner\n",
    "            jpg_subdir = os.path.join(jpg_dir, subfolder)\n",
    "            if not os.path.exists(jpg_subdir):\n",
    "                os.makedirs(jpg_subdir)\n",
    "            jpg_path = os.path.join(jpg_subdir, filename)\n",
    "            img.save(jpg_path, 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = readTrafficSignsTrain(train_path, PATH_TO_TRAIN_DF)\n",
    "create_voc_annotation_for_trainset(df_final_train, PATH_TO_TRAIN_ANNOTATIONS)\n",
    "convert_ppm_to_jpg_from_df_train(df_final_train, PATH_TO_TRAIN_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = readTrafficSignsTest(test_path, PATH_TO_TEST_DF)\n",
    "create_voc_annotation_for_testset(df_final_test, PATH_TO_TEST_ANNOTATIONS)\n",
    "convert_ppm_to_jpg_from_df(df_final_test, PATH_TO_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./myModules/data/df_train_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "df_train_cache = readTrafficSignsTrain(train_path, \"./myModules/data/df_train_raw.pkl\")\n",
    "df_train = pd.DataFrame()\n",
    "df_train = df_train_cache.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./myModules/data/df_test_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "df_test_cache = readTrafficSignsTest(test_path, \"./myModules/data/df_test_raw.pkl\")\n",
    "df_test = pd.DataFrame()\n",
    "df_test = df_test_cache.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_train = reassign_labels(df_train, id_mapping, unknown_label, SAMPLE_SIZE_TRAIN)\n",
    "df_final_test = reassign_labels(df_test, id_mapping, unknown_label, SAMPLE_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to exlude the unknown Labels\n",
    "df_final_train = df_final_train[df_final_train['Label'] != 6]\n",
    "df_final_test = df_final_test[df_final_test['Label'] != 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2790 entries, 0 to 2789\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Width   2790 non-null   int64 \n",
      " 1   Height  2790 non-null   int64 \n",
      " 2   Roi.X1  2790 non-null   int64 \n",
      " 3   Roi.Y1  2790 non-null   int64 \n",
      " 4   Roi.X2  2790 non-null   int64 \n",
      " 5   Roi.Y2  2790 non-null   int64 \n",
      " 6   Path    2790 non-null   object\n",
      " 7   Label   2790 non-null   int64 \n",
      "dtypes: int64(7), object(1)\n",
      "memory usage: 196.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8460 entries, 0 to 8459\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Width   8460 non-null   int64 \n",
      " 1   Height  8460 non-null   int64 \n",
      " 2   Roi.X1  8460 non-null   int64 \n",
      " 3   Roi.Y1  8460 non-null   int64 \n",
      " 4   Roi.X2  8460 non-null   int64 \n",
      " 5   Roi.Y2  8460 non-null   int64 \n",
      " 6   Path    8460 non-null   object\n",
      " 7   Label   8460 non-null   int64 \n",
      "dtypes: int64(7), object(1)\n",
      "memory usage: 594.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Verfify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "short_classes = { 1: 'Geschwindigkeitsbegrenzung (30km/h)', \n",
    "                  2: 'Geschwindigkeitsbegrenzung (50km/h)', \n",
    "                  3: 'VorrangstraÃŸe', \n",
    "                  4: 'Stop', \n",
    "                  5: 'Einfahrt verboten',\n",
    "                  6: 'Unbekannt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_train['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_test['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_label_df = df_final_train[df_final_train['Label'] == 6]\n",
    "\n",
    "# WÃ¤hle zufÃ¤llig 100 Bilder aus\n",
    "image_paths_to_display = unknown_label_df['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "# Plotten der ausgewÃ¤hlten Bilder\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_final = df_final_train['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_final)\n",
    "\n",
    "label_counts_named_final = {short_classes[key]: value for key, value in label_counts_final.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_final.keys()), y=list(label_counts_named_final.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen fÃ¼r bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_test = df_final_test['Label'].value_counts().sort_index()\n",
    "label_counts_train = df_final_train['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_train = {short_classes[key]: value for key, value in label_counts_train.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_train.keys()), y=list(label_counts_named_train.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen fÃ¼r bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_test = {short_classes[key]: value for key, value in label_counts_test.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_test.keys()), y=list(label_counts_named_test.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Testdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen fÃ¼r bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Tf-records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(row):\n",
    "    img_path = row['Path']\n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    \n",
    "    with io.BytesIO() as output:\n",
    "        image.save(output, format=\"JPEG\")\n",
    "        encoded_jpg = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = row['Path'].encode('utf8')\n",
    "    image_format = b'jpeg'\n",
    "    xmins = [row['Roi.X1'] / width]\n",
    "    xmaxs = [row['Roi.X2'] / width]\n",
    "    ymins = [row['Roi.Y1'] / height]\n",
    "    ymaxs = [row['Roi.Y2'] / height]\n",
    "    classes_text = [str(row['Label']).encode('utf8')]\n",
    "    classes = [int(row['Label'])]\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_jpg])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n",
    "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n",
    "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n",
    "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n",
    "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n",
    "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tf_record(df, output_path):\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    for _, row in df.iterrows():\n",
    "        tf_example = create_tf_example(row)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tf_record(df_final_train, './myModules/records/trainWoUnknown.record')\n",
    "create_tf_record(df_final_test, './myModules/records/testWoUnknown.record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Verify Tensord records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_example(serialized_example):\n",
    "    feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/class/text': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(record_file):\n",
    "    raw_dataset = tf.data.TFRecordDataset(record_file)\n",
    "    parsed_dataset = raw_dataset.map(parse_tf_example)\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_at_index(parsed_dataset, index):\n",
    "    for i, tf_example in enumerate(parsed_dataset):\n",
    "        if i == index:\n",
    "            height = tf_example['image/height'].numpy()\n",
    "            width = tf_example['image/width'].numpy()\n",
    "            encoded_image = tf_example['image/encoded'].numpy()\n",
    "            xmin = tf_example['image/object/bbox/xmin'].numpy()\n",
    "            xmax = tf_example['image/object/bbox/xmax'].numpy()\n",
    "            ymin = tf_example['image/object/bbox/ymin'].numpy()\n",
    "            ymax = tf_example['image/object/bbox/ymax'].numpy()\n",
    "            label = tf_example['image/object/class/label'].numpy()\n",
    "\n",
    "            image = tf.image.decode_jpeg(encoded_image)\n",
    "            image_np = image.numpy()\n",
    "\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(image_np)\n",
    "            plt.title(f'Label: {label}')\n",
    "\n",
    "            # Draw the bounding box\n",
    "            plt.gca().add_patch(plt.Rectangle((xmin * width, ymin * height), \n",
    "                                              (xmax - xmin) * width, (ymax - ymin) * height,\n",
    "                                              edgecolor='green', facecolor='none', linewidth=2))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record = read_tfrecord('./myModules/records/train.record')\n",
    "test_record = read_tfrecord('./myModules/records/test.record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_at_index(test_record, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_at_index(train_record, 9000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_length(df):\n",
    "    count_images = len(df)\n",
    "    labels_count = df['Label'].value_counts()\n",
    "\n",
    "    sorted_labels_count = labels_count.sort_index()\n",
    "    return sorted_labels_count.tolist(), count_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_info(batch, classes, steps, eval_steps, name, bfloat16, pipeline, \n",
    "                   checkpoint_path, labelmap, train_record, test_record, config, \n",
    "                   trained_model, train_time, df_train, df_test, notes):\n",
    "    \n",
    "    \n",
    "    test_count, test_len = count_length(df_test)\n",
    "    train_count, train_len = count_length(df_train)   \n",
    "    log_contents = f\"\"\"\n",
    "Trainingparameters:\n",
    "    BATCH_SIZE = {batch} # Cannot be higher than 2-4 (lack of ressources)\n",
    "    NUM_CLASSES = {classes} # Total number of classes to train is 43 - used for training are only 6-7\n",
    "    NUM_STEPS = {steps}\n",
    "    NUM_EVAL_STEPS = {eval_steps}\n",
    "    MODEL_NAME = {name}\n",
    "    use_bfloat16 = {bfloat16} # Use bfloat16 = True for trainign with TPU\n",
    "    \n",
    "Locations: \n",
    "    Path to Pipeline-Config = {pipeline}\n",
    "    Checkpoint from transfermodel  = {checkpoint_path}\n",
    "\n",
    "    Path to the trainedmodel = {trained_model}\n",
    "    Config of the trainedmodel = {config}\n",
    "    \n",
    "Used records: \n",
    "    Used labelmap = {labelmap}\n",
    "    Used train_record = {train_record}\n",
    "    Used test_record = {test_record}\n",
    "\n",
    "Generell Information: \n",
    "    Time needed for modeltraining = {train_time}\n",
    "    Length of traindataset = {train_len}\n",
    "    Counted values for each Label form Traindataset = {train_count}\n",
    "    Length of testdataset = {test_len}\n",
    "    Counted values for each Label form Testdataset = {test_count}\n",
    "\"\"\"\n",
    "\n",
    "    log_id = uuid.uuid4()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_entry = f\"ID: {log_id}\\nTimestamp: {timestamp}\\n\\n{log_contents}\\nNotes: {notes}\\n\"\n",
    "\n",
    "    filename = f\"./myModules/log/{name}_{log_id}.txt\"\n",
    "    with open(filename, \"a\") as file:\n",
    "        file.write(log_entry)\n",
    "        file.write(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_config_path) as f:\n",
    "    config = f.read()\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "  \n",
    "  # Set labelmap path\n",
    "  config = re.sub('label_map_path: \".*?\"', \n",
    "             'label_map_path: \"{}\"'.format(labelmap_path), config)\n",
    "  \n",
    "  # Set fine_tune_checkpoint path\n",
    "  config = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                  'fine_tune_checkpoint: \"{}\"'.format(base_checkpoint_path), config)\n",
    "  \n",
    "  # Set train tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(train_record_path), config)\n",
    "  \n",
    "  # Set test tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(test_record_path), config)\n",
    "  \n",
    "  # Set number of classes.\n",
    "  config = re.sub('num_classes: [0-9]+',\n",
    "                  'num_classes: {}'.format(NUM_CLASSES), config)\n",
    "  \n",
    "  # Set batch size\n",
    "  config = re.sub('batch_size: [0-9]+',\n",
    "                  'batch_size: {}'.format(BATCH_SIZE), config)\n",
    "  \n",
    "  # Set training steps\n",
    "  config = re.sub('num_steps: [0-9]+',\n",
    "                  'num_steps: {}'.format(NUM_STEPS), config)\n",
    "  \n",
    "    # Set use_bfloat16\n",
    "  config = re.sub('use_bfloat16: (true|false)',\n",
    "                  'use_bfloat16: {}'.format(str(use_bfloat16).lower()), config)\n",
    "  \n",
    "  # Set fine-tune checkpoint type to detection\n",
    "  config = re.sub('fine_tune_checkpoint_type: \"classification\"', \n",
    "             'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n",
    "  \n",
    "  f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config_path)\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "WARNING:tensorflow:From C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py:100: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "W0810 12:40:22.378415 21140 deprecation.py:350] From C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py:100: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "2024-08-10 12:40:22.379499: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-10 12:40:22.539296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6391 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0',), communication = CommunicationImplementation.AUTO\n",
      "I0810 12:40:40.164713 21140 collective_all_reduce_strategy.py:446] Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0',), communication = CommunicationImplementation.AUTO\n",
      "INFO:tensorflow:Maybe overwriting train_steps: 13000\n",
      "I0810 12:40:40.179747 21140 config_util.py:552] Maybe overwriting train_steps: 13000\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0810 12:40:40.180752 21140 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "I0810 12:40:40.215813 21140 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
      "I0810 12:40:40.216810 21140 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 224\n",
      "I0810 12:40:40.216810 21140 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n",
      "I0810 12:40:40.231855 21140 efficientnet_model.py:143] round_filter input=32 output=48\n",
      "I0810 12:40:40.476973 21140 efficientnet_model.py:143] round_filter input=32 output=48\n",
      "I0810 12:40:40.476973 21140 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I0810 12:40:40.832647 21140 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I0810 12:40:40.832647 21140 efficientnet_model.py:143] round_filter input=24 output=32\n",
      "I0810 12:40:41.576999 21140 efficientnet_model.py:143] round_filter input=24 output=32\n",
      "I0810 12:40:41.576999 21140 efficientnet_model.py:143] round_filter input=40 output=56\n",
      "I0810 12:40:42.947513 21140 efficientnet_model.py:143] round_filter input=40 output=56\n",
      "I0810 12:40:42.947513 21140 efficientnet_model.py:143] round_filter input=80 output=112\n",
      "I0810 12:40:43.901914 21140 efficientnet_model.py:143] round_filter input=80 output=112\n",
      "I0810 12:40:43.901914 21140 efficientnet_model.py:143] round_filter input=112 output=160\n",
      "I0810 12:40:44.629101 21140 efficientnet_model.py:143] round_filter input=112 output=160\n",
      "I0810 12:40:44.629101 21140 efficientnet_model.py:143] round_filter input=192 output=272\n",
      "I0810 12:40:45.679840 21140 efficientnet_model.py:143] round_filter input=192 output=272\n",
      "I0810 12:40:45.679840 21140 efficientnet_model.py:143] round_filter input=320 output=448\n",
      "I0810 12:40:46.059729 21140 efficientnet_model.py:143] round_filter input=1280 output=1792\n",
      "I0810 12:40:46.150381 21140 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W0810 12:40:59.305928 21140 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['./myModules/records/trainWoUnknown.record']\n",
      "I0810 12:40:59.802343 21140 dataset_builder.py:162] Reading unweighted datasets: ['./myModules/records/trainWoUnknown.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['./myModules/records/trainWoUnknown.record']\n",
      "I0810 12:40:59.804349 21140 dataset_builder.py:79] Reading record datasets for input file: ['./myModules/records/trainWoUnknown.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I0810 12:40:59.804349 21140 dataset_builder.py:80] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0810 12:40:59.804349 21140 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W0810 12:40:59.844221 21140 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0810 12:40:59.948529 21140 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W0810 12:41:10.714139 21140 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0810 12:41:16.289927 21140 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "2024-08-10 12:42:19.871151: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 14084784 exceeds 10% of free system memory.\n",
      "2024-08-10 12:42:19.871527: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 11638812 exceeds 10% of free system memory.\n",
      "2024-08-10 12:42:19.871906: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 29387628 exceeds 10% of free system memory.\n",
      "2024-08-10 12:42:19.872338: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4183776 exceeds 10% of free system memory.\n",
      "2024-08-10 12:42:19.873567: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 10165656 exceeds 10% of free system memory.\n",
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n",
      "I0810 12:42:33.076364 20732 api.py:459] feature_map_spatial_dims: [(128, 128), (64, 64), (32, 32), (16, 16), (8, 8)]\n",
      "I0810 12:42:44.498183  4896 api.py:459] feature_map_spatial_dims: [(128, 128), (64, 64), (32, 32), (16, 16), (8, 8)]\n",
      "2024-08-10 12:42:55.696232: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8907\n",
      "2024-08-10 12:42:58.127784: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.97GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-08-10 12:42:58.226002: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py\", line 114, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 36, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\absl\\app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\absl\\app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py\", line 105, in main\n",
      "    model_lib_v2.train_loop(\n",
      "  File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 605, in train_loop\n",
      "    load_fine_tune_checkpoint(\n",
      "  File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 407, in load_fine_tune_checkpoint\n",
      "    ckpt.restore(\n",
      "  File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint.py\", line 852, in assert_existing_objects_matched\n",
      "    raise AssertionError(\n",
      "AssertionError: Found 610 Python objects that were not bound to checkpointed values, likely due to changes in the Python program. Showing 10 of 610 unmatched objects: [MirroredVariable:{\n",
      "  0: <tf.Variable 'stack_4/block_1/depthwise_conv2d/depthwise_kernel:0' shape=(5, 5, 960, 1) dtype=float32, numpy=\n",
      "array([[[[-6.84607625e-02],\n",
      "         [ 2.32611999e-01],\n",
      "         [ 1.69344172e-01],\n",
      "         ...,\n",
      "         [-4.21366483e-01],\n",
      "         [ 1.70099989e-01],\n",
      "         [-2.67319679e-01]],\n",
      "\n",
      "        [[ 4.26057339e-01],\n",
      "         [ 3.44680458e-01],\n",
      "         [ 5.71874857e-01],\n",
      "         ...,\n",
      "         [-5.27831972e-01],\n",
      "         [ 4.71091092e-01],\n",
      "         [ 2.86580652e-01]],\n",
      "\n",
      "        [[ 9.54172313e-02],\n",
      "         [-3.01207066e-01],\n",
      "         [ 9.88210440e-02],\n",
      "         ...,\n",
      "         [-1.37178376e-02],\n",
      "         [ 3.54020223e-02],\n",
      "         [ 3.88090573e-02]],\n",
      "\n",
      "        [[ 9.72885787e-02],\n",
      "         [-4.28427607e-01],\n",
      "         [-1.49751231e-01],\n",
      "         ...,\n",
      "         [ 8.01020954e-03],\n",
      "         [ 1.86117604e-01],\n",
      "         [-2.46166974e-01]],\n",
      "\n",
      "        [[-1.59247786e-01],\n",
      "         [-4.20028150e-01],\n",
      "         [ 2.02410072e-02],\n",
      "         ...,\n",
      "         [ 2.92472929e-01],\n",
      "         [ 3.98893327e-01],\n",
      "         [-6.29921317e-01]]],\n",
      "\n",
      "\n",
      "       [[[ 4.68295991e-01],\n",
      "         [ 1.15121126e-01],\n",
      "         [-2.47053608e-01],\n",
      "         ...,\n",
      "         [ 2.60571003e-01],\n",
      "         [-2.44489580e-01],\n",
      "         [-1.64764956e-01]],\n",
      "\n",
      "        [[-3.95659357e-01],\n",
      "         [-2.65722334e-01],\n",
      "         [-8.65857452e-02],\n",
      "         ...,\n",
      "         [ 4.01787221e-01],\n",
      "         [-4.63847190e-01],\n",
      "         [-3.05919796e-01]],\n",
      "\n",
      "        [[ 1.85661316e-01],\n",
      "         [ 1.46940872e-01],\n",
      "         [-2.94665456e-01],\n",
      "         ...,\n",
      "         [-7.72201344e-02],\n",
      "         [ 2.54858136e-01],\n",
      "         [ 6.23643585e-02]],\n",
      "\n",
      "        [[-5.49675047e-01],\n",
      "         [-3.68367761e-01],\n",
      "         [ 5.02911747e-01],\n",
      "         ...,\n",
      "         [-2.64752656e-01],\n",
      "         [ 2.52419502e-01],\n",
      "         [-8.12499523e-02]],\n",
      "\n",
      "        [[-3.07662576e-01],\n",
      "         [ 1.88074961e-01],\n",
      "         [ 3.13073844e-01],\n",
      "         ...,\n",
      "         [-4.96350795e-01],\n",
      "         [-3.67221236e-02],\n",
      "         [-8.65054801e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 5.15262842e-01],\n",
      "         [ 5.37411049e-02],\n",
      "         [ 8.54946952e-03],\n",
      "         ...,\n",
      "         [ 6.06689811e-01],\n",
      "         [ 2.60730803e-01],\n",
      "         [-3.81902188e-01]],\n",
      "\n",
      "        [[-3.14065129e-01],\n",
      "         [-2.26567611e-01],\n",
      "         [-3.15340519e-01],\n",
      "         ...,\n",
      "         [-5.64836292e-03],\n",
      "         [ 4.71348763e-02],\n",
      "         [-5.63843772e-02]],\n",
      "\n",
      "        [[ 6.41585529e-01],\n",
      "         [ 1.39650822e-01],\n",
      "         [ 1.91720814e-01],\n",
      "         ...,\n",
      "         [ 2.88433731e-01],\n",
      "         [ 1.38386235e-01],\n",
      "         [ 1.48857683e-01]],\n",
      "\n",
      "        [[-2.21160680e-01],\n",
      "         [ 2.82392472e-01],\n",
      "         [-9.17481408e-02],\n",
      "         ...,\n",
      "         [ 1.49319813e-01],\n",
      "         [-2.58106649e-01],\n",
      "         [-7.33090118e-02]],\n",
      "\n",
      "        [[-4.98847179e-02],\n",
      "         [ 2.67543737e-02],\n",
      "         [ 1.88914075e-01],\n",
      "         ...,\n",
      "         [ 3.17166895e-01],\n",
      "         [-8.10967982e-02],\n",
      "         [ 1.68862462e-01]]],\n",
      "\n",
      "\n",
      "       [[[-2.50256490e-02],\n",
      "         [-3.38814139e-01],\n",
      "         [ 3.66218954e-01],\n",
      "         ...,\n",
      "         [ 1.13456257e-01],\n",
      "         [-1.40266746e-01],\n",
      "         [-1.91412494e-01]],\n",
      "\n",
      "        [[ 2.22332940e-01],\n",
      "         [-6.15818381e-01],\n",
      "         [ 5.93596280e-01],\n",
      "         ...,\n",
      "         [ 5.88034689e-01],\n",
      "         [-2.12208666e-02],\n",
      "         [ 7.78167695e-02]],\n",
      "\n",
      "        [[-2.44458392e-01],\n",
      "         [ 3.16815019e-01],\n",
      "         [ 1.75622657e-01],\n",
      "         ...,\n",
      "         [-2.43388012e-01],\n",
      "         [ 1.81439996e-01],\n",
      "         [ 4.05118078e-01]],\n",
      "\n",
      "        [[-3.23578626e-01],\n",
      "         [ 2.38371000e-01],\n",
      "         [-4.06027377e-01],\n",
      "         ...,\n",
      "         [ 2.64341444e-01],\n",
      "         [-3.26826245e-01],\n",
      "         [ 2.21248850e-01]],\n",
      "\n",
      "        [[-3.12210172e-01],\n",
      "         [-2.62041092e-01],\n",
      "         [-1.83194116e-01],\n",
      "         ...,\n",
      "         [-3.16038072e-01],\n",
      "         [-2.36620143e-01],\n",
      "         [-2.52644151e-01]]],\n",
      "\n",
      "\n",
      "       [[[ 1.86343208e-01],\n",
      "         [-5.36356792e-02],\n",
      "         [ 1.57874480e-01],\n",
      "         ...,\n",
      "         [ 1.24851704e-01],\n",
      "         [ 1.06145740e-01],\n",
      "         [ 1.71083584e-01]],\n",
      "\n",
      "        [[-3.64596903e-01],\n",
      "         [ 1.41423881e-01],\n",
      "         [ 1.59384474e-01],\n",
      "         ...,\n",
      "         [ 2.85665184e-01],\n",
      "         [-2.69790500e-01],\n",
      "         [-3.21545839e-01]],\n",
      "\n",
      "        [[-4.73917007e-01],\n",
      "         [-2.77497739e-01],\n",
      "         [ 1.20664248e-02],\n",
      "         ...,\n",
      "         [-4.27424759e-02],\n",
      "         [-3.14050436e-01],\n",
      "         [-3.99464458e-01]],\n",
      "\n",
      "        [[-3.33685458e-01],\n",
      "         [-4.85250115e-01],\n",
      "         [-1.87354460e-01],\n",
      "         ...,\n",
      "         [ 3.04005116e-01],\n",
      "         [-4.40457106e-01],\n",
      "         [-2.05929264e-01]],\n",
      "\n",
      "        [[-2.39678010e-01],\n",
      "         [-4.87204932e-04],\n",
      "         [ 4.43959713e-01],\n",
      "         ...,\n",
      "         [-3.98378342e-01],\n",
      "         [-2.93357760e-01],\n",
      "         [ 2.70192116e-01]]]], dtype=float32)>\n",
      "}, MirroredVariable:{\n",
      "  0: <tf.Variable 'stack_2/block_2/project_bn/gamma:0' shape=(56,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1.], dtype=float32)>\n",
      "}, MirroredVariable:{\n",
      "  0: <tf.Variable 'stack_4/block_4/se_reduce_conv2d/bias:0' shape=(40,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.], dtype=float32)>\n",
      "}, MirroredVariable:{\n",
      "  0: <tf.Variable 'stack_1/block_3/se_expand_conv2d/bias:0' shape=(192,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0.], dtype=float32)>\n",
      "}, SyncOnReadVariable:{\n",
      "  0: <tf.Variable 'stack_5/block_2/expand_bn/moving_mean:0' shape=(1632,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>\n",
      "}, SyncOnReadVariable:{\n",
      "  0: <tf.Variable 'stack_5/block_0/project_bn/moving_variance:0' shape=(272,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "      dtype=float32)>\n",
      "}, MirroredVariable:{\n",
      "  0: <tf.Variable 'stack_2/block_2/depthwise_bn/gamma:0' shape=(336,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>\n",
      "}, SyncOnReadVariable:{\n",
      "  0: <tf.Variable 'stack_1/block_2/expand_bn/moving_variance:0' shape=(192,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1.], dtype=float32)>\n",
      "}, MirroredVariable:{\n",
      "  0: <tf.Variable 'stack_1/block_1/se_expand_conv2d/bias:0' shape=(192,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0.], dtype=float32)>\n",
      "}, MirroredVariable:{\n",
      "  0: <tf.Variable 'stack_3/block_2/project_bn/beta:0' shape=(112,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py \\\n",
    "    --pipeline_config_path={config_path} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_workers=2 \\\n",
    "    --num_train_steps={NUM_STEPS} \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --num_eval_steps={NUM_EVAL_STEPS}\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "log_model_info(\n",
    "    batch=BATCH_SIZE,\n",
    "    classes=NUM_CLASSES,\n",
    "    steps=NUM_STEPS,\n",
    "    eval_steps=NUM_EVAL_STEPS,\n",
    "    name=MODEL_NAME,\n",
    "    bfloat16=use_bfloat16,\n",
    "    pipeline=base_config_path,\n",
    "    checkpoint_path=base_checkpoint_path,\n",
    "    labelmap=labelmap_path,\n",
    "    train_record=train_record_path,\n",
    "    test_record=test_record_path,\n",
    "    config=config_path,\n",
    "    trained_model=model_dir,\n",
    "    train_time=training_time,\n",
    "    df_test=df_final_test,\n",
    "    df_train=df_final_train,\n",
    "    notes=NOTES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"SSD_16000_640_mobilenet_v2_fpnlite\"\n",
    "config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\model_main_tf2.py \\\n",
    "            --pipeline_config_path={config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={model_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating model: {MODEL_NAME}\")\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "output, error = process.communicate()\n",
    "print(f\"Done Evaluating: {MODEL_NAME}\")\n",
    "if process.returncode != 0:\n",
    "    print(error.decode(\"utf-8\"))\n",
    "else:\n",
    "    print(f\"No Error: {MODEL_NAME}\")\n",
    "    output_str = output.decode(\"utf-8\")\n",
    "    header = f\"Evaluation Results for: {MODEL_NAME}\\n\\n\"\n",
    "    with open(f\"./myModules/log/{MODEL_NAME}_evaluation_results.txt\", \"w\") as f:\n",
    "        f.write(header)\n",
    "        f.write(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"SSD_8000_640_mobilenet_v1_fpn_custom\" # Bugged\n",
    "\"SSD_8000_640_mobilenet_v1_fpn\"\n",
    "\"SSD_8000_640_mobilenet_v2_fpnlite_custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"SSD_8000_640_resnet101_v1_fpn\",\n",
    "    \"SSD_16000_640_mobilenet_v2_fpnlite\"\n",
    "]\n",
    "\n",
    "config_template = './myModules/configs/{name}_config.config'\n",
    "model_dir_template = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'\n",
    "log_dir = './myModules/log'\n",
    "\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def evaluate_model(model_name):\n",
    "    config_path = config_template.format(name=model_name)\n",
    "    model_dir = model_dir_template.format(name=model_name)\n",
    "    \n",
    "    command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\model_main_tf2.py \\\n",
    "                --pipeline_config_path={config_path} \\\n",
    "                --model_dir={model_dir} \\\n",
    "                --checkpoint_dir={model_dir}\"\n",
    "    \n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        error_message = error.decode(\"utf-8\")\n",
    "        print(f\"Error evaluating model {model_name}: {error_message}\")\n",
    "        return error_message\n",
    "    else:\n",
    "        output_str = output.decode(\"utf-8\")\n",
    "        header = f\"Evaluation Results for: {model_name}\\n\\n\"\n",
    "        result_file = os.path.join(log_dir, f\"{model_name}_evaluation_results.txt\")\n",
    "        with open(result_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            f.write(output_str)\n",
    "        print(f\"Evaluation results for {model_name} saved to {result_file}\")\n",
    "        return output_str\n",
    "\n",
    "# Hauptschleife zur Evaluierung der Modelle\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    evaluate_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade die Evaluierungsdaten\n",
    "eval_dataset = tf.data.TFRecordDataset(test_record_path)\n",
    "\n",
    "# ZÃ¤hle die Anzahl der Elemente (Frames) im Dataset\n",
    "num_frames = sum(1 for _ in eval_dataset)\n",
    "print(f\"Anzahl der Frames: {num_frames}\")\n",
    "\n",
    "total_time = 17.09 + 10.86  # Gesamtzeit in Sekunden\n",
    "\n",
    "# Durchschnittliche Zeit pro Frame\n",
    "time_per_frame = total_time / num_frames\n",
    "\n",
    "# FPS\n",
    "fps = 1 / time_per_frame\n",
    "print(f\"FPS: {fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_checkpoints(config_path, model_dir, checkpoints, model_name):\n",
    "    \"\"\"Evaluates all checkpoints and logs the results.\"\"\"\n",
    "    for checkpoint in checkpoints:\n",
    "        eval_command = f\"python C:/Users/Alexej/Desktop/GTSRB/models/research/object_detection/model_main_tf2.py \\\n",
    "            --pipeline_config_path={config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={checkpoint} \\\n",
    "            --run_once\"\n",
    "\n",
    "        # Starte den Evaluierungsprozess\n",
    "        process = subprocess.Popen(eval_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "        output, error = process.communicate()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f\"Error evaluating checkpoint {checkpoint}:\")\n",
    "            print(error.decode(\"utf-8\"))\n",
    "        else:\n",
    "            output_str = output.decode(\"utf-8\")\n",
    "            checkpoint_name = os.path.basename(checkpoint)\n",
    "            header = f\"Evaluation Results for MODEL_NAME: {model_name} at Checkpoint: {checkpoint_name}\\n\\n\"\n",
    "            with open(f\"./myModules/log/{model_name}_evaluation_results_{checkpoint_name}.txt\", \"w\") as f:\n",
    "                f.write(header)\n",
    "                f.write(output_str)\n",
    "            print(f\"Evaluation for checkpoint {checkpoint} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = get_checkpoints(model_dir)\n",
    "\n",
    "evaluate_checkpoints(config_path, model_dir, checkpoints, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_multiple_images(model, image, min_score_threshold):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Filter out detections with a score below the threshold\n",
    "    scores = output_dict['detection_scores']\n",
    "    high_score_indices = scores >= min_score_threshold\n",
    "\n",
    "    output_dict['detection_boxes'] = output_dict['detection_boxes'][high_score_indices]\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'][high_score_indices]\n",
    "    output_dict['detection_scores'] = output_dict['detection_scores'][high_score_indices]\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path, width, height):\n",
    "    image = Image.open(path)\n",
    "    # Resize the image to a larger size\n",
    "    image = image.resize((width, height))  # Resize to 256x256, you can change this as needed\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes_with_color(image, boxes, classes, scores, category_index, box_color, line_thickness):\n",
    "    for i in range(boxes.shape[0]):\n",
    "        if scores is None or scores[i] > 0.5:\n",
    "            class_id = int(classes[i])\n",
    "            if class_id in category_index.keys():\n",
    "                class_name = category_index[class_id]['name']\n",
    "                display_str = str(class_name)\n",
    "                color = box_color\n",
    "                vis_util.draw_bounding_box_on_image_array(\n",
    "                    image,\n",
    "                    boxes[i][0],\n",
    "                    boxes[i][1],\n",
    "                    boxes[i][2],\n",
    "                    boxes[i][3],\n",
    "                    color=color,\n",
    "                    thickness=line_thickness,\n",
    "                    display_str_list=[display_str],\n",
    "                    use_normalized_coordinates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_legend(image, groundtruth_classes, category_index, iou):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    image_height = image.size[1]\n",
    "    font_size = max(int(image_height * 0.04), 12)  # SchriftgrÃ¶ÃŸe auf 4% der BildhÃ¶he begrenzt, aber mindestens 12\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "\n",
    "    y_offset = int(0.9 * image_height)  # Startoffset fÃ¼r die Legende, z.B. 90% der BildhÃ¶he\n",
    "    x_offset = int(0.05 * image.size[0])  # Startoffset fÃ¼r die Legende, z.B. 5% der Bildbreite\n",
    "\n",
    "    # Ground Truth Legende\n",
    "    for gt_class in groundtruth_classes:\n",
    "        if gt_class in category_index:\n",
    "            class_name = category_index[gt_class]['name']\n",
    "            draw.text((x_offset, y_offset), f\"GT: {class_name}\\nIoU: {iou:.2f}\", fill=\"yellow\", font=font)\n",
    "            y_offset += int(font_size * 1.5)  # ErhÃ¶he den Offset basierend auf der aktuellen SchriftgrÃ¶ÃŸe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    # Implementation of Intersection over Union (IoU) calculation\n",
    "    y1_max = min(box1[2], box2[2])\n",
    "    y1_min = max(box1[0], box2[0])\n",
    "    x1_max = min(box1[3], box2[3])\n",
    "    x1_min = max(box1[1], box2[1])\n",
    "\n",
    "    intersection_area = max(0, y1_max - y1_min) * max(0, x1_max - x1_min)\n",
    "\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    \n",
    "    image_pil = Image.fromarray(np.uint8(image_np)).convert(\"RGB\")\n",
    "    add_legend(image_pil, groundtruth_classes, category_index, iou)\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, ax):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    ax.imshow(image_np)\n",
    "    ax.axis('off')  # Hide the axis\n",
    "\n",
    "    # Adding legend to the plot\n",
    "    label_text = category_index[groundtruth_classes[0]]['name']\n",
    "    detected_class = output_dict['detection_classes'][0]\n",
    "    predicted_text = category_index[detected_class]['name']\n",
    "\n",
    "    if detected_class != groundtruth_classes[0]:\n",
    "        title_color = 'red'\n",
    "    else:\n",
    "        title_color = 'green'\n",
    "\n",
    "    ax.set_title(f'True: {label_text}, Pred.: {predicted_text}\\nIoU: {iou:.2f}', color=title_color, fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_images_for_labels(df, model, category_index, discard_predictions_below_acc, num_images_to_display, model_name):\n",
    "    sorted_labels = sorted(df['Label'].unique())\n",
    "\n",
    "    num_rows = len(sorted_labels)\n",
    "    num_cols = num_images_to_display\n",
    "\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols + num_cols * 0.5, num_rows * 2))\n",
    "    \n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, num_cols)\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for idx, label in enumerate(sorted_labels):\n",
    "        label_df = df[df['Label'] == label]\n",
    "        \n",
    "        random_indices = random.sample(range(len(label_df)), min(num_images_to_display, len(label_df)))\n",
    "\n",
    "        for jdx, r_idx in enumerate(random_indices):\n",
    "            row = label_df.iloc[r_idx]\n",
    "            image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "            groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "            groundtruth_classes = [row['Label']]\n",
    "            \n",
    "            output_dict = run_inference_for_multiple_images(model, image_np, discard_predictions_below_acc)\n",
    "            \n",
    "            plot_idx = idx * num_images_to_display + jdx\n",
    "            \n",
    "            visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, axes[plot_idx])\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.005, 1, 0.95])\n",
    "    plt.suptitle(f'{model_name}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_predictions_with_groundtruth (model, df, num_images_to_display, category_index):\n",
    "    \n",
    "    random_indices = random.sample(range(len(df)), num_images_to_display)\n",
    "\n",
    "    # Visualisiere zufÃ¤llige ausgewÃ¤hlte Bilder\n",
    "    for idx in random_indices:\n",
    "        row = df.iloc[idx]\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_endless_predictions(model, df, category_index):\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_endless_predictions_from_path(model, path, category_index):\n",
    "    \n",
    "    for image_path in glob.glob(path):\n",
    "        image_np = load_image_into_numpy_array(image_path, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=1)\n",
    "        display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"Faster_RCNN_short_640_101\",\n",
    "    \"Faster_RCNN_short_640_101_test\",\n",
    "    \"Faster_RCNN_short_640_101_unknown\",\n",
    "    \"Faster_RCNN_short_640_101_unknown_long\",\n",
    "    \"Faster_RCNN_640_50\",\n",
    "    \"Faster_RCNN_1024_50\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"Faster_RCNN_short_640_101\",\n",
    "    \"Faster_RCNN_short_640_101_test\",\n",
    "    \"Faster_RCNN_short_640_101_unknown\",\n",
    "    \"Faster_RCNN_short_640_101_unknown_long\",\n",
    "    \"Faster_RCNN_640_50\",\n",
    "    \"Faster_RCNN_1024_50\"\n",
    "]\n",
    "\n",
    "config_template = './myModules/configs/{name}_config.config'\n",
    "model_dir_template = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'\n",
    "output_dir_template = './inference/faster_rcnn/{name}'\n",
    "\n",
    "\n",
    "def evaluate_model(model_name):\n",
    "    config_path = config_template.format(name=model_name)\n",
    "    model_dir = model_dir_template.format(name=model_name)\n",
    "    output_dir = output_dir_template.format(name=model_name)\n",
    "    \n",
    "    command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\exporter_main_v2.py \\\n",
    "                --pipeline_config_path={config_path} \\\n",
    "                --output_directory={output_dir} \\\n",
    "                --trained_checkpoint_dir={model_dir}\"\n",
    "    \n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "    print(f\"Done creating Inference: {MODEL_NAME}\")\n",
    "    if process.returncode != 0:\n",
    "        print(error.decode(\"utf-8\"))\n",
    "    else:\n",
    "        print(f\"No Error: {MODEL_NAME}\")\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Infering model: {model_name}\")\n",
    "    evaluate_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\exporter_main_v2.py \\\n",
    "    --trained_checkpoint_dir {model_dir} \\\n",
    "    --output_directory {inference_path} \\\n",
    "    --pipeline_config_path  {config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_path = './inference/ssd/{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load(f'./{inference_path}/saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inference Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = './GTSRB/Final_Test/Images/*.ppm'\n",
    "min_score_threshold = 0.4\n",
    "number_of_images_to_display = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_endless_predictions_from_path(model, image_path, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_endless_predictions(model, df_final_test, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_predictions_with_groundtruth(model, df_final_test, number_of_images_to_display, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_images_for_labels(df_final_test, model, category_index, min_score_threshold, number_of_images_to_display, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation (discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_labels = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "for idx, row in df_test_raw.iterrows():\n",
    "    row = df_test_raw.iloc[idx]\n",
    "    image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "    groundtruth_classes = [row['Label']]\n",
    "    \n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "    \n",
    "    predicted_class = output_dict['detection_classes'][0]\n",
    "    all_predicted_labels.append(predicted_class)\n",
    "    all_true_labels.append(groundtruth_classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
    "recall = recall_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "precision = precision_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "f1 = f1_score(all_true_labels, all_predicted_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Vorhersage')\n",
    "plt.ylabel('Wahre Werte')\n",
    "plt.title('Konfusionsmatrix')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_true_labels, all_predicted_labels, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-Time detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoint_files = os.listdir(checkpoint_dir)\n",
    "    checkpoint_files = [f for f in checkpoint_files if f.startswith('ckpt-') and '.index' in f]\n",
    "    checkpoint_numbers = [int(re.findall(r'\\d+', f)[0]) for f in checkpoint_files]\n",
    "    if not checkpoint_numbers:\n",
    "        raise ValueError(\"No checkpoints found in the directory.\")\n",
    "    latest_checkpoint = max(checkpoint_numbers)\n",
    "    return os.path.join(checkpoint_dir, f'ckpt-{latest_checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH)).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load('./inference/{type}/{name}/saved_model'.format(name=MODEL_NAME, type=MODEL_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load('./inference/{type}/{name}/saved_model'.format(name=MODEL_NAME, type=MODEL_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Laden des Modells\n",
    "def load_model(model_path):\n",
    "    saved_model = tf.saved_model.load(model_path)\n",
    "    return saved_model\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "model_path = './inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model'\n",
    "detection_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Erstellen des Detektionsmodells und Wiederherstellen des Checkpoints\n",
    "def load_model():\n",
    "    configs = tf.compat.v2.saved_model.load('./inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model')\n",
    "    return configs['model']\n",
    "\n",
    "detection_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=10,\n",
    "                min_score_thresh=.9,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 800)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video capture testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum Eingabevideo und zum Ausgabevideo\n",
    "input_video_path = './videoInferenceData/shortTest.mp4'\n",
    "output_video_path = './videoInferenceData/tf/shortTest.mp4'\n",
    "\n",
    "# VideoCapture-Objekt erstellen\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# ÃœberprÃ¼fen, ob das Video geÃ¶ffnet werden kann\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Video-Eigenschaften abrufen\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# VideoWriter-Objekt erstellen\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Beenden der Schleife, wenn das Video zu Ende ist\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes sollten ints sein.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=10,\n",
    "                min_score_thresh=.9,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    # Frame mit Detektionen in das Ausgabevideo schreiben\n",
    "    out.write(image_np_with_detections)\n",
    "\n",
    "    # Optional: Zeige das Video mit Detektionen in einem Fenster an\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 800)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Ressourcen freigeben\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video capture testing end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(frame, axis=0), dtype=tf.float32)\n",
    "    \n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'],\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=100,  # Anzahl der maximal zu zeichnenden Boxen\n",
    "        min_score_thresh=0.5    # Minimale Vertrauensschwelle fÃ¼r die Anzeige\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    \n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Laden und Konfigurieren des Modells\n",
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Wiederherstellen des Checkpoints\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(TRAINED_CHECKPOINT_PATH, 'ckpt-9')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)\n",
    "\n",
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    \n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.9,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', cv2.resize(image_np, (800, 800)))\n",
    "    \n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Videoaufnahme von der Kamera starten (Kamera 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# ÃœberprÃ¼fen, ob die Kamera erfolgreich geÃ¶ffnet wurde\n",
    "if not cap.isOpened():\n",
    "    print(\"Fehler beim Ã–ffnen der Kamera\")\n",
    "    exit()\n",
    "\n",
    "# Unendlich Schleife, um Frames von der Kamera zu lesen und anzuzeigen\n",
    "while True:\n",
    "    # Lesen eines einzelnen Frames von der Kamera\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # ÃœberprÃ¼fen, ob das Frame erfolgreich gelesen wurde\n",
    "    if not ret:\n",
    "        print(\"Fehler beim Lesen des Frames\")\n",
    "        break\n",
    "    \n",
    "    # Anzeigen des Frames in einem Fenster\n",
    "    cv2.imshow('Videoaufnahme', frame)\n",
    "    \n",
    "    # Beenden der Schleife bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Freigeben der Videoquelle und SchlieÃŸen aller Fenster\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster_RCNN_640_50_fixed: good 8/10\n",
    "<br>Faster_RCNN_640_50_final: good 7/10\n",
    "<br>Faster_RCNN_640_50_fixed_distanz: good. Close 9/10, medium 3/10\n",
    "<br>Faster_RCNN_short_640_101_unknown: Erkennt 2 immer. Close 5/10, medium 6/10\n",
    "<br>Faster_RCNN_640_50_fixed_withAugmentation_noUnknown: Stopp (4) 10/10. Close 9/10, medium 6/10\n",
    "<br>Faster_RCNN_50_640_Step_4: idle State is awfull. Close 10/10, Medium depends on class.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_MODEL = \"Faster_RCNN_50_640_Step_4\"\n",
    "TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=INFERENCE_MODEL)\n",
    "TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\{type}\\\\{name}'.format(name=INFERENCE_MODEL, type=\"faster_rcnn\")\n",
    "#TRAINED_CHECKPOINT_PATH = \"./inference/faster_rcnn/Faster_RCNN_640_50_fixed_withAugmentation_noUnknown/checkpoint/\"\n",
    "TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short_woUnk.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Wiederherstellen des Checkpoints\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH)).expect_partial()\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Wiederherstellen des Checkpoints\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH)).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "#category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "#tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load('./inference/faster_rcnn/{name}/saved_model/'.format(name=INFERENCE_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    #input_tensor = input_tensor[tf.newaxis,...]\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    #output_dict['detection_boxes'] = output_dict['detection_boxes'] / [image.shape[0], image.shape[1], image.shape[0], image.shape[1]]\n",
    "    return output_dict\n",
    "\n",
    "def show_inference(frame):\n",
    "    \n",
    "    image_np = np.array(frame)\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "    #output_dict = apply_nms(output_dict)  # NMS anwenden\n",
    "    height, width, _ = image_np.shape\n",
    "\n",
    "    # Konvertiere normalisierte Koordinaten in Pixelkoordinaten\n",
    "    for i in range(output_dict['detection_boxes'].shape[0]):\n",
    "        ymin, xmin, ymax, xmax = output_dict['detection_boxes'][i]\n",
    "        output_dict['detection_boxes'][i] = [ymin * height, xmin * width, ymax * height, xmax * width]\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes']+1,\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=10,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    return image_np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "           \n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    cv2.imshow('object detection', cv2.resize(frame_with_detections, (800,600)))\n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    #input_tensor = input_tensor[tf.newaxis,...]\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference(frame):\n",
    "    \n",
    "    image_np = np.array(frame)\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "    output_dict = apply_nms(output_dict)  # NMS anwenden\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes']+1,\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=10,\n",
    "        min_score_thresh=0.5,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nms(output_dict, iou_threshold=0.4):\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    scores = output_dict['detection_scores']\n",
    "    classes = output_dict['detection_classes']\n",
    "    \n",
    "    selected_indices = tf.image.non_max_suppression(\n",
    "        boxes=boxes,\n",
    "        scores=scores,\n",
    "        max_output_size=output_dict['num_detections'],\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=0.4\n",
    "    )\n",
    "    \n",
    "    selected_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "    selected_scores = tf.gather(scores, selected_indices).numpy()\n",
    "    selected_classes = tf.gather(classes, selected_indices).numpy()\n",
    "    \n",
    "    output_dict['detection_boxes'] = selected_boxes\n",
    "    output_dict['detection_scores'] = selected_scores\n",
    "    output_dict['detection_classes'] = selected_classes\n",
    "    output_dict['num_detections'] = len(selected_boxes)\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum Eingabevideo und zum Ausgabevideo\n",
    "input_video_path = './videoInferenceData/15FPS_singleRow/15FPS_singleRow_test1.mp4'\n",
    "output_video_path = './videoInferenceData/tf/shortTest.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    out.write(image_np_with_detections)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(frame_with_detections, (640, 640)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion die den Inputstream HochauflÃ¶send macht\n",
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anzeige der Inferenz\n",
    "def show_inference(frame):\n",
    "    # Skaliere das Bild hoch\n",
    "    scale_percent = 50  # Prozentuale Skalierung\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    resized_frame = cv2.resize(frame, dim, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    image_np = np.array(resized_frame)\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        resized_frame,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes']+1,\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=20,  \n",
    "        min_score_thresh=0.6,  \n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "\n",
    "    # Skaliere das Bild zurÃ¼ck\n",
    "    original_dim = (frame.shape[1], frame.shape[0])\n",
    "    frame_with_detections = cv2.resize(resized_frame, original_dim, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    return frame_with_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur AusfÃ¼hrung der Inferenz\n",
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anzeige der Inferenz\n",
    "def show_inference(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    window_size = 300  # GrÃ¶ÃŸe des Sliding Windows\n",
    "    step_size = 100    # Schrittweite fÃ¼r das Sliding Window\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    scores = []\n",
    "\n",
    "    for y in range(0, height, step_size):\n",
    "        for x in range(0, width, step_size):\n",
    "            window = frame[y:y+window_size, x:x+window_size]\n",
    "            if window.shape[0] != window_size or window.shape[1] != window_size:\n",
    "                continue\n",
    "            \n",
    "            output_dict = run_inference_for_single_image(window)\n",
    "            \n",
    "            for i in range(output_dict['num_detections']):\n",
    "                box = output_dict['detection_boxes'][i]\n",
    "                class_id = output_dict['detection_classes'][i]\n",
    "                score = output_dict['detection_scores'][i]\n",
    "                \n",
    "                if score < 0.4:\n",
    "                    continue\n",
    "                \n",
    "                ymin, xmin, ymax, xmax = box\n",
    "                (left, right, top, bottom) = (int(xmin * window_size) + x, int(xmax * window_size) + x, int(ymin * window_size) + y, int(ymax * window_size) + y)\n",
    "                boxes.append([top, left, bottom, right])\n",
    "                classes.append(class_id + 1)\n",
    "                scores.append(score)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        np.array(boxes),\n",
    "        np.array(classes),\n",
    "        np.array(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=10,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur AusfÃ¼hrung der Inferenz\n",
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anwendung von Non-Maximum Suppression\n",
    "def apply_nms(boxes, scores, classes, max_output_size=5, iou_threshold=0.8, score_threshold=0.6):\n",
    "    indices = tf.image.non_max_suppression(\n",
    "        boxes=boxes,\n",
    "        scores=scores,\n",
    "        max_output_size=max_output_size,\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=score_threshold\n",
    "    )\n",
    "    selected_boxes = tf.gather(boxes, indices).numpy()\n",
    "    selected_scores = tf.gather(scores, indices).numpy()\n",
    "    selected_classes = tf.gather(classes, indices).numpy()\n",
    "    return selected_boxes, selected_scores, selected_classes\n",
    "\n",
    "# Funktion zur Anzeige der Inferenz\n",
    "def show_inference(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    window_size = 300  # GrÃ¶ÃŸe des Sliding Windows\n",
    "    step_size = 150    # Schrittweite fÃ¼r das Sliding Window\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    scores = []\n",
    "\n",
    "    for y in range(0, height, step_size):\n",
    "        for x in range(0, width, step_size):\n",
    "            window = frame[y:y+window_size, x:x+window_size]\n",
    "            if window.shape[0] != window_size or window.shape[1] != window_size:\n",
    "                continue\n",
    "            \n",
    "            output_dict = run_inference_for_single_image(window)\n",
    "            \n",
    "            for i in range(output_dict['num_detections']):\n",
    "                box = output_dict['detection_boxes'][i]\n",
    "                class_id = output_dict['detection_classes'][i]\n",
    "                score = output_dict['detection_scores'][i]\n",
    "                \n",
    "                if score < 0.4:\n",
    "                    continue\n",
    "                \n",
    "                ymin, xmin, ymax, xmax = box\n",
    "                (left, right, top, bottom) = (\n",
    "                    int(xmin * window_size) + x, \n",
    "                    int(xmax * window_size) + x, \n",
    "                    int(ymin * window_size) + y, \n",
    "                    int(ymax * window_size) + y\n",
    "                )\n",
    "                boxes.append([top, left, bottom, right])\n",
    "                classes.append(class_id + 1)\n",
    "                scores.append(score)\n",
    "    \n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "    classes = np.array(classes)\n",
    "\n",
    "    selected_boxes, selected_scores, selected_classes = apply_nms(boxes, scores, classes)\n",
    "\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        selected_boxes,\n",
    "        selected_classes,\n",
    "        selected_scores,\n",
    "        category_index,\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=5,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    \n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur AusfÃ¼hrung der Inferenz\n",
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anwendung von Non-Maximum Suppression\n",
    "def apply_nms(output_dict, iou_threshold=0.6):\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    scores = output_dict['detection_scores']\n",
    "    classes = output_dict['detection_classes']+1\n",
    "    \n",
    "    selected_indices = tf.image.non_max_suppression(\n",
    "        boxes=boxes,\n",
    "        scores=scores,\n",
    "        max_output_size=output_dict['num_detections'],\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=0.6\n",
    "    )\n",
    "    \n",
    "    selected_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "    selected_scores = tf.gather(scores, selected_indices).numpy()\n",
    "    selected_classes = tf.gather(classes, selected_indices).numpy()\n",
    "    \n",
    "    output_dict['detection_boxes'] = selected_boxes\n",
    "    output_dict['detection_scores'] = selected_scores\n",
    "    output_dict['detection_classes'] = selected_classes\n",
    "    output_dict['num_detections'] = len(selected_boxes)\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anzeige der Inferenz\n",
    "def show_inference(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    window_size = 300  # GrÃ¶ÃŸe des Sliding Windows\n",
    "    step_size = 150    # Schrittweite fÃ¼r das Sliding Window\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    scores = []\n",
    "\n",
    "    for y in range(0, height, step_size):\n",
    "        for x in range(0, width, step_size):\n",
    "            window = frame[y:y+window_size, x:x+window_size]\n",
    "            if window.shape[0] != window_size or window.shape[1] != window_size:\n",
    "                continue\n",
    "            \n",
    "            output_dict = run_inference_for_single_image(window)\n",
    "            output_dict = apply_nms(output_dict)  # NMS anwenden\n",
    "            \n",
    "            for i in range(output_dict['num_detections']):\n",
    "                box = output_dict['detection_boxes'][i]\n",
    "                class_id = output_dict['detection_classes'][i]\n",
    "                score = output_dict['detection_scores'][i]\n",
    "                \n",
    "                ymin, xmin, ymax, xmax = box\n",
    "                (left, right, top, bottom) = (\n",
    "                    int(xmin * window_size) + x, \n",
    "                    int(xmax * window_size) + x, \n",
    "                    int(ymin * window_size) + y, \n",
    "                    int(ymax * window_size) + y\n",
    "                )\n",
    "                boxes.append([top, left, bottom, right])\n",
    "                classes.append(class_id)\n",
    "                scores.append(score)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        np.array(boxes),\n",
    "        np.array(classes),\n",
    "        np.array(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=5,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anwendung von Non-Maximum Suppression\n",
    "def apply_nms(output_dict, iou_threshold=0.6):\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    scores = output_dict['detection_scores']\n",
    "    classes = output_dict['detection_classes']+1\n",
    "    \n",
    "    selected_indices = tf.image.non_max_suppression(\n",
    "        boxes=boxes,\n",
    "        scores=scores,\n",
    "        max_output_size=output_dict['num_detections'],\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=0.6\n",
    "    )\n",
    "    \n",
    "    selected_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "    selected_scores = tf.gather(scores, selected_indices).numpy()\n",
    "    selected_classes = tf.gather(classes, selected_indices).numpy()\n",
    "    \n",
    "    output_dict['detection_boxes'] = selected_boxes\n",
    "    output_dict['detection_scores'] = selected_scores\n",
    "    output_dict['detection_classes'] = selected_classes\n",
    "    output_dict['num_detections'] = len(selected_boxes)\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anzeige der Inferenz\n",
    "def show_inference(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    window_size = 300  # GrÃ¶ÃŸe des Sliding Windows\n",
    "    step_size = 200    # Schrittweite fÃ¼r das Sliding Window\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    scores = []\n",
    "\n",
    "    for y in range(0, height, step_size):\n",
    "        for x in range(0, width, step_size):\n",
    "            window = frame[y:y+window_size, x:x+window_size]\n",
    "            if window.shape[0] != window_size or window.shape[1] != window_size:\n",
    "                continue\n",
    "            \n",
    "            output_dict = run_inference_for_single_image(window)\n",
    "            output_dict = apply_nms(output_dict)  # NMS anwenden\n",
    "            \n",
    "            for i in range(output_dict['num_detections']):\n",
    "                box = output_dict['detection_boxes'][i]\n",
    "                class_id = output_dict['detection_classes'][i]\n",
    "                score = output_dict['detection_scores'][i]\n",
    "                \n",
    "                ymin, xmin, ymax, xmax = box\n",
    "                (left, right, top, bottom) = (\n",
    "                    int(xmin * window_size) + x, \n",
    "                    int(xmax * window_size) + x, \n",
    "                    int(ymin * window_size) + y, \n",
    "                    int(ymax * window_size) + y\n",
    "                )\n",
    "                boxes.append([top, left, bottom, right])\n",
    "                classes.append(class_id)\n",
    "                scores.append(score)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        np.array(boxes),\n",
    "        np.array(classes),\n",
    "        np.array(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=5,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Read frame from camera\n",
    "    ret, image_np = cap.read()\n",
    "\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "    # Things to try:\n",
    "    # Flip horizontally\n",
    "    # image_np = np.fliplr(image_np).copy()\n",
    "\n",
    "    # Convert image to grayscale\n",
    "    # image_np = np.tile(\n",
    "    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_detections,\n",
    "          detections['detection_boxes'][0].numpy(),\n",
    "          (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "          detections['detection_scores'][0].numpy(),\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=10,\n",
    "          min_score_thresh=0.60,\n",
    "          agnostic_mode=False)\n",
    "\n",
    "    # Display output\n",
    "    cv2.imshow('object detection', cv2.resize(image_np_with_detections, (800, 600)))\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "           \n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    cv2.imshow('object detection', frame_with_detections)\n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "  image = np.asarray(image)\n",
    "  input_tensor = tf.convert_to_tensor(image)\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    " \n",
    "\n",
    "  model_fn = model.signatures['serving_default']\n",
    "  output_dict = model_fn(input_tensor)\n",
    "  \n",
    "  num_detections = int(output_dict.pop('num_detections'))\n",
    "  output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "  output_dict['num_detections'] = num_detections\n",
    " \n",
    "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "  # Handle models with masks:\n",
    "  if 'detection_masks' in output_dict:\n",
    "    # Reframe the the bbox mask to the image size.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "               image.shape[0], image.shape[1])      \n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                       tf.uint8)\n",
    "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "     \n",
    "  return output_dict\n",
    "def show_inference(model, frame):\n",
    "  image_np = np.array(frame)\n",
    "     \n",
    "  output_dict = run_inference_for_single_image(model, image_np)\n",
    "\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=10,\n",
    "      min_score_thresh=0.6,\n",
    "      agnostic_mode=False,\n",
    "      line_thickness=5)\n",
    " \n",
    "  return(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    re,frame = video_capture.read()\n",
    "    Imagenp=show_inference(model, frame)\n",
    "    cv2.imshow('object detection', cv2.resize(Imagenp, (800,600)))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum Eingabevideo und zum Ausgabevideo\n",
    "input_video_path = './videoInferenceData/multiRow/allnUnkClassBrightToDark.mp4'\n",
    "output_video_path = './videoInferenceData/tf/shortTest.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    out.write(image_np_with_detections)\n",
    "\n",
    "    cv2.imshow('object detection',  frame_with_detections)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "fps = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_with_detections = show_inference(frame)\n",
    "\n",
    "    # Berechne die FPS\n",
    "    frame_count += 1\n",
    "    if (time.time() - start_time) > 1:  \n",
    "        fps = frame_count / (time.time() - start_time)\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    cv2.putText(frame_with_detections, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Object Detection', frame_with_detections)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kamera-Kalibrierungsparameter (mÃ¼ssen angepasst werden)\n",
    "f = 3000  # Brennweite in Pixeln\n",
    "objekt_breite = 0.5  # tatsÃ¤chliche Breite des Objekts in Metern\n",
    "\n",
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Entfernung berechnen und Ergebnisse anpassen\n",
    "    for i in range(num_detections):\n",
    "        box = detections['detection_boxes'][i]\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        box_width = xmax - xmin\n",
    "        box_height = ymax - ymin\n",
    "\n",
    "        # Berechne die GrÃ¶ÃŸe des Objekts in Pixeln\n",
    "        box_width_pixels = box_width * frame.shape[1]\n",
    "        box_height_pixels = box_height * frame.shape[0]\n",
    "\n",
    "        # Berechne die Entfernung zum Objekt (angenommene GrÃ¶ÃŸe des Objekts)\n",
    "        entfernung = (objekt_breite * f) / box_width_pixels\n",
    "\n",
    "        # Hier kannst du die Erkennungsergebnisse basierend auf der Entfernung anpassen\n",
    "        # Beispiel: Du kannst die Boxen oder Scores basierend auf der Entfernung anpassen\n",
    "\n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'] + 1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.3,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Definiere Skalierungsfaktoren\n",
    "scales = [0.25, 1.0, 1.5]\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Initialisiere eine Liste, um alle Erkennungen zu speichern\n",
    "    all_detections = []\n",
    "\n",
    "    for scale in scales:\n",
    "        # Skaliere das Bild\n",
    "        height, width, _ = frame.shape\n",
    "        new_size = (int(width * scale), int(height * scale))\n",
    "        resized_frame = cv2.resize(frame, new_size)\n",
    "\n",
    "        # Konvertieren des Frames zu einem Tensor\n",
    "        image_np = np.array(resized_frame)\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "        # DurchfÃ¼hrung der Objekterkennung\n",
    "        detections = detect_fn(input_tensor)\n",
    "\n",
    "        # Verarbeitung der erkannten Objekte\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "        detections['num_detections'] = num_detections\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        # Berechne die Skalierungskorrektur fÃ¼r die Box-Koordinaten\n",
    "        height_scale = height / new_size[1]\n",
    "        width_scale = width / new_size[0]\n",
    "        detections['detection_boxes'][:, [0, 2]] *= height_scale\n",
    "        detections['detection_boxes'][:, [1, 3]] *= width_scale\n",
    "\n",
    "        # Speichere die Ergebnisse\n",
    "        all_detections.append(detections)\n",
    "\n",
    "    # Kombiniere alle Erkennungen\n",
    "    # Du kannst hier die Detections nach deinem Bedarf kombinieren oder zusammenfÃ¼hren\n",
    "\n",
    "    # Verwende die Detections von der letzten Skala fÃ¼r die Visualisierung (oder alle)\n",
    "    final_detections = all_detections[-1]  # Hier nehmen wir die Erkennung von der letzten Skala\n",
    "\n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        final_detections['detection_boxes'],\n",
    "        final_detections['detection_classes'] + 1,\n",
    "        final_detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.3,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TFLite for Jetson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\export_tflite_graph_tf2.py \\\n",
    "    --pipeline_config_path={config_path} \\\n",
    "    --trained_checkpoint_dir={TRAINED_CHECKPOINT_PATH} \\\n",
    "    --output_directory={model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZEN_TFLITE_PATH = os.path.join(model_dir, 'saved_model')\n",
    "TFLITE_MODEL = os.path.join(model_dir, 'saved_model', 'detect.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saved_model_dir, TFLITE_MODEL)\n",
    "D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}\\\\saved_model\\\\'.format(name=MODEL_NAME)\n",
    "tflite_model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}\\\\saved_model\\\\detect.tflite'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"tflite_convert \\\n",
    "--saved_model_dir={} \\\n",
    "--output_file={} \\\n",
    "--input_shapes=1,300,300,3 \\\n",
    "--input_arrays=normalized_input_image_tensor \\\n",
    "--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
    "--inference_type=FLOAT \\\n",
    "--allow_custom_ops\".format(FROZEN_TFLITE_PATH, TFLITE_MODEL, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_convert --saved_model_dir=Tensorflow\\workspace\\models\\my_ssd_mobnet\\tfliteexport\\saved_model --output_file=Tensorflow\\workspace\\models\\my_ssd_mobnet\\tfliteexport\\saved_model\\detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --allow_custom_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tflite_convert --saved_model_dir=saved_model_dir --output_file=tflite_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'tf_lite'\n",
    "\n",
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\export_tflite_graph_tf2.py \\\n",
    "    --trained_checkpoint_dir={TRAINED_CHECKPOINT_PATH} \\\n",
    "    --output_directory={output_directory} \\\n",
    "    --pipeline_config_path={config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tflite_convert --saved_model_dir=D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\ --output_file=D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\model.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tflite_model_dir, saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tflite_convert --saved_model_dir=saved_model_dir --output_file=tflite_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster Rcnn results: \n",
    "<br>Faster_RCNN_1024_50_mutant_2: Close 10/10, medium 8.5/10 -> Best model out of all so far\n",
    "<br>Faster_RCNN_640_50_fixed: good 8/10\n",
    "<br>Faster_RCNN_640_50_final: good 7/10\n",
    "<br>Faster_RCNN_640_50_fixed_distanz: good. Close 9/10, medium 3/10\n",
    "<br>Faster_RCNN_short_640_101_unknown: Erkennt 2 immer. Close 5/10, medium 6/10\n",
    "<br>Faster_RCNN_640_50_fixed_withAugmentation_noUnknown: Stopp (4) 10/10. Close 9/10, medium 6/10 -> second best \n",
    "<br>Faster_RCNN_50_320_Step_2: idle State is awfull.\n",
    "<br>Faster_RCNN_50_480_Step_3: Close 0/10, Medium 8/10 -> Idle state is awful\n",
    "<br>Faster_RCNN_50_640_Step_4: idle State is awfull. Close 10/10, Medium depends on class.\n",
    "<br>Faster_RCNN_50_1024_Step_5: 0/10\n",
    "<br>Faster_RCNN_101_320_Step_1: 0/10\n",
    "<br>Faster_RCNN_101_480_Step_2: 0/10\n",
    "### SSD results: Idle state is awful in most cases\n",
    "<br>Efficientdet_8000_640_d1: Close 8/10. Medium 0/10 -> No tracking in medium\n",
    "<br>SSD_4000_320_mobilenet_v1_fpn: 0/10 -> Does not work\n",
    "<br>SSD_4000_640_mobilenet_v1_fpn: 4/10  \n",
    "<br>SSD_8000_640_mobilenet_v1_fpn_custom: 0\n",
    "<br>SSD_8000_640_mobilenet_v2_fpnlite: 0\n",
    "<br>SSD_8000_640_mobilenet_v2_fpnlite_custom: 0\n",
    "<br>SSD_8000_640_resnet101_v1_fpn: 5/10 Close. Medium 0/10 -> Limited tracking in Close\n",
    "<br>ssd_test: 0/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate single Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoint_files = os.listdir(checkpoint_dir)\n",
    "    checkpoint_files = [f for f in checkpoint_files if f.startswith('ckpt-') and '.index' in f]\n",
    "    checkpoint_numbers = [int(re.findall(r'\\d+', f)[0]) for f in checkpoint_files]\n",
    "    if not checkpoint_numbers:\n",
    "        raise ValueError(\"No checkpoints found in the directory.\")\n",
    "    latest_checkpoint = max(checkpoint_numbers)\n",
    "    return os.path.join(checkpoint_dir, f'ckpt-{latest_checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_MODEL = \"SSD_640_mobilenet_v1_fixed_withAugmentation_noUnknown\"\n",
    "MODEL_TYPE = \"ssd\"\n",
    "\n",
    "TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=INFERENCE_MODEL)\n",
    "TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\{type}\\\\{name}'.format(name=INFERENCE_MODEL, type=MODEL_TYPE)\n",
    "#TRAINED_CHECKPOINT_PATH = \"./inference/faster_rcnn/Faster_RCNN_640_50_fixed_withAugmentation_noUnknown/checkpoint/\"\n",
    "TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "\n",
    "'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "\n",
    "VIDEO_NAME = \"30FPS_closeup_allClassesAndUnk\"\n",
    "VIDEO_INPUT_PATH =\"./videoInferenceData/30FPS_Closeup/{video}.mp4\".format(video=VIDEO_NAME)\n",
    "VIDEO_OUTPUT_PATH = \"D:\\\\Desktop-Short\\\\trained_models\\\\{type}\\\\{video}\\\\{name}.mp4\".format(name=INFERENCE_MODEL, type=MODEL_TYPE, video=VIDEO_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH)).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    #input_tensor = input_tensor[tf.newaxis,...]\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "def show_inference(frame):\n",
    "    \n",
    "    image_np = np.array(frame)\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "    output_dict = apply_nms(output_dict)  # NMS anwenden\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes']+1,\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=20,\n",
    "        min_score_thresh=0.5,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    return image_np\n",
    "\n",
    "def apply_nms(output_dict, iou_threshold=0.4):\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    scores = output_dict['detection_scores']\n",
    "    classes = output_dict['detection_classes']\n",
    "    \n",
    "    selected_indices = tf.image.non_max_suppression(\n",
    "        boxes=boxes,\n",
    "        scores=scores,\n",
    "        max_output_size=output_dict['num_detections'],\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=0.4\n",
    "    )\n",
    "    \n",
    "    selected_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "    selected_scores = tf.gather(scores, selected_indices).numpy()\n",
    "    selected_classes = tf.gather(classes, selected_indices).numpy()\n",
    "    \n",
    "    output_dict['detection_boxes'] = selected_boxes\n",
    "    output_dict['detection_scores'] = selected_scores\n",
    "    output_dict['detection_classes'] = selected_classes\n",
    "    output_dict['num_detections'] = len(selected_boxes)\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_video_path, output_video_path, show_stream, name, video):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file {input_video_path}\")\n",
    "        return\n",
    "    \n",
    "    output_dir = os.path.dirname(output_video_path)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    if output_video_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    else:\n",
    "        out = None\n",
    "\n",
    "    frame_count = 0\n",
    "    total_time = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        start_time = time.time()\n",
    "        frame_with_detections = show_inference(frame)\n",
    "        end_time = time.time()\n",
    "\n",
    "        frame_time = end_time - start_time\n",
    "        total_time += frame_time\n",
    "        frame_count += 1\n",
    "\n",
    "        if out:\n",
    "            out.write(frame_with_detections)\n",
    "        \n",
    "        if show_stream:\n",
    "            cv2.imshow('object detection', frame_with_detections)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    if out:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    avg_fps = frame_count / total_time\n",
    "    with open(f\"./myModules/log/{name}_{video}.txt\", \"w\") as f:\n",
    "                f.write(f\"Model:{name}, on Video:{video}\\n\")\n",
    "                f.write(f\"Processed {frame_count} frames in {total_time:.2f} seconds\\n\")\n",
    "                f.write(f\"Average FPS: {avg_fps:.2f}\")\n",
    "    print(f\"Processed {frame_count} frames in {total_time:.2f} seconds\")\n",
    "    print(f\"Average FPS: {avg_fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(VIDEO_INPUT_PATH, VIDEO_OUTPUT_PATH, False, INFERENCE_MODEL, VIDEO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate multiple models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top model:\n",
    "#   Faster_RCNN_1024_50 -> Ohne Unknown Labels testen und eventuell Augmentierung hinzufÃ¼gen + Anchors\n",
    "#       -> Faster_RCNN_1024_50_mutant -> von 8000 -> 16k steps, Groessere Anchors, Augmentierung unveraendert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaputt:\n",
    "# SSD_8000_640_mobilenet_v1_fpn_custom\n",
    "# SSD_640_101_fixed_withAugmentation_noUnknown\n",
    "# Faster_RCNN_short_640_101\n",
    "#\n",
    "# Just bad:\n",
    "#    \"Faster_RCNN_50_480_Step_2\"\n",
    "#    \"Faster_RCNN_50_1024_Step_5\"\n",
    "#    \"Faster_RCNN_101_480_Step_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:\n",
    "#    \"SSD_1024_101_fixed_withAugmentation_noUnknown\",\n",
    "#    \"SSD_fpn_640_101\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate = [\n",
    "    {\n",
    "        \"name\": \"Efficientdet_8000_640_d1\",\n",
    "        \"type\": \"ssd\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SSD_640_101_fixed_withAugmentation_noUnknown2\",\n",
    "        \"type\": \"ssd\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SSD_4000_320_mobilenet_v1_fpn\",\n",
    "        \"type\": \"ssd\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SSD_4000_640_mobilenet_v1_fpn\",\n",
    "        \"type\": \"ssd\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SSD_8000_640_mobilenet_v2_fpnlite\",\n",
    "        \"type\": \"ssd\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SSD_8000_640_mobilenet_v2_fpnlite_custom\",\n",
    "        \"type\": \"ssd\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SSD_8000_640_resnet101_v1_fpn\",\n",
    "        \"type\": \"ssd\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ssd_test\",\n",
    "        \"type\": \"ssd\",\n",
    "        \"label_map\": \"./myModules/label_map_short.pbtxt\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate = [\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_50_480_Step_3\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_50_640_Step_4\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_50_1024_Step_5\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_101_320_Step_1\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_101_480_Step_2\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50_final\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_unk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50_fixed\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50_fixed_distanz\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50_fixed_withAugmentation_noUnknown\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_short_640_101\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_short_640_101_test\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_short_640_101_unknown\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_unk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_short_640_101_unknown_long\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_unk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_unk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_1024_50\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_unk.pbtxt\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate = [\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_50_320_Step_2\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_50_640_Step_4\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_101_320_Step_1\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50_final\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_unk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50_fixed\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50_fixed_distanz\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50_fixed_withAugmentation_noUnknown\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_short_640_101_test\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_short_640_101_unknown\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_unk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_short_640_101_unknown_long\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_unk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_unk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_1024_50\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_unk.pbtxt\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate = [\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_1024_50_mutant_2\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50_fixed_withAugmentation_noUnknown_dropout50\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Faster_RCNN_640_50_fixed_withAugmentation_noUnknown_dropout\",\n",
    "        \"type\": \"faster_rcnn\",\n",
    "        \"label_map\": \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_NAME = \"30FPS_closeup_tracking_allClassesAndUnk\"\n",
    "VIDEO_INPUT_PATH =\"./videoInferenceData/30FPS_Closeup/{video}.mp4\".format(video=VIDEO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config_path, checkpoint_path, label_map_path):\n",
    "    configs = config_util.get_configs_from_pipeline_file(config_path)\n",
    "    detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "    \n",
    "    ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "    ckpt.restore(get_latest_checkpoint(checkpoint_path)).expect_partial()\n",
    "    \n",
    "    category_index = label_map_util.create_category_index_from_labelmap(label_map_path, use_display_name=True)\n",
    "    \n",
    "    @tf.function\n",
    "    def detect_fn(image):\n",
    "        image, shapes = detection_model.preprocess(image)\n",
    "        prediction_dict = detection_model.predict(image, shapes)\n",
    "        detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "        return detections\n",
    "    \n",
    "    return detect_fn, category_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, detect_fn):\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    #input_tensor = input_tensor[tf.newaxis,...]\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "def show_inference(frame, category_index, fn):\n",
    "    \n",
    "    image_np = np.array(frame)\n",
    "    output_dict = run_inference_for_single_image(image_np, fn)\n",
    "    output_dict = apply_nms(output_dict)  # NMS anwenden\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes']+1,\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=20,\n",
    "        min_score_thresh=0.5,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    return image_np\n",
    "\n",
    "def apply_nms(output_dict, iou_threshold=0.4):\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    scores = output_dict['detection_scores']\n",
    "    classes = output_dict['detection_classes']\n",
    "    \n",
    "    selected_indices = tf.image.non_max_suppression(\n",
    "        boxes=boxes,\n",
    "        scores=scores,\n",
    "        max_output_size=output_dict['num_detections'],\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=0.4\n",
    "    )\n",
    "    \n",
    "    selected_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "    selected_scores = tf.gather(scores, selected_indices).numpy()\n",
    "    selected_classes = tf.gather(classes, selected_indices).numpy()\n",
    "    \n",
    "    output_dict['detection_boxes'] = selected_boxes\n",
    "    output_dict['detection_scores'] = selected_scores\n",
    "    output_dict['detection_classes'] = selected_classes\n",
    "    output_dict['num_detections'] = len(selected_boxes)\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_video_path, show_stream, models):\n",
    "    # Use the corresponding model settings\n",
    "    for model_info in models:\n",
    "        model_name = model_info[\"name\"]\n",
    "        model_type = model_info[\"type\"]\n",
    "        label_map_path = model_info[\"label_map\"]\n",
    "        VIDEO_OUTPUT_PATH = \"D:\\\\Desktop-Short\\\\videoInferenceModels\\\\{type}\\\\{video}\\\\{name}.mp4\".format(name=model_name, type=model_type, video=VIDEO_NAME)\n",
    "        TRAINED_CONFIG_PATH = f\"./myModules/configs/{model_name}_config.config\"\n",
    "        TRAINED_CHECKPOINT_PATH = f'D:\\\\Desktop-Short\\\\trained_models\\\\{model_type}\\\\{model_name}'\n",
    "        \n",
    "        detect_fn, category_index = load_model(TRAINED_CONFIG_PATH, TRAINED_CHECKPOINT_PATH, label_map_path)\n",
    "        print(\"Working on:\", model_name,model_type, VIDEO_NAME)\n",
    "        print(VIDEO_OUTPUT_PATH)\n",
    "        \n",
    "        output_dir = os.path.dirname(VIDEO_OUTPUT_PATH)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        cap = cv2.VideoCapture(input_video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error opening video file {input_video_path}\")\n",
    "            return\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        if VIDEO_OUTPUT_PATH:\n",
    "\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, fps, (width, height))\n",
    "            if not out.isOpened():\n",
    "                print(f\"Error opening video writer {VIDEO_OUTPUT_PATH}\")\n",
    "                return\n",
    "        else:\n",
    "            out = None\n",
    "\n",
    "        frame_count = 0\n",
    "        total_time = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            start_time = time.time()\n",
    "            frame_with_detections = show_inference(frame, category_index, detect_fn)\n",
    "            end_time = time.time()\n",
    "\n",
    "            frame_time = end_time - start_time\n",
    "            total_time += frame_time\n",
    "            frame_count += 1\n",
    "\n",
    "            if out:\n",
    "                if (frame_with_detections.shape[1], frame_with_detections.shape[0]) == (width, height):\n",
    "                    out.write(frame_with_detections)\n",
    "                else:\n",
    "                    print(f\"Frame size mismatch: Expected ({width}, {height}), got ({frame_with_detections.shape[1]}, {frame_with_detections.shape[0]})\")\n",
    "            \n",
    "            if show_stream:\n",
    "                cv2.imshow('object detection', frame_with_detections)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "        cap.release()\n",
    "        if out:\n",
    "            out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        avg_fps = frame_count / total_time\n",
    "        log_dir = \"./myModules/log/\"\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        log_file_path = os.path.join(log_dir, f\"{model_name}_{VIDEO_NAME}.txt\")\n",
    "        with open(log_file_path, \"w\") as f:\n",
    "            f.write(f\"Model:{model_name}\\n\")\n",
    "            f.write(f\"Processed {frame_count} frames in {total_time:.2f} seconds\\n\")\n",
    "            f.write(f\"Average FPS: {avg_fps:.2f}\")\n",
    "        \n",
    "        print(f\"Processed {frame_count} frames in {total_time:.2f} seconds\")\n",
    "        print(f\"Average FPS: {avg_fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(VIDEO_INPUT_PATH, show_stream=False, models=models_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Videostream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "           \n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    cv2.imshow('object detection', frame_with_detections)\n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracker testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanBoxTracker:\n",
    "    count = 0\n",
    "\n",
    "    def __init__(self, bbox):\n",
    "        self.kalman = cv2.KalmanFilter(4, 2)\n",
    "        self.kalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\n",
    "        self.kalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\n",
    "        self.kalman.processNoiseCov = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32) * 0.03\n",
    "        self.kalman.measurementNoiseCov = np.array([[1, 0], [0, 1]], np.float32) * 0.5\n",
    "\n",
    "        self.kalman.statePre[:2, 0] = bbox[:2]\n",
    "        self.kalman.statePre[2:, 0] = 0\n",
    "\n",
    "        self.id = KalmanBoxTracker.count\n",
    "        KalmanBoxTracker.count += 1\n",
    "\n",
    "    def predict(self):\n",
    "        return self.kalman.predict()\n",
    "\n",
    "    def update(self, bbox):\n",
    "        self.kalman.correct(bbox[:2])\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH)).expect_partial()\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "def show_inference(frame, trackers, detection_interval, frame_count):\n",
    "    image_np = np.array(frame)\n",
    "    output_dict = {}\n",
    "\n",
    "    detection_boxes = np.array([])\n",
    "    detection_scores = np.array([])\n",
    "    detection_classes = np.array([])\n",
    "\n",
    "    if frame_count % detection_interval == 0:\n",
    "        output_dict = run_inference_for_single_image(image_np)\n",
    "        detection_boxes = output_dict.get('detection_boxes', np.array([]))\n",
    "        detection_scores = output_dict.get('detection_scores', np.array([]))\n",
    "        detection_classes = output_dict.get('detection_classes', np.array([]))\n",
    "\n",
    "        # Debugging-Ausgaben\n",
    "        print(f\"Raw Detection Boxes: {detection_boxes}\")\n",
    "        print(f\"Raw Detection Scores: {detection_scores}\")\n",
    "        print(f\"Raw Detection Classes: {detection_classes}\")\n",
    "\n",
    "        # Filter nur die Boxen mit einer Score grÃ¶ÃŸer als 0.5\n",
    "        valid_indices = np.where(detection_scores > 0.5)[0]\n",
    "        detection_boxes = detection_boxes[valid_indices]\n",
    "        detection_scores = detection_scores[valid_indices]\n",
    "        detection_classes = detection_classes[valid_indices]\n",
    "\n",
    "        # Debugging-Ausgaben nach dem Filtern\n",
    "        print(f\"Filtered Detection Boxes: {detection_boxes}\")\n",
    "        print(f\"Filtered Detection Scores: {detection_scores}\")\n",
    "        print(f\"Filtered Detection Classes: {detection_classes}\")\n",
    "\n",
    "        trackers.clear()\n",
    "        for box in detection_boxes:\n",
    "            ymin, xmin, ymax, xmax = box\n",
    "            bbox = np.array([xmin * frame.shape[1], ymin * frame.shape[0]], dtype=np.float32)\n",
    "            tracker = KalmanBoxTracker(bbox)\n",
    "            trackers.append(tracker)\n",
    "\n",
    "    updated_boxes = []\n",
    "    for tracker in trackers:\n",
    "        prediction = tracker.predict()\n",
    "        x, y, dx, dy = prediction.flatten()\n",
    "        xmin = max(0, x - 0.5 * dx)\n",
    "        ymin = max(0, y - 0.5 * dy)\n",
    "        xmax = min(frame.shape[1], x + 0.5 * dx)\n",
    "        ymax = min(frame.shape[0], y + 0.5 * dy)\n",
    "        updated_boxes.append([ymin, xmin, ymax, xmax])\n",
    "\n",
    "    updated_boxes = np.array(updated_boxes)\n",
    "    if updated_boxes.size == 0:\n",
    "        updated_boxes = np.array([[0, 0, 0, 0]])\n",
    "\n",
    "    # Sicherstellen, dass die Arrays immer Werte enthalten\n",
    "    if detection_classes.size == 0:\n",
    "        detection_classes = np.array([0])  # Default Class ID\n",
    "    if detection_scores.size == 0:\n",
    "        detection_scores = np.array([0])  # Default Score\n",
    "\n",
    "    # Debugging-Ausgaben fÃ¼r Ã¼berprÃ¼fte Arrays\n",
    "    print(f\"Updated Boxes: {updated_boxes}\")\n",
    "    print(f\"Detection Classes (post-filtering): {detection_classes}\")\n",
    "    print(f\"Detection Scores (post-filtering): {detection_scores}\")\n",
    "\n",
    "    # Sicherstellen, dass Boxen und Scores gÃ¼ltig sind, bevor sie visualisiert werden\n",
    "    if detection_boxes.size > 0 and detection_scores.size > 0 and detection_classes.size > 0:\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            updated_boxes,\n",
    "            detection_classes + 1,  # Ensure class IDs are correct\n",
    "            detection_scores,\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=20,\n",
    "            min_score_thresh=0.5,\n",
    "            agnostic_mode=False,\n",
    "            line_thickness=5\n",
    "        )\n",
    "    else:\n",
    "        print(\"Keine gÃ¼ltigen Boxen, Klassen oder Scores zum Visualisieren.\")\n",
    "\n",
    "    return image_np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "trackers = []\n",
    "detection_interval = 10\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    frame_with_detections = show_inference(frame, trackers, detection_interval, frame_count)\n",
    "    \n",
    "    cv2.imshow('object detection', frame_with_detections)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
