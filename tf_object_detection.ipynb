{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import cv2 \n",
    "import subprocess\n",
    "import time\n",
    "import uuid\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as minidom\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config_path = './pipeline.config'\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(base_config_path)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "print(\"Model was successfully built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "#Checking for gpu\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "   raise SystemError('GPU device not found')\n",
    "\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(21)\n",
    "tf.compat.v1.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id_mapping:\n",
    "    # 1:'Speed limit (30km/h)', \n",
    "    # 2:'Speed limit (50km/h)',\n",
    "    # 12:'Priority road', \n",
    "    # 14:'Stop', \n",
    "    # 17:'No entry',\n",
    "    # 41:'Ende des Ãœberholverbots',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE_TRAIN = 50 # Default: 50 Images of each class excluding values in :id_mapping. Used for Trainset\n",
    "SAMPLE_SIZE_TEST = 10 # Default: 10 Images of each class excluding values in :id_mapping . Used for Testset\n",
    "\n",
    "IMAGE_WIDTH = 280 # Image width in pixels to resize for visualization\n",
    "IMAGE_HEIGHT = 280 # Image height in pixels to resize for visualization\n",
    "\n",
    "train_path = './GTSRB/Final_Training/Images' # Location of the training images\n",
    "test_path = './GTSRB/Final_Test/Images' # Location of the test images\n",
    "\n",
    "id_mapping = {1: 1, 2: 2, 12: 3, 14: 4, 17: 5}\n",
    "unknown_label = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8\"\n",
    "MODEL_NAME = \"SSD_640_mobilenet_v1_fixed_withAugmentation_noUnknown\"\n",
    "IS_SSD = True # True if SSD model is used. False otherwise \n",
    "TRANSERLEARNING = False\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 5\n",
    "NUM_STEPS = 10000\n",
    "NUM_EVAL_STEPS = 1000\n",
    "use_bfloat16 = False # Use bfloat16 True for TPU\n",
    "\n",
    "NOTES = \"\" # Training Notes for the log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for Evaluation\n",
    "if IS_SSD:\n",
    "    TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=MODEL_NAME)\n",
    "    TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)\n",
    "    TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    MODEL_TYPE = \"ssd\"\n",
    "else:\n",
    "    TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=MODEL_NAME)\n",
    "    TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "    TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "    MODEL_TYPE = \"faster_rcnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSERLEARNING:\n",
    "        base_config_path = \"./inference/faster_rcnn/{name}/pipeline.config\".format(name=BASE_MODEL)\n",
    "        base_checkpoint_path = \"./inference/faster_rcnn/{name}/checkpoint/ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "        # Save to path\n",
    "        model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "        config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "        # Upload from path\n",
    "        labelmap_path = \"./myModules/label_map_short.pbtxt\"\n",
    "        train_record_path = \"./myModules/records/train.record\"\n",
    "        test_record_path = \"./myModules/records/test.record\"\n",
    "        \n",
    "        inference_path = './inference/faster_rcnn/{name}'.format(name=MODEL_NAME)\n",
    "else:\n",
    "    if IS_SSD:\n",
    "        # Download from path \n",
    "        base_config_path = \"./base_models/ssd/{name}/pipeline.config\".format(name=BASE_MODEL)\n",
    "        base_checkpoint_path = \"./base_models/ssd/{name}/checkpoint/ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "        # Save to path\n",
    "        model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)\n",
    "        config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "        # Upload from path\n",
    "        labelmap_path = \"./myModules/label_map_short_woUnk.pbtxt\"\n",
    "        train_record_path = \"./myModules/records/trainWoUnknown.record\"\n",
    "        test_record_path = \"./myModules/records/testWoUnknown.record\"\n",
    "        \n",
    "        inference_path = './inference/ssd/{name}'.format(name=MODEL_NAME)\n",
    "    else:\n",
    "        base_config_path = \"D:\\\\Desktop-Short\\\\base_models\\\\faster_rcnn\\\\{name}\\\\pipeline.config\".format(name=BASE_MODEL)\n",
    "        base_checkpoint_path = \"D:\\\\Desktop-Short\\\\base_models\\\\{name}\\\\checkpoint\\\\ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "        # Save to path\n",
    "        model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "        config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "        # Upload from path\n",
    "        labelmap_path = \"./myModules/label_map_short.pbtxt\"\n",
    "        train_record_path = \"./myModules/records/train.record\"\n",
    "        test_record_path = \"./myModules/records/test.record\"\n",
    "        \n",
    "        inference_path = './inference/faster_rcnn/{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTrain(rootpath, cache_path):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "    Arguments:\n",
    "        rootpath: path to the traffic sign data, for example './GTSRB/Training'\n",
    "        cache_path: path to the cached DataFrame file, default is 'traffic_signs_data.pkl'\n",
    "    Returns:\n",
    "        DataFrame containing labels, image shapes, ROIs, and image paths\n",
    "    '''\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading: {cache_path}\")\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    data = []\n",
    "    # loop over all 43 classes\n",
    "    for c in range(0, 43):\n",
    "        prefix = f\"{rootpath}/{c:05d}/\"  # subdirectory for class\n",
    "        gtFile = open(prefix + f'GT-{c:05d}.csv')  # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';')  # csv parser for annotations file\n",
    "        next(gtReader)  # skip header\n",
    "\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            img_path = prefix + row[0]\n",
    "            img = plt.imread(img_path)  # load image\n",
    " \n",
    "            label = int(row[7])  # the 8th column is the label\n",
    "            height = img.shape[0]  # height of the image\n",
    "            width = img.shape[1]  # width of the image\n",
    "            #channels = img.shape[2] if len(img.shape) > 2 else 1  # channels of the image (default to 1 if grayscale)\n",
    "            roi_x1 = int(row[3])  # ROI X1 coordinate\n",
    "            roi_y1 = int(row[4])  # ROI Y1 coordinate\n",
    "            roi_x2 = int(row[5])  # ROI X2 coordinate\n",
    "            roi_y2 = int(row[6])  # ROI Y2 coordinate\n",
    "\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "\n",
    "        gtFile.close()\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    \n",
    "    df.to_pickle(cache_path)\n",
    "    print(f\"Dataframe saved in: {cache_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTest(rootpath, cache_path):\n",
    "    '''Reads the final test data for the German Traffic Sign Recognition Benchmark.\n",
    "    Arguments: path to the final test data, for example './GTSRB/Final_Test/Images'\n",
    "    Returns: DataFrame with image data, labels, image shapes, and ROI\n",
    "    '''\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading: {cache_path}\")\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    # Pfad zur CSV-Datei\n",
    "    csv_file = os.path.join(rootpath, 'GT-final_test_gt.csv')\n",
    "    \n",
    "    # Lade die CSV-Datei\n",
    "    df = pd.read_csv(csv_file, delimiter=';')\n",
    "    \n",
    "    # Liste zum Speichern der Daten\n",
    "    data = []\n",
    "    \n",
    "    # Schleife Ã¼ber alle Zeilen der CSV-Datei\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.join(rootpath, row['Filename'])\n",
    "        img = plt.imread(img_path)  # Lade das Bild\n",
    "        \n",
    "        if img is not None:\n",
    "            label = int(row['ClassId'])\n",
    "            height, width, channels = img.shape\n",
    "            roi_x1 = int(row['Roi.X1'])\n",
    "            roi_y1 = int(row['Roi.Y1'])\n",
    "            roi_x2 = int(row['Roi.X2'])\n",
    "            roi_y2 = int(row['Roi.Y2'])\n",
    "            \n",
    "            # FÃ¼ge die Daten als Zeile hinzu\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "        else:\n",
    "            print(f\"Bild {img_path} konnte nicht geladen werden.\")\n",
    "    \n",
    "    # Erstelle ein DataFrame aus der Liste\n",
    "    df_test = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    df_test.to_pickle(cache_path)\n",
    "    print(f\"Dataframe saved in: {cache_path}\")\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))  # Resize to 280x280\n",
    "    return np.array(image).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_labels(df, selected_labels, unknown_label, sample_size):\n",
    "    \"\"\"\n",
    "    Reassign labels in the dataframe according to selected_labels.\n",
    "    Keep all rows for the labels in selected_labels.\n",
    "    For remaining labels, sample a specified number of rows and reassign them to unknown_label.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with a column named 'Label'.\n",
    "    selected_labels (dict): A dictionary mapping old labels to new labels.\n",
    "    unknown_label (int): The label to assign to the remaining sampled rows.\n",
    "    sample_size (int): The number of rows to sample for each remaining label. Default is 15.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new dataframe with reassigned labels.\n",
    "    \"\"\"\n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    for old_label, new_label in selected_labels.items():\n",
    "        label_df = df[df['Label'] == old_label].copy()\n",
    "        label_df['Label'] = new_label\n",
    "        new_df = pd.concat([new_df, label_df])\n",
    "\n",
    "    unique_labels = df['Label'].unique()\n",
    "    remaining_labels = [label for label in unique_labels if label not in selected_labels]\n",
    "\n",
    "    for label in remaining_labels:\n",
    "        label_df = df[df['Label'] == label]\n",
    "        if len(label_df) > sample_size:\n",
    "            selected_rows = label_df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            selected_rows = label_df\n",
    "        selected_rows['Label'] = unknown_label\n",
    "        new_df = pd.concat([new_df, selected_rows])\n",
    "\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Dataset for YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN_DF = \"./myModules/data/df_train_raw.pkl\"\n",
    "PATH_TO_TEST_DF = \"./myModules/data/df_test_raw.pkl\"\n",
    "\n",
    "PATH_TO_TRAIN_ANNOTATIONS = './yoloNoUnkData/Train/Annotations/'\n",
    "PATH_TO_TRAIN_IMAGES = './yoloNoUnkData/Train/Images/'\n",
    "\n",
    "PATH_TO_TEST_ANNOTATIONS = './yoloNoUnkData/Test/Annotations/'\n",
    "PATH_TO_TEST_IMAGES = './yoloNoUnkData/Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_xml(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ET.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voc_annotation_for_trainset(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        annotation = ET.Element('annotation')\n",
    "        \n",
    "        # Extract subfolder name (e.g., '0000')\n",
    "        subfolder_name = os.path.basename(os.path.dirname(row['Path']))\n",
    "        \n",
    "        # Create unique filename with subfolder prefix (e.g., '0000_00000_00000.xml')\n",
    "        os.path.basename(row['Path']).replace('.ppm', '.jpg')\n",
    "        filename = f\"{subfolder_name}_{os.path.splitext(os.path.basename(row['Path']))[0]}.xml\"\n",
    "        \n",
    "        # Create annotation elements\n",
    "        ET.SubElement(annotation, 'folder').text = 'Images'\n",
    "        ET.SubElement(annotation, 'filename').text = filename.replace('.xml', '.jpg')\n",
    "        \n",
    "        size = ET.SubElement(annotation, 'size')\n",
    "        ET.SubElement(size, 'width').text = str(row['Width'])\n",
    "        ET.SubElement(size, 'height').text = str(row['Height'])\n",
    "        ET.SubElement(size, 'depth').text = '3'  # Assuming RGB images\n",
    "\n",
    "        segmented = ET.SubElement(annotation, 'segmented').text = '0'\n",
    "        \n",
    "        obj = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(obj, 'name').text = str(row['Label'])\n",
    "        ET.SubElement(obj, 'pose').text = 'Unspecified'\n",
    "        ET.SubElement(obj, 'truncated').text = '0'\n",
    "        ET.SubElement(obj, 'difficult').text = '0'\n",
    "        \n",
    "        bndbox = ET.SubElement(obj, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(row['Roi.X1'])\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(row['Roi.Y1'])\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(row['Roi.X2'])\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(row['Roi.Y2'])\n",
    "        \n",
    "        # Write XML file with pretty print\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(prettify_xml(annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voc_annotation_for_testset(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        annotation = ET.Element('annotation')\n",
    "        ET.SubElement(annotation, 'folder').text = 'Images'\n",
    "        ET.SubElement(annotation, 'filename').text = os.path.basename(row['Path']).replace('.ppm', '.jpg')\n",
    "        \n",
    "        size = ET.SubElement(annotation, 'size')\n",
    "        ET.SubElement(size, 'width').text = str(row['Width'])\n",
    "        ET.SubElement(size, 'height').text = str(row['Height'])\n",
    "        ET.SubElement(size, 'depth').text = '3'  # Assuming RGB images\n",
    "\n",
    "        segmented = ET.SubElement(annotation, 'segmented').text = '0'\n",
    "        \n",
    "        obj = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(obj, 'name').text = str(row['Label'])\n",
    "        ET.SubElement(obj, 'pose').text = 'Unspecified'\n",
    "        ET.SubElement(obj, 'truncated').text = '0'\n",
    "        ET.SubElement(obj, 'difficult').text = '0'\n",
    "        \n",
    "        bndbox = ET.SubElement(obj, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(row['Roi.X1'])\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(row['Roi.Y1'])\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(row['Roi.X2'])\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(row['Roi.Y2'])\n",
    "        \n",
    "        # Write XML file with pretty print\n",
    "        output_file = os.path.join(output_dir, os.path.basename(row['Path']).replace('.ppm', '.xml'))\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(prettify_xml(annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ppm_to_jpg_from_df_train(df, jpg_root):\n",
    "    if not os.path.exists(jpg_root):\n",
    "        os.makedirs(jpg_root)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        img_path = row['Path']\n",
    "        if img_path.endswith('.ppm'):\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Extrahiere den Dateinamen und den Subordner aus dem Pfad\n",
    "            folder_name = os.path.basename(os.path.dirname(img_path))  # Subordner\n",
    "            filename = os.path.basename(img_path).replace('.ppm', '.jpg')\n",
    "            jpg_filename = f\"{folder_name}_{filename}\"  # FÃ¼ge den Subordner-Namen vor dem Dateinamen hinzu\n",
    "            \n",
    "            # Erstelle den Ziel-JPEG-Pfad\n",
    "            jpg_path = os.path.join(jpg_root, jpg_filename)\n",
    "            \n",
    "            # Speichere das Bild im JPEG-Format\n",
    "            img.save(jpg_path, 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ppm_to_jpg_from_df(df, jpg_dir):\n",
    "    if not os.path.exists(jpg_dir):\n",
    "        os.makedirs(jpg_dir)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        img_path = row['Path']\n",
    "        if img_path.endswith('.ppm'):\n",
    "            img = Image.open(img_path)\n",
    "            # Extrahiere den Dateinamen und den Unterordner aus dem Pfad\n",
    "            subfolder = os.path.basename(os.path.dirname(img_path))\n",
    "            filename = os.path.basename(img_path).replace('.ppm', '.jpg')\n",
    "            # Erstelle den Zielpfad inklusive Unterordner\n",
    "            jpg_subdir = os.path.join(jpg_dir, subfolder)\n",
    "            if not os.path.exists(jpg_subdir):\n",
    "                os.makedirs(jpg_subdir)\n",
    "            jpg_path = os.path.join(jpg_subdir, filename)\n",
    "            img.save(jpg_path, 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = readTrafficSignsTrain(train_path, PATH_TO_TRAIN_DF)\n",
    "create_voc_annotation_for_trainset(df_final_train, PATH_TO_TRAIN_ANNOTATIONS)\n",
    "convert_ppm_to_jpg_from_df_train(df_final_train, PATH_TO_TRAIN_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = readTrafficSignsTest(test_path, PATH_TO_TEST_DF)\n",
    "create_voc_annotation_for_testset(df_final_test, PATH_TO_TEST_ANNOTATIONS)\n",
    "convert_ppm_to_jpg_from_df(df_final_test, PATH_TO_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./myModules/data/df_train_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "df_train_cache = readTrafficSignsTrain(train_path, \"./myModules/data/df_train_raw.pkl\")\n",
    "df_train = pd.DataFrame()\n",
    "df_train = df_train_cache.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./myModules/data/df_test_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "df_test_cache = readTrafficSignsTest(test_path, \"./myModules/data/df_test_raw.pkl\")\n",
    "df_test = pd.DataFrame()\n",
    "df_test = df_test_cache.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_train = reassign_labels(df_train, id_mapping, unknown_label, SAMPLE_SIZE_TRAIN)\n",
    "df_final_test = reassign_labels(df_test, id_mapping, unknown_label, SAMPLE_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to exlude the unknown Labels\n",
    "df_final_train = df_final_train[df_final_train['Label'] != 6]\n",
    "df_final_test = df_final_test[df_final_test['Label'] != 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Verfify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "short_classes = { 1: 'Geschwindigkeitsbegrenzung (30km/h)', \n",
    "                  2: 'Geschwindigkeitsbegrenzung (50km/h)', \n",
    "                  3: 'VorrangstraÃŸe', \n",
    "                  4: 'Stop', \n",
    "                  5: 'Einfahrt verboten',\n",
    "                  6: 'Unbekannt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_train['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_test['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_label_df = df_final_train[df_final_train['Label'] == 6]\n",
    "\n",
    "# WÃ¤hle zufÃ¤llig 100 Bilder aus\n",
    "image_paths_to_display = unknown_label_df['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "# Plotten der ausgewÃ¤hlten Bilder\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_final = df_final_train['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_final)\n",
    "\n",
    "label_counts_named_final = {short_classes[key]: value for key, value in label_counts_final.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_final.keys()), y=list(label_counts_named_final.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen fÃ¼r bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_test = df_final_test['Label'].value_counts().sort_index()\n",
    "label_counts_train = df_final_train['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_train = {short_classes[key]: value for key, value in label_counts_train.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_train.keys()), y=list(label_counts_named_train.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen fÃ¼r bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_test = {short_classes[key]: value for key, value in label_counts_test.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_test.keys()), y=list(label_counts_named_test.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Testdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen fÃ¼r bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Tf-records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(row):\n",
    "    img_path = row['Path']\n",
    "    # Lade das Bild und konvertiere es in ein kompatibles Format (z.B. JPEG)\n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    with io.BytesIO() as output:\n",
    "        image.save(output, format=\"JPEG\")\n",
    "        encoded_jpg = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = row['Path'].encode('utf8')\n",
    "    image_format = b'jpeg'  # Ã„ndere dies entsprechend des konvertierten Bildformats\n",
    "    xmins = [row['Roi.X1'] / width]\n",
    "    xmaxs = [row['Roi.X2'] / width]\n",
    "    ymins = [row['Roi.Y1'] / height]\n",
    "    ymaxs = [row['Roi.Y2'] / height]\n",
    "    classes_text = [str(row['Label']).encode('utf8')]\n",
    "    classes = [int(row['Label'])]\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_jpg])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n",
    "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n",
    "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n",
    "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n",
    "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n",
    "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(df, output_path):\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    for _, row in df.iterrows():\n",
    "        tf_example = create_tf_example(row)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tf_record(df_final_train, './myModules/records/trainWoUnknown.record')\n",
    "create_tf_record(df_final_test, './myModules/records/testWoUnknown.record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Verify Tensord records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_example(serialized_example):\n",
    "    feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/class/text': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(record_file):\n",
    "    raw_dataset = tf.data.TFRecordDataset(record_file)\n",
    "    parsed_dataset = raw_dataset.map(parse_tf_example)\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_at_index(parsed_dataset, index):\n",
    "    for i, tf_example in enumerate(parsed_dataset):\n",
    "        if i == index:\n",
    "            height = tf_example['image/height'].numpy()\n",
    "            width = tf_example['image/width'].numpy()\n",
    "            encoded_image = tf_example['image/encoded'].numpy()\n",
    "            xmin = tf_example['image/object/bbox/xmin'].numpy()\n",
    "            xmax = tf_example['image/object/bbox/xmax'].numpy()\n",
    "            ymin = tf_example['image/object/bbox/ymin'].numpy()\n",
    "            ymax = tf_example['image/object/bbox/ymax'].numpy()\n",
    "            label = tf_example['image/object/class/label'].numpy()\n",
    "\n",
    "            image = tf.image.decode_jpeg(encoded_image)\n",
    "            image_np = image.numpy()\n",
    "\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(image_np)\n",
    "            plt.title(f'Label: {label}')\n",
    "\n",
    "            # Draw the bounding box\n",
    "            plt.gca().add_patch(plt.Rectangle((xmin * width, ymin * height), \n",
    "                                              (xmax - xmin) * width, (ymax - ymin) * height,\n",
    "                                              edgecolor='green', facecolor='none', linewidth=2))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record = read_tfrecord('./myModules/records/train.record')\n",
    "test_record = read_tfrecord('./myModules/records/test.record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_at_index(test_record, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_at_index(train_record, 9000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_length(df):\n",
    "    count_images = len(df)\n",
    "    labels_count = df['Label'].value_counts()\n",
    "\n",
    "    sorted_labels_count = labels_count.sort_index()\n",
    "    return sorted_labels_count.tolist(), count_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_info(batch, classes, steps, eval_steps, name, bfloat16, pipeline, \n",
    "                   checkpoint_path, labelmap, train_record, test_record, config, \n",
    "                   trained_model, train_time, df_train, df_test, notes):\n",
    "    \n",
    "    \n",
    "    test_count, test_len = count_length(df_test)\n",
    "    train_count, train_len = count_length(df_train)   \n",
    "    log_contents = f\"\"\"\n",
    "Trainingparameters:\n",
    "    BATCH_SIZE = {batch} # Cannot be higher than 2-4 (lack of ressources)\n",
    "    NUM_CLASSES = {classes} # Total number of classes to train is 43 - used for training are only 6-7\n",
    "    NUM_STEPS = {steps}\n",
    "    NUM_EVAL_STEPS = {eval_steps}\n",
    "    MODEL_NAME = {name}\n",
    "    use_bfloat16 = {bfloat16} # Use bfloat16 = True for trainign with TPU\n",
    "    \n",
    "Locations: \n",
    "    Path to Pipeline-Config = {pipeline}\n",
    "    Checkpoint from transfermodel  = {checkpoint_path}\n",
    "\n",
    "    Path to the trainedmodel = {trained_model}\n",
    "    Config of the trainedmodel = {config}\n",
    "    \n",
    "Used records: \n",
    "    Used labelmap = {labelmap}\n",
    "    Used train_record = {train_record}\n",
    "    Used test_record = {test_record}\n",
    "\n",
    "Generell Information: \n",
    "    Time needed for modeltraining = {train_time}\n",
    "    Length of traindataset = {train_len}\n",
    "    Counted values for each Label form Traindataset = {train_count}\n",
    "    Length of testdataset = {test_len}\n",
    "    Counted values for each Label form Testdataset = {test_count}\n",
    "\"\"\"\n",
    "\n",
    "    log_id = uuid.uuid4()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_entry = f\"ID: {log_id}\\nTimestamp: {timestamp}\\n\\n{log_contents}\\nNotes: {notes}\\n\"\n",
    "\n",
    "    filename = f\"./myModules/log/{name}_{log_id}.txt\"\n",
    "    with open(filename, \"a\") as file:\n",
    "        file.write(log_entry)\n",
    "        file.write(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_config_path) as f:\n",
    "    config = f.read()\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "  \n",
    "  # Set labelmap path\n",
    "  config = re.sub('label_map_path: \".*?\"', \n",
    "             'label_map_path: \"{}\"'.format(labelmap_path), config)\n",
    "  \n",
    "  # Set fine_tune_checkpoint path\n",
    "  config = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                  'fine_tune_checkpoint: \"{}\"'.format(base_checkpoint_path), config)\n",
    "  \n",
    "  # Set train tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(train_record_path), config)\n",
    "  \n",
    "  # Set test tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(test_record_path), config)\n",
    "  \n",
    "  # Set number of classes.\n",
    "  config = re.sub('num_classes: [0-9]+',\n",
    "                  'num_classes: {}'.format(NUM_CLASSES), config)\n",
    "  \n",
    "  # Set batch size\n",
    "  config = re.sub('batch_size: [0-9]+',\n",
    "                  'batch_size: {}'.format(BATCH_SIZE), config)\n",
    "  \n",
    "  # Set training steps\n",
    "  config = re.sub('num_steps: [0-9]+',\n",
    "                  'num_steps: {}'.format(NUM_STEPS), config)\n",
    "  \n",
    "    # Set use_bfloat16\n",
    "  config = re.sub('use_bfloat16: (true|false)',\n",
    "                  'use_bfloat16: {}'.format(str(use_bfloat16).lower()), config)\n",
    "  \n",
    "  # Set fine-tune checkpoint type to detection\n",
    "  config = re.sub('fine_tune_checkpoint_type: \"classification\"', \n",
    "             'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n",
    "  \n",
    "  f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model {\n",
      "  ssd {\n",
      "    num_classes: 5\n",
      "    image_resizer {\n",
      "      fixed_shape_resizer {\n",
      "        height: 640\n",
      "        width: 640\n",
      "      }\n",
      "    }\n",
      "    feature_extractor {\n",
      "      type: \"ssd_mobilenet_v1_fpn_keras\"\n",
      "      depth_multiplier: 1.0\n",
      "      min_depth: 16\n",
      "      conv_hyperparams {\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 3.9999998989515007e-05\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          random_normal_initializer {\n",
      "            mean: 0.0\n",
      "            stddev: 0.009999999776482582\n",
      "          }\n",
      "        }\n",
      "        activation: RELU_6\n",
      "        batch_norm {\n",
      "          decay: 0.996999979019165\n",
      "          scale: true\n",
      "          epsilon: 0.0010000000474974513\n",
      "        }\n",
      "      }\n",
      "      override_base_feature_extractor_hyperparams: true\n",
      "      fpn {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "      }\n",
      "    }\n",
      "    box_coder {\n",
      "      faster_rcnn_box_coder {\n",
      "        y_scale: 10.0\n",
      "        x_scale: 10.0\n",
      "        height_scale: 5.0\n",
      "        width_scale: 5.0\n",
      "      }\n",
      "    }\n",
      "    matcher {\n",
      "      argmax_matcher {\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.5\n",
      "        ignore_thresholds: false\n",
      "        negatives_lower_than_unmatched: true\n",
      "        force_match_for_each_row: true\n",
      "        use_matmul_gather: true\n",
      "      }\n",
      "    }\n",
      "    similarity_calculator {\n",
      "      iou_similarity {\n",
      "      }\n",
      "    }\n",
      "    box_predictor {\n",
      "      weight_shared_convolutional_box_predictor {\n",
      "        conv_hyperparams {\n",
      "          regularizer {\n",
      "            l2_regularizer {\n",
      "              weight: 3.9999998989515007e-05\n",
      "            }\n",
      "          }\n",
      "          initializer {\n",
      "            random_normal_initializer {\n",
      "              mean: 0.0\n",
      "              stddev: 0.009999999776482582\n",
      "            }\n",
      "          }\n",
      "          activation: RELU_6\n",
      "          batch_norm {\n",
      "            decay: 0.996999979019165\n",
      "            scale: true\n",
      "            epsilon: 0.0010000000474974513\n",
      "          }\n",
      "        }\n",
      "        depth: 256\n",
      "        num_layers_before_predictor: 4\n",
      "        kernel_size: 3\n",
      "        class_prediction_bias_init: -4.599999904632568\n",
      "      }\n",
      "    }\n",
      "    anchor_generator {\n",
      "      multiscale_anchor_generator {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "        anchor_scale: 4.0\n",
      "        aspect_ratios: 1.0\n",
      "        aspect_ratios: 2.0\n",
      "        aspect_ratios: 0.5\n",
      "        scales_per_octave: 2\n",
      "      }\n",
      "    }\n",
      "    post_processing {\n",
      "      batch_non_max_suppression {\n",
      "        score_threshold: 9.99999993922529e-09\n",
      "        iou_threshold: 0.6000000238418579\n",
      "        max_detections_per_class: 100\n",
      "        max_total_detections: 100\n",
      "        use_static_shapes: false\n",
      "      }\n",
      "      score_converter: SIGMOID\n",
      "    }\n",
      "    normalize_loss_by_num_matches: true\n",
      "    loss {\n",
      "      localization_loss {\n",
      "        weighted_smooth_l1 {\n",
      "        }\n",
      "      }\n",
      "      classification_loss {\n",
      "        weighted_sigmoid_focal {\n",
      "          gamma: 2.0\n",
      "          alpha: 0.25\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 1.0\n",
      "    }\n",
      "    encode_background_as_zeros: true\n",
      "    normalize_loc_loss_by_codesize: true\n",
      "    inplace_batchnorm_update: true\n",
      "    freeze_batchnorm: false\n",
      "  }\n",
      "}\n",
      "train_config {\n",
      "  batch_size: 8\n",
      "  data_augmentation_options {\n",
      "    random_horizontal_flip {\n",
      "    }\n",
      "  }\n",
      "  data_augmentation_options {\n",
      "    random_crop_image {\n",
      "      min_object_covered: 0.0\n",
      "      min_aspect_ratio: 0.75\n",
      "      max_aspect_ratio: 3.0\n",
      "      min_area: 0.75\n",
      "      max_area: 1.0\n",
      "      overlap_thresh: 0.0\n",
      "    }\n",
      "  }\n",
      "  sync_replicas: true\n",
      "  optimizer {\n",
      "    momentum_optimizer {\n",
      "      learning_rate {\n",
      "        cosine_decay_learning_rate {\n",
      "          learning_rate_base: 0.03999999910593033\n",
      "          total_steps: 25000\n",
      "          warmup_learning_rate: 0.013333000242710114\n",
      "          warmup_steps: 2000\n",
      "        }\n",
      "      }\n",
      "      momentum_optimizer_value: 0.8999999761581421\n",
      "    }\n",
      "    use_moving_average: false\n",
      "  }\n",
      "  fine_tune_checkpoint: \"./base_models/ssd/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
      "  num_steps: 10000\n",
      "  startup_delay_steps: 0.0\n",
      "  replicas_to_aggregate: 8\n",
      "  max_number_of_boxes: 100\n",
      "  unpad_groundtruth_tensors: false\n",
      "  fine_tune_checkpoint_type: \"detection\"\n",
      "  fine_tune_checkpoint_version: V2\n",
      "}\n",
      "train_input_reader {\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/trainWoUnknown.record\"\n",
      "  }\n",
      "}\n",
      "eval_config {\n",
      "  metrics_set: \"coco_detection_metrics\"\n",
      "  use_moving_averages: false\n",
      "  batch_size: 8\n",
      "}\n",
      "eval_input_reader {\n",
      "  label_map_path: \"./myModules/label_map_short_woUnk.pbtxt\"\n",
      "  shuffle: false\n",
      "  num_epochs: 1\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/trainWoUnknown.record\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(config_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsizing larger images to match the size of smaller images is often a better bet than increasing the size of small images to be larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./myModules/configs/SSD_640_mobilenet_v1_fixed_withAugmentation_noUnknown_config.config\n",
      "D:\\Desktop-Short\\trained_models\\ssd\\SSD_640_mobilenet_v1_fixed_withAugmentation_noUnknown\n"
     ]
    }
   ],
   "source": [
    "print(config_path)\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py \\\n",
    "    --pipeline_config_path={config_path} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_workers=4 \\\n",
    "    --num_train_steps={NUM_STEPS} \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --num_eval_steps={NUM_EVAL_STEPS}\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "log_model_info(\n",
    "    batch=BATCH_SIZE,\n",
    "    classes=NUM_CLASSES,\n",
    "    steps=NUM_STEPS,\n",
    "    eval_steps=NUM_EVAL_STEPS,\n",
    "    name=MODEL_NAME,\n",
    "    bfloat16=use_bfloat16,\n",
    "    pipeline=base_config_path,\n",
    "    checkpoint_path=base_checkpoint_path,\n",
    "    labelmap=labelmap_path,\n",
    "    train_record=train_record_path,\n",
    "    test_record=test_record_path,\n",
    "    config=config_path,\n",
    "    trained_model=model_dir,\n",
    "    train_time=training_time,\n",
    "    df_test=df_final_test,\n",
    "    df_train=df_final_train,\n",
    "    notes=NOTES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"SSD_16000_640_mobilenet_v2_fpnlite\"\n",
    "config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\model_main_tf2.py \\\n",
    "            --pipeline_config_path={config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={model_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: SSD_640_101_fixed_withAugmentation_noUnknown2\n",
      "Done Evaluating: SSD_640_101_fixed_withAugmentation_noUnknown2\n",
      "No Error: SSD_640_101_fixed_withAugmentation_noUnknown2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating model: {MODEL_NAME}\")\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "output, error = process.communicate()\n",
    "print(f\"Done Evaluating: {MODEL_NAME}\")\n",
    "if process.returncode != 0:\n",
    "    print(error.decode(\"utf-8\"))\n",
    "else:\n",
    "    print(f\"No Error: {MODEL_NAME}\")\n",
    "    output_str = output.decode(\"utf-8\")\n",
    "    header = f\"Evaluation Results for: {MODEL_NAME}\\n\\n\"\n",
    "    with open(f\"./myModules/log/{MODEL_NAME}_evaluation_results.txt\", \"w\") as f:\n",
    "        f.write(header)\n",
    "        f.write(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"SSD_8000_640_mobilenet_v1_fpn_custom\" # Bugged\n",
    "\"SSD_8000_640_mobilenet_v1_fpn\"\n",
    "\"SSD_8000_640_mobilenet_v2_fpnlite_custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"SSD_8000_640_resnet101_v1_fpn\",\n",
    "    \"SSD_16000_640_mobilenet_v2_fpnlite\"\n",
    "]\n",
    "\n",
    "config_template = './myModules/configs/{name}_config.config'\n",
    "model_dir_template = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'\n",
    "log_dir = './myModules/log'\n",
    "\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def evaluate_model(model_name):\n",
    "    config_path = config_template.format(name=model_name)\n",
    "    model_dir = model_dir_template.format(name=model_name)\n",
    "    \n",
    "    command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\model_main_tf2.py \\\n",
    "                --pipeline_config_path={config_path} \\\n",
    "                --model_dir={model_dir} \\\n",
    "                --checkpoint_dir={model_dir}\"\n",
    "    \n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        error_message = error.decode(\"utf-8\")\n",
    "        print(f\"Error evaluating model {model_name}: {error_message}\")\n",
    "        return error_message\n",
    "    else:\n",
    "        output_str = output.decode(\"utf-8\")\n",
    "        header = f\"Evaluation Results for: {model_name}\\n\\n\"\n",
    "        result_file = os.path.join(log_dir, f\"{model_name}_evaluation_results.txt\")\n",
    "        with open(result_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            f.write(output_str)\n",
    "        print(f\"Evaluation results for {model_name} saved to {result_file}\")\n",
    "        return output_str\n",
    "\n",
    "# Hauptschleife zur Evaluierung der Modelle\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    evaluate_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade die Evaluierungsdaten\n",
    "eval_dataset = tf.data.TFRecordDataset(test_record_path)\n",
    "\n",
    "# ZÃ¤hle die Anzahl der Elemente (Frames) im Dataset\n",
    "num_frames = sum(1 for _ in eval_dataset)\n",
    "print(f\"Anzahl der Frames: {num_frames}\")\n",
    "\n",
    "total_time = 17.09 + 10.86  # Gesamtzeit in Sekunden\n",
    "\n",
    "# Durchschnittliche Zeit pro Frame\n",
    "time_per_frame = total_time / num_frames\n",
    "\n",
    "# FPS\n",
    "fps = 1 / time_per_frame\n",
    "print(f\"FPS: {fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_checkpoints(config_path, model_dir, checkpoints, model_name):\n",
    "    \"\"\"Evaluates all checkpoints and logs the results.\"\"\"\n",
    "    for checkpoint in checkpoints:\n",
    "        eval_command = f\"python C:/Users/Alexej/Desktop/GTSRB/models/research/object_detection/model_main_tf2.py \\\n",
    "            --pipeline_config_path={config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={checkpoint} \\\n",
    "            --run_once\"\n",
    "\n",
    "        # Starte den Evaluierungsprozess\n",
    "        process = subprocess.Popen(eval_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "        output, error = process.communicate()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f\"Error evaluating checkpoint {checkpoint}:\")\n",
    "            print(error.decode(\"utf-8\"))\n",
    "        else:\n",
    "            output_str = output.decode(\"utf-8\")\n",
    "            checkpoint_name = os.path.basename(checkpoint)\n",
    "            header = f\"Evaluation Results for MODEL_NAME: {model_name} at Checkpoint: {checkpoint_name}\\n\\n\"\n",
    "            with open(f\"./myModules/log/{model_name}_evaluation_results_{checkpoint_name}.txt\", \"w\") as f:\n",
    "                f.write(header)\n",
    "                f.write(output_str)\n",
    "            print(f\"Evaluation for checkpoint {checkpoint} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = get_checkpoints(model_dir)\n",
    "\n",
    "evaluate_checkpoints(config_path, model_dir, checkpoints, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_multiple_images(model, image, min_score_threshold):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Filter out detections with a score below the threshold\n",
    "    scores = output_dict['detection_scores']\n",
    "    high_score_indices = scores >= min_score_threshold\n",
    "\n",
    "    output_dict['detection_boxes'] = output_dict['detection_boxes'][high_score_indices]\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'][high_score_indices]\n",
    "    output_dict['detection_scores'] = output_dict['detection_scores'][high_score_indices]\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path, width, height):\n",
    "    image = Image.open(path)\n",
    "    # Resize the image to a larger size\n",
    "    image = image.resize((width, height))  # Resize to 256x256, you can change this as needed\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes_with_color(image, boxes, classes, scores, category_index, box_color, line_thickness):\n",
    "    for i in range(boxes.shape[0]):\n",
    "        if scores is None or scores[i] > 0.5:\n",
    "            class_id = int(classes[i])\n",
    "            if class_id in category_index.keys():\n",
    "                class_name = category_index[class_id]['name']\n",
    "                display_str = str(class_name)\n",
    "                color = box_color\n",
    "                vis_util.draw_bounding_box_on_image_array(\n",
    "                    image,\n",
    "                    boxes[i][0],\n",
    "                    boxes[i][1],\n",
    "                    boxes[i][2],\n",
    "                    boxes[i][3],\n",
    "                    color=color,\n",
    "                    thickness=line_thickness,\n",
    "                    display_str_list=[display_str],\n",
    "                    use_normalized_coordinates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_legend(image, groundtruth_classes, category_index, iou):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    image_height = image.size[1]\n",
    "    font_size = max(int(image_height * 0.04), 12)  # SchriftgrÃ¶ÃŸe auf 4% der BildhÃ¶he begrenzt, aber mindestens 12\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "\n",
    "    y_offset = int(0.9 * image_height)  # Startoffset fÃ¼r die Legende, z.B. 90% der BildhÃ¶he\n",
    "    x_offset = int(0.05 * image.size[0])  # Startoffset fÃ¼r die Legende, z.B. 5% der Bildbreite\n",
    "\n",
    "    # Ground Truth Legende\n",
    "    for gt_class in groundtruth_classes:\n",
    "        if gt_class in category_index:\n",
    "            class_name = category_index[gt_class]['name']\n",
    "            draw.text((x_offset, y_offset), f\"GT: {class_name}\\nIoU: {iou:.2f}\", fill=\"yellow\", font=font)\n",
    "            y_offset += int(font_size * 1.5)  # ErhÃ¶he den Offset basierend auf der aktuellen SchriftgrÃ¶ÃŸe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxA, boxB):\n",
    "    yA = max(boxA[0], boxB[0])\n",
    "    xA = max(boxA[1], boxB[1])\n",
    "    yB = min(boxA[2], boxB[2])\n",
    "    xB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    \n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    \n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    \n",
    "    image_pil = Image.fromarray(np.uint8(image_np)).convert(\"RGB\")\n",
    "    add_legend(image_pil, groundtruth_classes, category_index, iou)\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, ax):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    ax.imshow(image_np)\n",
    "    ax.axis('off')  # Hide the axis\n",
    "\n",
    "    # Adding legend to the plot\n",
    "    label_text = category_index[groundtruth_classes[0]]['name']\n",
    "    ax.set_title(f'{label_text}\\nIoU: {iou:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_images_for_labels(df, model, category_index, discard_predictions_below_acc, num_images_to_display):\n",
    "    sorted_labels = sorted(df['Label'].unique())\n",
    "\n",
    "    num_rows = len(sorted_labels)\n",
    "    num_cols = num_images_to_display\n",
    "\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols + num_cols * 0.5, num_rows * 2))\n",
    "    \n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, num_cols)\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for idx, label in enumerate(sorted_labels):\n",
    "        label_df = df[df['Label'] == label]\n",
    "        \n",
    "        random_indices = random.sample(range(len(label_df)), min(num_images_to_display, len(label_df)))\n",
    "\n",
    "        for jdx, r_idx in enumerate(random_indices):\n",
    "            row = label_df.iloc[r_idx]\n",
    "            image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "            groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "            groundtruth_classes = [row['Label']]\n",
    "            \n",
    "            output_dict = run_inference_for_multiple_images(model, image_np, discard_predictions_below_acc)\n",
    "            \n",
    "            plot_idx = idx * num_images_to_display + jdx\n",
    "            \n",
    "            visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, axes[plot_idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_predictions_with_groundtruth (model, df, num_images_to_display, category_index):\n",
    "    \n",
    "    random_indices = random.sample(range(len(df)), num_images_to_display)\n",
    "\n",
    "    # Visualisiere zufÃ¤llige ausgewÃ¤hlte Bilder\n",
    "    for idx in random_indices:\n",
    "        row = df.iloc[idx]\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_endless_predictions(model, df, category_index):\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_endless_predictions_from_path(model, path, category_index):\n",
    "    \n",
    "    for image_path in glob.glob(path):\n",
    "        image_np = load_image_into_numpy_array(image_path, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=1)\n",
    "        display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"SSD_4000_320_mobilenet_v1_fpn\",\n",
    "    \"SSD_8000_640_mobilenet_v2_fpnlite\",\n",
    "    \"SSD_8000_640_mobilenet_v2_fpnlite_custom\",\n",
    "    \"SSD_8000_640_mobilenet_v1_fpn_custom\",\n",
    "    \"SSD_8000_640_resnet101_v1_fpn\",\n",
    "    \"Efficientdet_8000_640_d1\"\n",
    "]\n",
    "\n",
    "config_template = './myModules/configs/{name}_config.config'\n",
    "model_dir_template = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'\n",
    "output_dir_template = './inference/ssd/{name}'\n",
    "\n",
    "\n",
    "def evaluate_model(model_name):\n",
    "    config_path = config_template.format(name=model_name)\n",
    "    model_dir = model_dir_template.format(name=model_name)\n",
    "    output_dir = output_dir_template.format(name=model_name)\n",
    "    \n",
    "    command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\exporter_main_v2.py \\\n",
    "                --pipeline_config_path={config_path} \\\n",
    "                --output_directory={output_dir} \\\n",
    "                --trained_checkpoint_dir={model_dir}\"\n",
    "    \n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "    print(f\"Done creating Inference: {MODEL_NAME}\")\n",
    "    if process.returncode != 0:\n",
    "        print(error.decode(\"utf-8\"))\n",
    "    else:\n",
    "        print(f\"No Error: {MODEL_NAME}\")\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Infering model: {model_name}\")\n",
    "    evaluate_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 00:02:17.175212: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 00:02:17.232333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "W0730 00:02:17.681291 14160 deprecation.py:623] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "I0730 00:02:23.944147 14160 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0730 00:02:32.996178 14160 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0730 00:02:38.950141 14160 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001EA075A08B0>, because it is not built.\n",
      "W0730 00:02:41.434893 14160 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001EA075A08B0>, because it is not built.\n",
      "W0730 00:03:09.759076 14160 save.py:269] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 329). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: ./inference/ssd/SSD_640_101_fixed_withAugmentation_noUnknown2\\saved_model\\assets\n",
      "I0730 00:03:19.621471 14160 builder_impl.py:779] Assets written to: ./inference/ssd/SSD_640_101_fixed_withAugmentation_noUnknown2\\saved_model\\assets\n",
      "INFO:tensorflow:Writing pipeline config file to ./inference/ssd/SSD_640_101_fixed_withAugmentation_noUnknown2\\pipeline.config\n",
      "I0730 00:03:21.188974 14160 config_util.py:253] Writing pipeline config file to ./inference/ssd/SSD_640_101_fixed_withAugmentation_noUnknown2\\pipeline.config\n"
     ]
    }
   ],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\exporter_main_v2.py \\\n",
    "    --trained_checkpoint_dir {model_dir} \\\n",
    "    --output_directory {inference_path} \\\n",
    "    --pipeline_config_path  {config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_path = './inference/ssd/{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load(f'./{inference_path}/saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inference Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = './GTSRB/Final_Test/Images/*.ppm'\n",
    "min_score_threshold = 0.4\n",
    "number_of_images_to_display = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_endless_predictions_from_path(model, image_path, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_endless_predictions(model, df_final_test, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_predictions_with_groundtruth(model, df_final_test, number_of_images_to_display, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_images_for_labels(df_final_test, model, category_index, min_score_threshold, number_of_images_to_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation (discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_labels = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "for idx, row in df_test_raw.iterrows():\n",
    "    row = df_test_raw.iloc[idx]\n",
    "    image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "    groundtruth_classes = [row['Label']]\n",
    "    \n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "    \n",
    "    predicted_class = output_dict['detection_classes'][0]\n",
    "    all_predicted_labels.append(predicted_class)\n",
    "    all_true_labels.append(groundtruth_classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
    "recall = recall_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "precision = precision_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "f1 = f1_score(all_true_labels, all_predicted_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Vorhersage')\n",
    "plt.ylabel('Wahre Werte')\n",
    "plt.title('Konfusionsmatrix')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_true_labels, all_predicted_labels, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-Time detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoint_files = os.listdir(checkpoint_dir)\n",
    "    checkpoint_files = [f for f in checkpoint_files if f.startswith('ckpt-') and '.index' in f]\n",
    "    checkpoint_numbers = [int(re.findall(r'\\d+', f)[0]) for f in checkpoint_files]\n",
    "    if not checkpoint_numbers:\n",
    "        raise ValueError(\"No checkpoints found in the directory.\")\n",
    "    latest_checkpoint = max(checkpoint_numbers)\n",
    "    return os.path.join(checkpoint_dir, f'ckpt-{latest_checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH)).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load('./inference/{type}/{name}/saved_model'.format(name=MODEL_NAME, type=MODEL_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load('./inference/{type}/{name}/saved_model'.format(name=MODEL_NAME, type=MODEL_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Laden des Modells\n",
    "def load_model(model_path):\n",
    "    saved_model = tf.saved_model.load(model_path)\n",
    "    return saved_model\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "model_path = './inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model'\n",
    "detection_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Erstellen des Detektionsmodells und Wiederherstellen des Checkpoints\n",
    "def load_model():\n",
    "    configs = tf.compat.v2.saved_model.load('./inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model')\n",
    "    return configs['model']\n",
    "\n",
    "detection_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=10,\n",
    "                min_score_thresh=.9,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 800)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video capture testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum Eingabevideo und zum Ausgabevideo\n",
    "input_video_path = './videoInferenceData/shortTest.mp4'\n",
    "output_video_path = './videoInferenceData/tf/shortTest.mp4'\n",
    "\n",
    "# VideoCapture-Objekt erstellen\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# ÃœberprÃ¼fen, ob das Video geÃ¶ffnet werden kann\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Video-Eigenschaften abrufen\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# VideoWriter-Objekt erstellen\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Beenden der Schleife, wenn das Video zu Ende ist\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes sollten ints sein.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=10,\n",
    "                min_score_thresh=.9,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    # Frame mit Detektionen in das Ausgabevideo schreiben\n",
    "    out.write(image_np_with_detections)\n",
    "\n",
    "    # Optional: Zeige das Video mit Detektionen in einem Fenster an\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 800)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Ressourcen freigeben\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video capture testing end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(frame, axis=0), dtype=tf.float32)\n",
    "    \n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'],\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=100,  # Anzahl der maximal zu zeichnenden Boxen\n",
    "        min_score_thresh=0.5    # Minimale Vertrauensschwelle fÃ¼r die Anzeige\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    \n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Laden und Konfigurieren des Modells\n",
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Wiederherstellen des Checkpoints\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(TRAINED_CHECKPOINT_PATH, 'ckpt-9')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)\n",
    "\n",
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    \n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.9,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', cv2.resize(image_np, (800, 800)))\n",
    "    \n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Videoaufnahme von der Kamera starten (Kamera 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# ÃœberprÃ¼fen, ob die Kamera erfolgreich geÃ¶ffnet wurde\n",
    "if not cap.isOpened():\n",
    "    print(\"Fehler beim Ã–ffnen der Kamera\")\n",
    "    exit()\n",
    "\n",
    "# Unendlich Schleife, um Frames von der Kamera zu lesen und anzuzeigen\n",
    "while True:\n",
    "    # Lesen eines einzelnen Frames von der Kamera\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # ÃœberprÃ¼fen, ob das Frame erfolgreich gelesen wurde\n",
    "    if not ret:\n",
    "        print(\"Fehler beim Lesen des Frames\")\n",
    "        break\n",
    "    \n",
    "    # Anzeigen des Frames in einem Fenster\n",
    "    cv2.imshow('Videoaufnahme', frame)\n",
    "    \n",
    "    # Beenden der Schleife bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Freigeben der Videoquelle und SchlieÃŸen aller Fenster\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster_RCNN_640_50_fixed: good 8/10\n",
    "<br>Faster_RCNN_640_50_final: good 7/10\n",
    "<br>Faster_RCNN_640_50_fixed_distanz: good. Close 9/10, medium 3/10\n",
    "<br>Faster_RCNN_short_640_101_unknown: Erkennt 2 immer. Close 5/10, medium 6/10\n",
    "<br>Faster_RCNN_640_50_fixed_withAugmentation_noUnknown: Stopp (4) 10/10. Close 9/10, medium 6/10\n",
    "<br>Faster_RCNN_50_640_Step_4: idle State is awfull. Close 10/10, Medium depends on class.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_MODEL = \"Faster_RCNN_50_640_Step_4\"\n",
    "TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=INFERENCE_MODEL)\n",
    "TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\{type}\\\\{name}'.format(name=INFERENCE_MODEL, type=\"faster_rcnn\")\n",
    "#TRAINED_CHECKPOINT_PATH = \"./inference/faster_rcnn/Faster_RCNN_640_50_fixed_withAugmentation_noUnknown/checkpoint/\"\n",
    "TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short_woUnk.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Wiederherstellen des Checkpoints\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH)).expect_partial()\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Wiederherstellen des Checkpoints\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH)).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "#category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "#tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load('./inference/faster_rcnn/{name}/saved_model/'.format(name=INFERENCE_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    #input_tensor = input_tensor[tf.newaxis,...]\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    #output_dict['detection_boxes'] = output_dict['detection_boxes'] / [image.shape[0], image.shape[1], image.shape[0], image.shape[1]]\n",
    "    return output_dict\n",
    "\n",
    "def show_inference(frame):\n",
    "    \n",
    "    image_np = np.array(frame)\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "    #output_dict = apply_nms(output_dict)  # NMS anwenden\n",
    "    height, width, _ = image_np.shape\n",
    "\n",
    "    # Konvertiere normalisierte Koordinaten in Pixelkoordinaten\n",
    "    for i in range(output_dict['detection_boxes'].shape[0]):\n",
    "        ymin, xmin, ymax, xmax = output_dict['detection_boxes'][i]\n",
    "        output_dict['detection_boxes'][i] = [ymin * height, xmin * width, ymax * height, xmax * width]\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes']+1,\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=10,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    return image_np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "           \n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    cv2.imshow('object detection', cv2.resize(frame_with_detections, (800,600)))\n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    #input_tensor = input_tensor[tf.newaxis,...]\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference(frame):\n",
    "    \n",
    "    image_np = np.array(frame)\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "    output_dict = apply_nms(output_dict)  # NMS anwenden\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes']+1,\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=10,\n",
    "        min_score_thresh=0.5,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nms(output_dict, iou_threshold=0.4):\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    scores = output_dict['detection_scores']\n",
    "    classes = output_dict['detection_classes']\n",
    "    \n",
    "    selected_indices = tf.image.non_max_suppression(\n",
    "        boxes=boxes,\n",
    "        scores=scores,\n",
    "        max_output_size=output_dict['num_detections'],\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=0.4\n",
    "    )\n",
    "    \n",
    "    selected_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "    selected_scores = tf.gather(scores, selected_indices).numpy()\n",
    "    selected_classes = tf.gather(classes, selected_indices).numpy()\n",
    "    \n",
    "    output_dict['detection_boxes'] = selected_boxes\n",
    "    output_dict['detection_scores'] = selected_scores\n",
    "    output_dict['detection_classes'] = selected_classes\n",
    "    output_dict['num_detections'] = len(selected_boxes)\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum Eingabevideo und zum Ausgabevideo\n",
    "input_video_path = './videoInferenceData/15FPS_singleRow/15FPS_singleRow_test1.mp4'\n",
    "output_video_path = './videoInferenceData/tf/shortTest.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    out.write(image_np_with_detections)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(frame_with_detections, (640, 640)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion die den Inputstream HochauflÃ¶send macht\n",
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anzeige der Inferenz\n",
    "def show_inference(frame):\n",
    "    # Skaliere das Bild hoch\n",
    "    scale_percent = 50  # Prozentuale Skalierung\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    resized_frame = cv2.resize(frame, dim, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    image_np = np.array(resized_frame)\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        resized_frame,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes']+1,\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=20,  \n",
    "        min_score_thresh=0.6,  \n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "\n",
    "    # Skaliere das Bild zurÃ¼ck\n",
    "    original_dim = (frame.shape[1], frame.shape[0])\n",
    "    frame_with_detections = cv2.resize(resized_frame, original_dim, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    return frame_with_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur AusfÃ¼hrung der Inferenz\n",
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anzeige der Inferenz\n",
    "def show_inference(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    window_size = 300  # GrÃ¶ÃŸe des Sliding Windows\n",
    "    step_size = 100    # Schrittweite fÃ¼r das Sliding Window\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    scores = []\n",
    "\n",
    "    for y in range(0, height, step_size):\n",
    "        for x in range(0, width, step_size):\n",
    "            window = frame[y:y+window_size, x:x+window_size]\n",
    "            if window.shape[0] != window_size or window.shape[1] != window_size:\n",
    "                continue\n",
    "            \n",
    "            output_dict = run_inference_for_single_image(window)\n",
    "            \n",
    "            for i in range(output_dict['num_detections']):\n",
    "                box = output_dict['detection_boxes'][i]\n",
    "                class_id = output_dict['detection_classes'][i]\n",
    "                score = output_dict['detection_scores'][i]\n",
    "                \n",
    "                if score < 0.4:\n",
    "                    continue\n",
    "                \n",
    "                ymin, xmin, ymax, xmax = box\n",
    "                (left, right, top, bottom) = (int(xmin * window_size) + x, int(xmax * window_size) + x, int(ymin * window_size) + y, int(ymax * window_size) + y)\n",
    "                boxes.append([top, left, bottom, right])\n",
    "                classes.append(class_id + 1)\n",
    "                scores.append(score)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        np.array(boxes),\n",
    "        np.array(classes),\n",
    "        np.array(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=10,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur AusfÃ¼hrung der Inferenz\n",
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anwendung von Non-Maximum Suppression\n",
    "def apply_nms(boxes, scores, classes, max_output_size=5, iou_threshold=0.8, score_threshold=0.6):\n",
    "    indices = tf.image.non_max_suppression(\n",
    "        boxes=boxes,\n",
    "        scores=scores,\n",
    "        max_output_size=max_output_size,\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=score_threshold\n",
    "    )\n",
    "    selected_boxes = tf.gather(boxes, indices).numpy()\n",
    "    selected_scores = tf.gather(scores, indices).numpy()\n",
    "    selected_classes = tf.gather(classes, indices).numpy()\n",
    "    return selected_boxes, selected_scores, selected_classes\n",
    "\n",
    "# Funktion zur Anzeige der Inferenz\n",
    "def show_inference(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    window_size = 300  # GrÃ¶ÃŸe des Sliding Windows\n",
    "    step_size = 150    # Schrittweite fÃ¼r das Sliding Window\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    scores = []\n",
    "\n",
    "    for y in range(0, height, step_size):\n",
    "        for x in range(0, width, step_size):\n",
    "            window = frame[y:y+window_size, x:x+window_size]\n",
    "            if window.shape[0] != window_size or window.shape[1] != window_size:\n",
    "                continue\n",
    "            \n",
    "            output_dict = run_inference_for_single_image(window)\n",
    "            \n",
    "            for i in range(output_dict['num_detections']):\n",
    "                box = output_dict['detection_boxes'][i]\n",
    "                class_id = output_dict['detection_classes'][i]\n",
    "                score = output_dict['detection_scores'][i]\n",
    "                \n",
    "                if score < 0.4:\n",
    "                    continue\n",
    "                \n",
    "                ymin, xmin, ymax, xmax = box\n",
    "                (left, right, top, bottom) = (\n",
    "                    int(xmin * window_size) + x, \n",
    "                    int(xmax * window_size) + x, \n",
    "                    int(ymin * window_size) + y, \n",
    "                    int(ymax * window_size) + y\n",
    "                )\n",
    "                boxes.append([top, left, bottom, right])\n",
    "                classes.append(class_id + 1)\n",
    "                scores.append(score)\n",
    "    \n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "    classes = np.array(classes)\n",
    "\n",
    "    selected_boxes, selected_scores, selected_classes = apply_nms(boxes, scores, classes)\n",
    "\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        selected_boxes,\n",
    "        selected_classes,\n",
    "        selected_scores,\n",
    "        category_index,\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=5,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    \n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur AusfÃ¼hrung der Inferenz\n",
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anwendung von Non-Maximum Suppression\n",
    "def apply_nms(output_dict, iou_threshold=0.6):\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    scores = output_dict['detection_scores']\n",
    "    classes = output_dict['detection_classes']+1\n",
    "    \n",
    "    selected_indices = tf.image.non_max_suppression(\n",
    "        boxes=boxes,\n",
    "        scores=scores,\n",
    "        max_output_size=output_dict['num_detections'],\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=0.6\n",
    "    )\n",
    "    \n",
    "    selected_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "    selected_scores = tf.gather(scores, selected_indices).numpy()\n",
    "    selected_classes = tf.gather(classes, selected_indices).numpy()\n",
    "    \n",
    "    output_dict['detection_boxes'] = selected_boxes\n",
    "    output_dict['detection_scores'] = selected_scores\n",
    "    output_dict['detection_classes'] = selected_classes\n",
    "    output_dict['num_detections'] = len(selected_boxes)\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anzeige der Inferenz\n",
    "def show_inference(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    window_size = 300  # GrÃ¶ÃŸe des Sliding Windows\n",
    "    step_size = 150    # Schrittweite fÃ¼r das Sliding Window\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    scores = []\n",
    "\n",
    "    for y in range(0, height, step_size):\n",
    "        for x in range(0, width, step_size):\n",
    "            window = frame[y:y+window_size, x:x+window_size]\n",
    "            if window.shape[0] != window_size or window.shape[1] != window_size:\n",
    "                continue\n",
    "            \n",
    "            output_dict = run_inference_for_single_image(window)\n",
    "            output_dict = apply_nms(output_dict)  # NMS anwenden\n",
    "            \n",
    "            for i in range(output_dict['num_detections']):\n",
    "                box = output_dict['detection_boxes'][i]\n",
    "                class_id = output_dict['detection_classes'][i]\n",
    "                score = output_dict['detection_scores'][i]\n",
    "                \n",
    "                ymin, xmin, ymax, xmax = box\n",
    "                (left, right, top, bottom) = (\n",
    "                    int(xmin * window_size) + x, \n",
    "                    int(xmax * window_size) + x, \n",
    "                    int(ymin * window_size) + y, \n",
    "                    int(ymax * window_size) + y\n",
    "                )\n",
    "                boxes.append([top, left, bottom, right])\n",
    "                classes.append(class_id)\n",
    "                scores.append(score)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        np.array(boxes),\n",
    "        np.array(classes),\n",
    "        np.array(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=5,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anwendung von Non-Maximum Suppression\n",
    "def apply_nms(output_dict, iou_threshold=0.6):\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    scores = output_dict['detection_scores']\n",
    "    classes = output_dict['detection_classes']+1\n",
    "    \n",
    "    selected_indices = tf.image.non_max_suppression(\n",
    "        boxes=boxes,\n",
    "        scores=scores,\n",
    "        max_output_size=output_dict['num_detections'],\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=0.6\n",
    "    )\n",
    "    \n",
    "    selected_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "    selected_scores = tf.gather(scores, selected_indices).numpy()\n",
    "    selected_classes = tf.gather(classes, selected_indices).numpy()\n",
    "    \n",
    "    output_dict['detection_boxes'] = selected_boxes\n",
    "    output_dict['detection_scores'] = selected_scores\n",
    "    output_dict['detection_classes'] = selected_classes\n",
    "    output_dict['num_detections'] = len(selected_boxes)\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Funktion zur Anzeige der Inferenz\n",
    "def show_inference(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    window_size = 300  # GrÃ¶ÃŸe des Sliding Windows\n",
    "    step_size = 200    # Schrittweite fÃ¼r das Sliding Window\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    scores = []\n",
    "\n",
    "    for y in range(0, height, step_size):\n",
    "        for x in range(0, width, step_size):\n",
    "            window = frame[y:y+window_size, x:x+window_size]\n",
    "            if window.shape[0] != window_size or window.shape[1] != window_size:\n",
    "                continue\n",
    "            \n",
    "            output_dict = run_inference_for_single_image(window)\n",
    "            output_dict = apply_nms(output_dict)  # NMS anwenden\n",
    "            \n",
    "            for i in range(output_dict['num_detections']):\n",
    "                box = output_dict['detection_boxes'][i]\n",
    "                class_id = output_dict['detection_classes'][i]\n",
    "                score = output_dict['detection_scores'][i]\n",
    "                \n",
    "                ymin, xmin, ymax, xmax = box\n",
    "                (left, right, top, bottom) = (\n",
    "                    int(xmin * window_size) + x, \n",
    "                    int(xmax * window_size) + x, \n",
    "                    int(ymin * window_size) + y, \n",
    "                    int(ymax * window_size) + y\n",
    "                )\n",
    "                boxes.append([top, left, bottom, right])\n",
    "                classes.append(class_id)\n",
    "                scores.append(score)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        np.array(boxes),\n",
    "        np.array(classes),\n",
    "        np.array(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=5,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Read frame from camera\n",
    "    ret, image_np = cap.read()\n",
    "\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "    # Things to try:\n",
    "    # Flip horizontally\n",
    "    # image_np = np.fliplr(image_np).copy()\n",
    "\n",
    "    # Convert image to grayscale\n",
    "    # image_np = np.tile(\n",
    "    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_detections,\n",
    "          detections['detection_boxes'][0].numpy(),\n",
    "          (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "          detections['detection_scores'][0].numpy(),\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=10,\n",
    "          min_score_thresh=0.60,\n",
    "          agnostic_mode=False)\n",
    "\n",
    "    # Display output\n",
    "    cv2.imshow('object detection', cv2.resize(image_np_with_detections, (800, 600)))\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "           \n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    cv2.imshow('object detection', frame_with_detections)\n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "  image = np.asarray(image)\n",
    "  input_tensor = tf.convert_to_tensor(image)\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    " \n",
    "\n",
    "  model_fn = model.signatures['serving_default']\n",
    "  output_dict = model_fn(input_tensor)\n",
    "  \n",
    "  num_detections = int(output_dict.pop('num_detections'))\n",
    "  output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "  output_dict['num_detections'] = num_detections\n",
    " \n",
    "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "  # Handle models with masks:\n",
    "  if 'detection_masks' in output_dict:\n",
    "    # Reframe the the bbox mask to the image size.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "               image.shape[0], image.shape[1])      \n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                       tf.uint8)\n",
    "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "     \n",
    "  return output_dict\n",
    "def show_inference(model, frame):\n",
    "  image_np = np.array(frame)\n",
    "     \n",
    "  output_dict = run_inference_for_single_image(model, image_np)\n",
    "\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=10,\n",
    "      min_score_thresh=0.6,\n",
    "      agnostic_mode=False,\n",
    "      line_thickness=5)\n",
    " \n",
    "  return(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    re,frame = video_capture.read()\n",
    "    Imagenp=show_inference(model, frame)\n",
    "    cv2.imshow('object detection', cv2.resize(Imagenp, (800,600)))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum Eingabevideo und zum Ausgabevideo\n",
    "input_video_path = './videoInferenceData/multiRow/allnUnkClassBrightToDark.mp4'\n",
    "output_video_path = './videoInferenceData/tf/shortTest.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    out.write(image_np_with_detections)\n",
    "\n",
    "    cv2.imshow('object detection',  frame_with_detections)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "fps = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_with_detections = show_inference(frame)\n",
    "\n",
    "    # Berechne die FPS\n",
    "    frame_count += 1\n",
    "    if (time.time() - start_time) > 1:  \n",
    "        fps = frame_count / (time.time() - start_time)\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    cv2.putText(frame_with_detections, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Object Detection', frame_with_detections)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kamera-Kalibrierungsparameter (mÃ¼ssen angepasst werden)\n",
    "f = 3000  # Brennweite in Pixeln\n",
    "objekt_breite = 0.5  # tatsÃ¤chliche Breite des Objekts in Metern\n",
    "\n",
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    image_np = np.array(frame)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "    # DurchfÃ¼hrung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Entfernung berechnen und Ergebnisse anpassen\n",
    "    for i in range(num_detections):\n",
    "        box = detections['detection_boxes'][i]\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        box_width = xmax - xmin\n",
    "        box_height = ymax - ymin\n",
    "\n",
    "        # Berechne die GrÃ¶ÃŸe des Objekts in Pixeln\n",
    "        box_width_pixels = box_width * frame.shape[1]\n",
    "        box_height_pixels = box_height * frame.shape[0]\n",
    "\n",
    "        # Berechne die Entfernung zum Objekt (angenommene GrÃ¶ÃŸe des Objekts)\n",
    "        entfernung = (objekt_breite * f) / box_width_pixels\n",
    "\n",
    "        # Hier kannst du die Erkennungsergebnisse basierend auf der Entfernung anpassen\n",
    "        # Beispiel: Du kannst die Boxen oder Scores basierend auf der Entfernung anpassen\n",
    "\n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'] + 1,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.3,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videoaufnahme starten\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Definiere Skalierungsfaktoren\n",
    "scales = [0.25, 1.0, 1.5]\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # ÃœberprÃ¼fen der Form des Bildes\n",
    "    if len(frame.shape) == 2:  # Falls das Bild ein Graustufenbild ist\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Initialisiere eine Liste, um alle Erkennungen zu speichern\n",
    "    all_detections = []\n",
    "\n",
    "    for scale in scales:\n",
    "        # Skaliere das Bild\n",
    "        height, width, _ = frame.shape\n",
    "        new_size = (int(width * scale), int(height * scale))\n",
    "        resized_frame = cv2.resize(frame, new_size)\n",
    "\n",
    "        # Konvertieren des Frames zu einem Tensor\n",
    "        image_np = np.array(resized_frame)\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "        # DurchfÃ¼hrung der Objekterkennung\n",
    "        detections = detect_fn(input_tensor)\n",
    "\n",
    "        # Verarbeitung der erkannten Objekte\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "        detections['num_detections'] = num_detections\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        # Berechne die Skalierungskorrektur fÃ¼r die Box-Koordinaten\n",
    "        height_scale = height / new_size[1]\n",
    "        width_scale = width / new_size[0]\n",
    "        detections['detection_boxes'][:, [0, 2]] *= height_scale\n",
    "        detections['detection_boxes'][:, [1, 3]] *= width_scale\n",
    "\n",
    "        # Speichere die Ergebnisse\n",
    "        all_detections.append(detections)\n",
    "\n",
    "    # Kombiniere alle Erkennungen\n",
    "    # Du kannst hier die Detections nach deinem Bedarf kombinieren oder zusammenfÃ¼hren\n",
    "\n",
    "    # Verwende die Detections von der letzten Skala fÃ¼r die Visualisierung (oder alle)\n",
    "    final_detections = all_detections[-1]  # Hier nehmen wir die Erkennung von der letzten Skala\n",
    "\n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        final_detections['detection_boxes'],\n",
    "        final_detections['detection_classes'] + 1,\n",
    "        final_detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.3,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Stoppen der Erfassung bei DrÃ¼cken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TFLite for Jetson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\export_tflite_graph_tf2.py \\\n",
    "    --pipeline_config_path={config_path} \\\n",
    "    --trained_checkpoint_dir={TRAINED_CHECKPOINT_PATH} \\\n",
    "    --output_directory={model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZEN_TFLITE_PATH = os.path.join(model_dir, 'saved_model')\n",
    "TFLITE_MODEL = os.path.join(model_dir, 'saved_model', 'detect.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saved_model_dir, TFLITE_MODEL)\n",
    "D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}\\\\saved_model\\\\'.format(name=MODEL_NAME)\n",
    "tflite_model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}\\\\saved_model\\\\detect.tflite'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"tflite_convert \\\n",
    "--saved_model_dir={} \\\n",
    "--output_file={} \\\n",
    "--input_shapes=1,300,300,3 \\\n",
    "--input_arrays=normalized_input_image_tensor \\\n",
    "--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
    "--inference_type=FLOAT \\\n",
    "--allow_custom_ops\".format(FROZEN_TFLITE_PATH, TFLITE_MODEL, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_convert --saved_model_dir=Tensorflow\\workspace\\models\\my_ssd_mobnet\\tfliteexport\\saved_model --output_file=Tensorflow\\workspace\\models\\my_ssd_mobnet\\tfliteexport\\saved_model\\detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --allow_custom_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tflite_convert --saved_model_dir=saved_model_dir --output_file=tflite_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'tf_lite'\n",
    "\n",
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\export_tflite_graph_tf2.py \\\n",
    "    --trained_checkpoint_dir={TRAINED_CHECKPOINT_PATH} \\\n",
    "    --output_directory={output_directory} \\\n",
    "    --pipeline_config_path={config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tflite_convert --saved_model_dir=D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\ --output_file=D:\\Desktop-Short\\trained_models\\ssd\\ssd_test\\saved_model\\model.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tflite_model_dir, saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tflite_convert --saved_model_dir=saved_model_dir --output_file=tflite_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster Rcnn results: \n",
    "<br>Faster_RCNN_640_50_fixed: good 8/10\n",
    "<br>Faster_RCNN_640_50_final: good 7/10\n",
    "<br>Faster_RCNN_640_50_fixed_distanz: good. Close 9/10, medium 3/10\n",
    "<br>Faster_RCNN_short_640_101_unknown: Erkennt 2 immer. Close 5/10, medium 6/10\n",
    "<br>Faster_RCNN_640_50_fixed_withAugmentation_noUnknown: Stopp (4) 10/10. Close 9/10, medium 6/10 -> Best model out of all so far\n",
    "<br>Faster_RCNN_50_320_Step_2: idle State is awfull.\n",
    "<br>Faster_RCNN_50_480_Step_3: Close 0/10, Medium 8/10 -> Idle state is awful\n",
    "<br>Faster_RCNN_50_640_Step_4: idle State is awfull. Close 10/10, Medium depends on class.\n",
    "<br>Faster_RCNN_50_1024_Step_5: 0/10\n",
    "<br>Faster_RCNN_101_320_Step_1: 0/10\n",
    "<br>Faster_RCNN_101_480_Step_2: 0/10\n",
    "### SSD results: Idle state is awful in most cases\n",
    "<br>Efficientdet_8000_640_d1: Close 8/10. Medium 0/10 -> No tracking in medium\n",
    "<br>SSD_4000_320_mobilenet_v1_fpn: 0/10 -> Does not work\n",
    "<br>SSD_4000_640_mobilenet_v1_fpn: 4/10  \n",
    "<br>SSD_8000_640_mobilenet_v1_fpn_custom: 0\n",
    "<br>SSD_8000_640_mobilenet_v2_fpnlite: 0\n",
    "<br>SSD_8000_640_mobilenet_v2_fpnlite_custom: 0\n",
    "<br>SSD_8000_640_resnet101_v1_fpn: 5/10 Close. Medium 0/10 -> Limited tracking in Close\n",
    "<br>ssd_test: 0/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoint_files = os.listdir(checkpoint_dir)\n",
    "    checkpoint_files = [f for f in checkpoint_files if f.startswith('ckpt-') and '.index' in f]\n",
    "    checkpoint_numbers = [int(re.findall(r'\\d+', f)[0]) for f in checkpoint_files]\n",
    "    if not checkpoint_numbers:\n",
    "        raise ValueError(\"No checkpoints found in the directory.\")\n",
    "    latest_checkpoint = max(checkpoint_numbers)\n",
    "    return os.path.join(checkpoint_dir, f'ckpt-{latest_checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_MODEL = \"Faster_RCNN_640_50_fixed_withAugmentation_noUnknown\"\n",
    "TRAINED_CONFIG_PATH = \"./myModules/configs/{name}_config.config\".format(name=INFERENCE_MODEL)\n",
    "TRAINED_CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\{type}\\\\{name}'.format(name=INFERENCE_MODEL, type=\"faster_rcnn\")\n",
    "#TRAINED_CHECKPOINT_PATH = \"./inference/faster_rcnn/Faster_RCNN_640_50_fixed_withAugmentation_noUnknown/checkpoint/\"\n",
    "TRAINED_LABLE_MAP_PATH = \"./myModules/label_map_short_woUnk.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Wiederherstellen des Checkpoints\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH)).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "#category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH, use_display_name=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    #input_tensor = input_tensor[tf.newaxis,...]\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    if 'detection_masks' in output_dict:\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "def show_inference(frame):\n",
    "    \n",
    "    image_np = np.array(frame)\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "    output_dict = apply_nms(output_dict)  # NMS anwenden\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes']+1,\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=20,\n",
    "        min_score_thresh=0.5,\n",
    "        agnostic_mode=False,\n",
    "        line_thickness=5)\n",
    "    return image_np\n",
    "\n",
    "def apply_nms(output_dict, iou_threshold=0.4):\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    scores = output_dict['detection_scores']\n",
    "    classes = output_dict['detection_classes']\n",
    "    \n",
    "    selected_indices = tf.image.non_max_suppression(\n",
    "        boxes=boxes,\n",
    "        scores=scores,\n",
    "        max_output_size=output_dict['num_detections'],\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=0.4\n",
    "    )\n",
    "    \n",
    "    selected_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "    selected_scores = tf.gather(scores, selected_indices).numpy()\n",
    "    selected_classes = tf.gather(classes, selected_indices).numpy()\n",
    "    \n",
    "    output_dict['detection_boxes'] = selected_boxes\n",
    "    output_dict['detection_scores'] = selected_scores\n",
    "    output_dict['detection_classes'] = selected_classes\n",
    "    output_dict['num_detections'] = len(selected_boxes)\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "           \n",
    "    if len(frame.shape) == 2:  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    frame_with_detections = show_inference(frame)\n",
    "    \n",
    "    cv2.imshow('object detection', frame_with_detections)\n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracker testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanBoxTracker:\n",
    "    count = 0\n",
    "\n",
    "    def __init__(self, bbox):\n",
    "        self.kalman = cv2.KalmanFilter(4, 2)\n",
    "        self.kalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\n",
    "        self.kalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\n",
    "        self.kalman.processNoiseCov = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32) * 0.03\n",
    "        self.kalman.measurementNoiseCov = np.array([[1, 0], [0, 1]], np.float32) * 0.5\n",
    "\n",
    "        self.kalman.statePre[:2, 0] = bbox[:2]\n",
    "        self.kalman.statePre[2:, 0] = 0\n",
    "\n",
    "        self.id = KalmanBoxTracker.count\n",
    "        KalmanBoxTracker.count += 1\n",
    "\n",
    "    def predict(self):\n",
    "        return self.kalman.predict()\n",
    "\n",
    "    def update(self, bbox):\n",
    "        self.kalman.correct(bbox[:2])\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(TRAINED_CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(get_latest_checkpoint(TRAINED_CHECKPOINT_PATH)).expect_partial()\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(TRAINED_LABLE_MAP_PATH)\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "def run_inference_for_single_image(image):\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image, 0), dtype=tf.float32)\n",
    "    output_dict = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "def show_inference(frame, trackers, detection_interval, frame_count):\n",
    "    image_np = np.array(frame)\n",
    "    output_dict = {}\n",
    "\n",
    "    detection_boxes = np.array([])\n",
    "    detection_scores = np.array([])\n",
    "    detection_classes = np.array([])\n",
    "\n",
    "    if frame_count % detection_interval == 0:\n",
    "        output_dict = run_inference_for_single_image(image_np)\n",
    "        detection_boxes = output_dict.get('detection_boxes', np.array([]))\n",
    "        detection_scores = output_dict.get('detection_scores', np.array([]))\n",
    "        detection_classes = output_dict.get('detection_classes', np.array([]))\n",
    "\n",
    "        # Debugging-Ausgaben\n",
    "        print(f\"Raw Detection Boxes: {detection_boxes}\")\n",
    "        print(f\"Raw Detection Scores: {detection_scores}\")\n",
    "        print(f\"Raw Detection Classes: {detection_classes}\")\n",
    "\n",
    "        # Filter nur die Boxen mit einer Score grÃ¶ÃŸer als 0.5\n",
    "        valid_indices = np.where(detection_scores > 0.5)[0]\n",
    "        detection_boxes = detection_boxes[valid_indices]\n",
    "        detection_scores = detection_scores[valid_indices]\n",
    "        detection_classes = detection_classes[valid_indices]\n",
    "\n",
    "        # Debugging-Ausgaben nach dem Filtern\n",
    "        print(f\"Filtered Detection Boxes: {detection_boxes}\")\n",
    "        print(f\"Filtered Detection Scores: {detection_scores}\")\n",
    "        print(f\"Filtered Detection Classes: {detection_classes}\")\n",
    "\n",
    "        trackers.clear()\n",
    "        for box in detection_boxes:\n",
    "            ymin, xmin, ymax, xmax = box\n",
    "            bbox = np.array([xmin * frame.shape[1], ymin * frame.shape[0]], dtype=np.float32)\n",
    "            tracker = KalmanBoxTracker(bbox)\n",
    "            trackers.append(tracker)\n",
    "\n",
    "    updated_boxes = []\n",
    "    for tracker in trackers:\n",
    "        prediction = tracker.predict()\n",
    "        x, y, dx, dy = prediction.flatten()\n",
    "        xmin = max(0, x - 0.5 * dx)\n",
    "        ymin = max(0, y - 0.5 * dy)\n",
    "        xmax = min(frame.shape[1], x + 0.5 * dx)\n",
    "        ymax = min(frame.shape[0], y + 0.5 * dy)\n",
    "        updated_boxes.append([ymin, xmin, ymax, xmax])\n",
    "\n",
    "    updated_boxes = np.array(updated_boxes)\n",
    "    if updated_boxes.size == 0:\n",
    "        updated_boxes = np.array([[0, 0, 0, 0]])\n",
    "\n",
    "    # Sicherstellen, dass die Arrays immer Werte enthalten\n",
    "    if detection_classes.size == 0:\n",
    "        detection_classes = np.array([0])  # Default Class ID\n",
    "    if detection_scores.size == 0:\n",
    "        detection_scores = np.array([0])  # Default Score\n",
    "\n",
    "    # Debugging-Ausgaben fÃ¼r Ã¼berprÃ¼fte Arrays\n",
    "    print(f\"Updated Boxes: {updated_boxes}\")\n",
    "    print(f\"Detection Classes (post-filtering): {detection_classes}\")\n",
    "    print(f\"Detection Scores (post-filtering): {detection_scores}\")\n",
    "\n",
    "    # Sicherstellen, dass Boxen und Scores gÃ¼ltig sind, bevor sie visualisiert werden\n",
    "    if detection_boxes.size > 0 and detection_scores.size > 0 and detection_classes.size > 0:\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            updated_boxes,\n",
    "            detection_classes + 1,  # Ensure class IDs are correct\n",
    "            detection_scores,\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=20,\n",
    "            min_score_thresh=0.5,\n",
    "            agnostic_mode=False,\n",
    "            line_thickness=5\n",
    "        )\n",
    "    else:\n",
    "        print(\"Keine gÃ¼ltigen Boxen, Klassen oder Scores zum Visualisieren.\")\n",
    "\n",
    "    return image_np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "trackers = []\n",
    "detection_interval = 10\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    frame_with_detections = show_inference(frame, trackers, detection_interval, frame_count)\n",
    "    \n",
    "    cv2.imshow('object detection', frame_with_detections)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
