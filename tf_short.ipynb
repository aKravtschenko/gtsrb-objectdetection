{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2 \n",
    "import subprocess\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from six import BytesIO\n",
    "\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = './myModules/faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.config'\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "print(\"Model was successfully built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "#Checking for gpu\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "   raise SystemError('GPU device not found')\n",
    "\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(21)\n",
    "tf.compat.v1.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTrain(rootpath, cache_path):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "    Arguments:\n",
    "        rootpath: path to the traffic sign data, for example './GTSRB/Training'\n",
    "        cache_path: path to the cached DataFrame file, default is 'traffic_signs_data.pkl'\n",
    "    Returns:\n",
    "        DataFrame containing labels, image shapes, ROIs, and image paths\n",
    "    '''\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading: {cache_path}\")\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    data = []\n",
    "    # loop over all 43 classes\n",
    "    for c in range(0, 43):\n",
    "        prefix = f\"{rootpath}/{c:05d}/\"  # subdirectory for class\n",
    "        gtFile = open(prefix + f'GT-{c:05d}.csv')  # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';')  # csv parser for annotations file\n",
    "        next(gtReader)  # skip header\n",
    "\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            img_path = prefix + row[0]\n",
    "            img = plt.imread(img_path)  # load image\n",
    "\n",
    "            label = int(row[7])  # the 8th column is the label\n",
    "            height = img.shape[0]  # height of the image\n",
    "            width = img.shape[1]  # width of the image\n",
    "            #channels = img.shape[2] if len(img.shape) > 2 else 1  # channels of the image (default to 1 if grayscale)\n",
    "            roi_x1 = int(row[3])  # ROI X1 coordinate\n",
    "            roi_y1 = int(row[4])  # ROI Y1 coordinate\n",
    "            roi_x2 = int(row[5])  # ROI X2 coordinate\n",
    "            roi_y2 = int(row[6])  # ROI Y2 coordinate\n",
    "\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "\n",
    "        gtFile.close()\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    \n",
    "    df.to_pickle(cache_path)\n",
    "    print(f\"Dataframe saved in: {cache_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cache = readTrafficSignsTrain(train_path, \"./myModules/data/df_train_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:'Speed limit (30km/h)', \n",
    "# 2:'Speed limit (50km/h)',\n",
    "# 12:'Priority road', \n",
    "# 14:'Stop', \n",
    "# 17:'No entry',\n",
    "# 41: 'Ende des Überholverbots',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE_TRAIN = 50 # Default: 50 Images of each class excluding values in :id_mapping. Used for Trainset\n",
    "SAMPLE_SIZE_TEST = 10 # Default: 10 Images of each class excluding values in :id_mapping . Used for Testset\n",
    "\n",
    "IMAGE_WIDTH = 280 # Image width in pixels to resize \n",
    "IMAGE_HEIGHT = 280 # Image height in pixels to resize \n",
    "\n",
    "train_path = './GTSRB/Final_Training/Images'\n",
    "test_path = './GTSRB/Final_Test/Images'\n",
    "\n",
    "id_mapping = {1: 1, 2: 2, 12: 3, 14: 4, 17: 5}\n",
    "unknown_label = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTrain(rootpath):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "    Arguments: path to the traffic sign data, for example './GTSRB/Training'\n",
    "    Returns: DataFrame containing labels, image shapes, ROIs, and image paths'''\n",
    "    \n",
    "    # Initialisiere eine leere Liste für die Daten\n",
    "    data = []\n",
    "\n",
    "    # loop over all 43 classes\n",
    "    for c in range(0, 43):\n",
    "        prefix = f\"{rootpath}/{c:05d}/\"  # subdirectory for class\n",
    "        gtFile = open(prefix + f'GT-{c:05d}.csv')  # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';')  # csv parser for annotations file\n",
    "        next(gtReader)  # skip header\n",
    "\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            img_path = prefix + row[0]\n",
    "            img = plt.imread(img_path)  # load image\n",
    "\n",
    "            label = int(row[7])  # the 8th column is the label\n",
    "            height = img.shape[0]  # height of the image\n",
    "            width = img.shape[1]  # width of the image\n",
    "            #channels = img.shape[2] if len(img.shape) > 2 else 1  # channels of the image (default to 1 if grayscale)\n",
    "            roi_x1 = int(row[3])  # ROI X1 coordinate\n",
    "            roi_y1 = int(row[4])  # ROI Y1 coordinate\n",
    "            roi_x2 = int(row[5])  # ROI X2 coordinate\n",
    "            roi_y2 = int(row[6])  # ROI Y2 coordinate\n",
    "\n",
    "            # Füge die Daten als Zeile hinzu\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "\n",
    "        gtFile.close()\n",
    "\n",
    "    # Erstelle ein DataFrame aus der Liste\n",
    "    df = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTest(rootpath):\n",
    "    '''Reads the final test data for the German Traffic Sign Recognition Benchmark.\n",
    "    Arguments: path to the final test data, for example './GTSRB/Final_Test/Images'\n",
    "    Returns: DataFrame with image data, labels, image shapes, and ROI\n",
    "    '''\n",
    "    # Pfad zur CSV-Datei\n",
    "    csv_file = os.path.join(rootpath, 'GT-final_test_gt.csv')\n",
    "    \n",
    "    # Lade die CSV-Datei\n",
    "    df = pd.read_csv(csv_file, delimiter=';')\n",
    "    \n",
    "    # Liste zum Speichern der Daten\n",
    "    data = []\n",
    "    \n",
    "    # Schleife über alle Zeilen der CSV-Datei\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.join(rootpath, row['Filename'])\n",
    "        img = plt.imread(img_path)  # Lade das Bild\n",
    "        \n",
    "        if img is not None:\n",
    "            label = int(row['ClassId'])\n",
    "            height, width, channels = img.shape\n",
    "            roi_x1 = int(row['Roi.X1'])\n",
    "            roi_y1 = int(row['Roi.Y1'])\n",
    "            roi_x2 = int(row['Roi.X2'])\n",
    "            roi_y2 = int(row['Roi.Y2'])\n",
    "            \n",
    "            # Füge die Daten als Zeile hinzu\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "        else:\n",
    "            print(f\"Bild {img_path} konnte nicht geladen werden.\")\n",
    "    \n",
    "    # Erstelle ein DataFrame aus der Liste\n",
    "    df_test = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((280, 280))  # Resize to 280x280\n",
    "    return np.array(image).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_labels(df, selected_labels, unknown_label, sample_size):\n",
    "    \"\"\"\n",
    "    Reassign labels in the dataframe according to selected_labels.\n",
    "    Keep all rows for the labels in selected_labels.\n",
    "    For remaining labels, sample a specified number of rows and reassign them to unknown_label.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with a column named 'Label'.\n",
    "    selected_labels (dict): A dictionary mapping old labels to new labels.\n",
    "    unknown_label (int): The label to assign to the remaining sampled rows.\n",
    "    sample_size (int): The number of rows to sample for each remaining label. Default is 15.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new dataframe with reassigned labels.\n",
    "    \"\"\"\n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    for old_label, new_label in selected_labels.items():\n",
    "        label_df = df[df['Label'] == old_label].copy()\n",
    "        label_df['Label'] = new_label\n",
    "        new_df = pd.concat([new_df, label_df])\n",
    "\n",
    "    unique_labels = df['Label'].unique()\n",
    "    remaining_labels = [label for label in unique_labels if label not in selected_labels]\n",
    "\n",
    "    for label in remaining_labels:\n",
    "        label_df = df[df['Label'] == label]\n",
    "        if len(label_df) > sample_size:\n",
    "            selected_rows = label_df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            selected_rows = label_df\n",
    "        selected_rows['Label'] = unknown_label\n",
    "        new_df = pd.concat([new_df, selected_rows])\n",
    "\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = readTrafficSignsTrain(train_path)\n",
    "df_train = pd.DataFrame()\n",
    "df_train = df_train_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_raw = readTrafficSignsTest(test_path)\n",
    "df_test = pd.DataFrame()\n",
    "df_test = df_test_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_train = reassign_labels(df_train, id_mapping, unknown_label, SAMPLE_SIZE_TRAIN)\n",
    "df_final_test = reassign_labels(df_test, id_mapping, unknown_label, SAMPLE_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Verfify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "short_classes = { 1: 'Geschwindigkeitsbegrenzung (30km/h)', \n",
    "                  2: 'Geschwindigkeitsbegrenzung (50km/h)', \n",
    "                  3: 'Vorrangstraße', \n",
    "                  4: 'Stop', \n",
    "                  5: 'Einfahrt verboten',\n",
    "                  6: 'Unbekannt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_train['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_test['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_label_df = df_final_train[df_final_train['Label'] == 6]\n",
    "\n",
    "# Wähle zufällig 100 Bilder aus\n",
    "image_paths_to_display = unknown_label_df['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "# Plotten der ausgewählten Bilder\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_final = df_final_train['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_final)\n",
    "\n",
    "label_counts_named_final = {short_classes[key]: value for key, value in label_counts_final.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_final.keys()), y=list(label_counts_named_final.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen für bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_test = df_final_test['Label'].value_counts().sort_index()\n",
    "#label_counts_train = df_train_short['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_train = {short_classes[key]: value for key, value in label_counts_train.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_train.keys()), y=list(label_counts_named_train.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen für bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_test = {short_classes[key]: value for key, value in label_counts_test.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_test.keys()), y=list(label_counts_named_test.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Testdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen für bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create Tf-records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(row):\n",
    "    img_path = row['Path']\n",
    "    # Lade das Bild und konvertiere es in ein kompatibles Format (z.B. JPEG)\n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    with io.BytesIO() as output:\n",
    "        image.save(output, format=\"JPEG\")\n",
    "        encoded_jpg = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = row['Path'].encode('utf8')\n",
    "    image_format = b'jpeg'  # Ändere dies entsprechend des konvertierten Bildformats\n",
    "    xmins = [row['Roi.X1'] / width]\n",
    "    xmaxs = [row['Roi.X2'] / width]\n",
    "    ymins = [row['Roi.Y1'] / height]\n",
    "    ymaxs = [row['Roi.Y2'] / height]\n",
    "    classes_text = [str(row['Label']).encode('utf8')]\n",
    "    classes = [int(row['Label'])]\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_jpg])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n",
    "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n",
    "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n",
    "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n",
    "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n",
    "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(df, output_path):\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    for _, row in df.iterrows():\n",
    "        tf_example = create_tf_example(row)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tf_record(df_final_train, './myModules/records/train.record')\n",
    "create_tf_record(df_final_test, './myModules/records/test.record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Verify Tensord records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_example(serialized_example):\n",
    "    feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/class/text': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(record_file):\n",
    "    raw_dataset = tf.data.TFRecordDataset(record_file)\n",
    "    parsed_dataset = raw_dataset.map(parse_tf_example)\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_at_index(parsed_dataset, index):\n",
    "    for i, tf_example in enumerate(parsed_dataset):\n",
    "        if i == index:\n",
    "            height = tf_example['image/height'].numpy()\n",
    "            width = tf_example['image/width'].numpy()\n",
    "            encoded_image = tf_example['image/encoded'].numpy()\n",
    "            xmin = tf_example['image/object/bbox/xmin'].numpy()\n",
    "            xmax = tf_example['image/object/bbox/xmax'].numpy()\n",
    "            ymin = tf_example['image/object/bbox/ymin'].numpy()\n",
    "            ymax = tf_example['image/object/bbox/ymax'].numpy()\n",
    "            label = tf_example['image/object/class/label'].numpy()\n",
    "\n",
    "            image = tf.image.decode_jpeg(encoded_image)\n",
    "            image_np = image.numpy()\n",
    "\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(image_np)\n",
    "            plt.title(f'Label: {label}')\n",
    "\n",
    "            # Draw the bounding box\n",
    "            plt.gca().add_patch(plt.Rectangle((xmin * width, ymin * height), \n",
    "                                              (xmax - xmin) * width, (ymax - ymin) * height,\n",
    "                                              edgecolor='green', facecolor='none', linewidth=2))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record = read_tfrecord('./myModules/records/train.record')\n",
    "test_record = read_tfrecord('./myModules/records/test.record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image_at_index(test_record, 888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image_at_index(train_record, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"ssd_resnet101_v1_fpn_640x640_coco17_tpu-8\"\n",
    "MODEL_NAME = \"ssd_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model {\n",
    "  ssd {\n",
    "    num_classes: 6\n",
    "    image_resizer {\n",
    "      fixed_shape_resizer {\n",
    "        height: 640\n",
    "        width: 640\n",
    "      }\n",
    "    }\n",
    "    feature_extractor {\n",
    "      type: \"ssd_resnet101_v1_fpn_keras\"\n",
    "      depth_multiplier: 1.0\n",
    "      min_depth: 16\n",
    "      conv_hyperparams {\n",
    "        regularizer {\n",
    "          l2_regularizer {\n",
    "            weight: 0.0004\n",
    "          }\n",
    "        }\n",
    "        initializer {\n",
    "          truncated_normal_initializer {\n",
    "            mean: 0.0\n",
    "            stddev: 0.03\n",
    "          }\n",
    "        }\n",
    "        activation: RELU_6\n",
    "        batch_norm {\n",
    "          decay: 0.997\n",
    "          scale: true\n",
    "          epsilon: 0.001\n",
    "        }\n",
    "      }\n",
    "      override_base_feature_extractor_hyperparams: true\n",
    "      fpn {\n",
    "        min_level: 3\n",
    "        max_level: 7\n",
    "      }\n",
    "    }\n",
    "    box_coder {\n",
    "      faster_rcnn_box_coder {\n",
    "        y_scale: 10.0\n",
    "        x_scale: 10.0\n",
    "        height_scale: 5.0\n",
    "        width_scale: 5.0\n",
    "      }\n",
    "    }\n",
    "    matcher {\n",
    "      argmax_matcher {\n",
    "        matched_threshold: 0.5\n",
    "        unmatched_threshold: 0.5\n",
    "        ignore_thresholds: false\n",
    "        negatives_lower_than_unmatched: true\n",
    "        force_match_for_each_row: true\n",
    "        use_matmul_gather: true\n",
    "      }\n",
    "    }\n",
    "    similarity_calculator {\n",
    "      iou_similarity {\n",
    "      }\n",
    "    }\n",
    "    box_predictor {\n",
    "      weight_shared_convolutional_box_predictor {\n",
    "        conv_hyperparams {\n",
    "          regularizer {\n",
    "            l2_regularizer {\n",
    "              weight: 0.0004\n",
    "            }\n",
    "          }\n",
    "          initializer {\n",
    "            random_normal_initializer {\n",
    "              mean: 0.0\n",
    "              stddev: 0.01\n",
    "            }\n",
    "          }\n",
    "          activation: RELU_6\n",
    "          batch_norm {\n",
    "            decay: 0.997\n",
    "            scale: true\n",
    "            epsilon: 0.001\n",
    "          }\n",
    "        }\n",
    "        depth: 256\n",
    "        num_layers_before_predictor: 4\n",
    "        kernel_size: 3\n",
    "        class_prediction_bias_init: -4.6\n",
    "      }\n",
    "    }\n",
    "    anchor_generator {\n",
    "      multiscale_anchor_generator {\n",
    "        min_level: 3\n",
    "        max_level: 7\n",
    "        anchor_scale: 4.0\n",
    "        aspect_ratios: 1.0\n",
    "        aspect_ratios: 2.0\n",
    "        aspect_ratios: 0.5\n",
    "        scales_per_octave: 2\n",
    "      }\n",
    "    }\n",
    "    post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 1e-8\n",
    "        iou_threshold: 0.6\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 100\n",
    "        use_static_shapes: false\n",
    "      }\n",
    "      score_converter: SIGMOID\n",
    "    }\n",
    "    normalize_loss_by_num_matches: true\n",
    "    loss {\n",
    "      localization_loss {\n",
    "        weighted_smooth_l1 {\n",
    "        }\n",
    "      }\n",
    "      classification_loss {\n",
    "        weighted_sigmoid_focal {\n",
    "          gamma: 2.0\n",
    "          alpha: 0.25\n",
    "        }\n",
    "      }\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }\n",
    "    encode_background_as_zeros: true\n",
    "    normalize_loc_loss_by_codesize: true\n",
    "    inplace_batchnorm_update: true\n",
    "    freeze_batchnorm: false\n",
    "  }\n",
    "}\n",
    "train_config {\n",
    "  batch_size: 4\n",
    "  data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_adjust_brightness {\n",
    "      max_delta: 0.2\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_adjust_contrast {\n",
    "      min_delta: 0.8\n",
    "      max_delta: 1.2\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_rotation90 {\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_crop_image {\n",
    "      min_object_covered: 0.0\n",
    "      min_aspect_ratio: 0.75\n",
    "      max_aspect_ratio: 1.33\n",
    "      min_area: 0.75\n",
    "      max_area: 1.0\n",
    "      overlap_thresh: 0.0\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_jpeg_quality {\n",
    "      min_jpeg_quality: 50\n",
    "      max_jpeg_quality: 100\n",
    "    }\n",
    "  }\n",
    "  sync_replicas: true\n",
    "  optimizer {\n",
    "    momentum_optimizer {\n",
    "      learning_rate {\n",
    "        cosine_decay_learning_rate {\n",
    "          learning_rate_base: 0.01\n",
    "          total_steps: 25000\n",
    "          warmup_learning_rate: 0.003333\n",
    "          warmup_steps: 2000\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.9\n",
    "    }\n",
    "    use_moving_average: false\n",
    "  }\n",
    "  fine_tune_checkpoint: \"./base_models/ssd/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
    "  num_steps: 8000\n",
    "  startup_delay_steps: 0.0\n",
    "  replicas_to_aggregate: 8\n",
    "  max_number_of_boxes: 100\n",
    "  unpad_groundtruth_tensors: false\n",
    "  fine_tune_checkpoint_type: \"detection\"\n",
    "  use_bfloat16: false\n",
    "  fine_tune_checkpoint_version: V2\n",
    "}\n",
    "train_input_reader {\n",
    "  label_map_path: \"./myModules/label_map_short.pbtxt\"\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"./myModules/records/train.record\"\n",
    "  }\n",
    "}\n",
    "eval_config {\n",
    "  metrics_set: \"coco_detection_metrics\"\n",
    "  use_moving_averages: false\n",
    "}\n",
    "eval_input_reader {\n",
    "  label_map_path: \"./myModules/label_map_short.pbtxt\"\n",
    "  shuffle: false\n",
    "  num_epochs: 1\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"./myModules/records/train.record\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "NUM_CLASSES = 6\n",
    "NUM_STEPS = 8000\n",
    "NUM_EVAL_STEPS = 1000\n",
    "use_bfloat16 = False # Use bfloat16 True for TPU\n",
    "\n",
    "NOTES = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_length(df):\n",
    "    count_images = len(df)\n",
    "    labels_count = df['Label'].value_counts()\n",
    "\n",
    "    sorted_labels_count = labels_count.sort_index()\n",
    "    return sorted_labels_count.tolist(), count_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_info(batch, classes, steps, eval_steps, name, bfloat16, pipeline, \n",
    "                   checkpoint_path, labelmap, train_record, test_record, config, \n",
    "                   trained_model, train_time, df_train, df_test, notes):\n",
    "    \n",
    "    \n",
    "    test_count, test_len = count_length(df_test)\n",
    "    train_count, train_len = count_length(df_train)   \n",
    "    log_contents = f\"\"\"\n",
    "Trainingparameters:\n",
    "    BATCH_SIZE = {batch} # Cannot be higher than 2-4 (lack of ressources)\n",
    "    NUM_CLASSES = {classes} # Total number of classes to train is 43 - used for training are only 6-7\n",
    "    NUM_STEPS = {steps}\n",
    "    NUM_EVAL_STEPS = {eval_steps}\n",
    "    MODEL_NAME = {name}\n",
    "    use_bfloat16 = {bfloat16} # Use bfloat16 = True for trainign with TPU\n",
    "    \n",
    "Locations: \n",
    "    Path to Pipeline-Config = {pipeline}\n",
    "    Checkpoint from transfermodel  = {checkpoint_path}\n",
    "\n",
    "    Path to the trainedmodel = {trained_model}\n",
    "    Config of the trainedmodel = {config}\n",
    "    \n",
    "Used records: \n",
    "    Used labelmap = {labelmap}\n",
    "    Used train_record = {train_record}\n",
    "    Used test_record = {test_record}\n",
    "\n",
    "Generell Information: \n",
    "    Time needed for modeltraining = {train_time}\n",
    "    Length of traindataset = {train_len}\n",
    "    Counted values for each Label form Traindataset = {train_count}\n",
    "    Length of testdataset = {test_len}\n",
    "    Counted values for each Label form Testdataset = {test_count}\n",
    "\"\"\"\n",
    "\n",
    "    log_id = uuid.uuid4()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_entry = f\"ID: {log_id}\\nTimestamp: {timestamp}\\n\\n{log_contents}\\nNotes: {notes}\\n\"\n",
    "\n",
    "    filename = f\"./myModules/log/{name}_{log_id}.txt\"\n",
    "    with open(filename, \"a\") as file:\n",
    "        file.write(log_entry)\n",
    "        file.write(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from path \n",
    "pipeline_config = \"./base_models/ssd/{name}/pipeline.config\".format(name=BASE_MODEL)\n",
    "fine_tune_checkpoint_path = \"./base_models/ssd/{name}/checkpoint/ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "# Save to path\n",
    "model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)\n",
    "pipeline_config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "# Upload from path\n",
    "short_labelmap_path = \"./myModules/label_map_short.pbtxt\"\n",
    "short_train_record_path = \"./myModules/records/train.record\"\n",
    "short_test_record_path = \"./myModules/records/test.record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pipeline_config) as f:\n",
    "    config = f.read()\n",
    "\n",
    "with open(pipeline_config_path, 'w') as f:\n",
    "  \n",
    "  # Set labelmap path\n",
    "  config = re.sub('label_map_path: \".*?\"', \n",
    "             'label_map_path: \"{}\"'.format(short_labelmap_path), config)\n",
    "  \n",
    "  # Set fine_tune_checkpoint path\n",
    "  config = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                  'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint_path), config)\n",
    "  \n",
    "  # Set train tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(short_train_record_path), config)\n",
    "  \n",
    "  # Set test tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(short_test_record_path), config)\n",
    "  \n",
    "  # Set number of classes.\n",
    "  config = re.sub('num_classes: [0-9]+',\n",
    "                  'num_classes: {}'.format(NUM_CLASSES), config)\n",
    "  \n",
    "  # Set batch size\n",
    "  config = re.sub('batch_size: [0-9]+',\n",
    "                  'batch_size: {}'.format(BATCH_SIZE), config)\n",
    "  \n",
    "  # Set training steps\n",
    "  config = re.sub('num_steps: [0-9]+',\n",
    "                  'num_steps: {}'.format(NUM_STEPS), config)\n",
    "  \n",
    "    # Set use_bfloat16\n",
    "  config = re.sub('use_bfloat16: (true|false)',\n",
    "                  'use_bfloat16: {}'.format(str(use_bfloat16).lower()), config)\n",
    "  \n",
    "  # Set fine-tune checkpoint type to detection\n",
    "  config = re.sub('fine_tune_checkpoint_type: \"classification\"', \n",
    "             'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n",
    "  \n",
    "  f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model {\n",
      "  ssd {\n",
      "    num_classes: 6\n",
      "    image_resizer {\n",
      "      fixed_shape_resizer {\n",
      "        height: 640\n",
      "        width: 640\n",
      "      }\n",
      "    }\n",
      "    feature_extractor {\n",
      "      type: \"ssd_resnet101_v1_fpn_keras\"\n",
      "      depth_multiplier: 1.0\n",
      "      min_depth: 16\n",
      "      conv_hyperparams {\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 0.0004\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          truncated_normal_initializer {\n",
      "            mean: 0.0\n",
      "            stddev: 0.03\n",
      "          }\n",
      "        }\n",
      "        activation: RELU_6\n",
      "        batch_norm {\n",
      "          decay: 0.997\n",
      "          scale: true\n",
      "          epsilon: 0.001\n",
      "        }\n",
      "      }\n",
      "      override_base_feature_extractor_hyperparams: true\n",
      "      fpn {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "      }\n",
      "    }\n",
      "    box_coder {\n",
      "      faster_rcnn_box_coder {\n",
      "        y_scale: 10.0\n",
      "        x_scale: 10.0\n",
      "        height_scale: 5.0\n",
      "        width_scale: 5.0\n",
      "      }\n",
      "    }\n",
      "    matcher {\n",
      "      argmax_matcher {\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.5\n",
      "        ignore_thresholds: false\n",
      "        negatives_lower_than_unmatched: true\n",
      "        force_match_for_each_row: true\n",
      "        use_matmul_gather: true\n",
      "      }\n",
      "    }\n",
      "    similarity_calculator {\n",
      "      iou_similarity {\n",
      "      }\n",
      "    }\n",
      "    box_predictor {\n",
      "      weight_shared_convolutional_box_predictor {\n",
      "        conv_hyperparams {\n",
      "          regularizer {\n",
      "            l2_regularizer {\n",
      "              weight: 0.0004\n",
      "            }\n",
      "          }\n",
      "          initializer {\n",
      "            random_normal_initializer {\n",
      "              mean: 0.0\n",
      "              stddev: 0.01\n",
      "            }\n",
      "          }\n",
      "          activation: RELU_6\n",
      "          batch_norm {\n",
      "            decay: 0.997\n",
      "            scale: true\n",
      "            epsilon: 0.001\n",
      "          }\n",
      "        }\n",
      "        depth: 256\n",
      "        num_layers_before_predictor: 4\n",
      "        kernel_size: 3\n",
      "        class_prediction_bias_init: -4.6\n",
      "      }\n",
      "    }\n",
      "    anchor_generator {\n",
      "      multiscale_anchor_generator {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "        anchor_scale: 4.0\n",
      "        aspect_ratios: 1.0\n",
      "        aspect_ratios: 2.0\n",
      "        aspect_ratios: 0.5\n",
      "        scales_per_octave: 2\n",
      "      }\n",
      "    }\n",
      "    post_processing {\n",
      "      batch_non_max_suppression {\n",
      "        score_threshold: 1e-8\n",
      "        iou_threshold: 0.6\n",
      "        max_detections_per_class: 100\n",
      "        max_total_detections: 100\n",
      "        use_static_shapes: false\n",
      "      }\n",
      "      score_converter: SIGMOID\n",
      "    }\n",
      "    normalize_loss_by_num_matches: true\n",
      "    loss {\n",
      "      localization_loss {\n",
      "        weighted_smooth_l1 {\n",
      "        }\n",
      "      }\n",
      "      classification_loss {\n",
      "        weighted_sigmoid_focal {\n",
      "          gamma: 2.0\n",
      "          alpha: 0.25\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 1.0\n",
      "    }\n",
      "    encode_background_as_zeros: true\n",
      "    normalize_loc_loss_by_codesize: true\n",
      "    inplace_batchnorm_update: true\n",
      "    freeze_batchnorm: false\n",
      "  }\n",
      "}\n",
      "train_config {\n",
      "  batch_size: 2\n",
      "  data_augmentation_options {\n",
      "    random_horizontal_flip {\n",
      "    }\n",
      "  }\n",
      "  data_augmentation_options {\n",
      "    random_adjust_brightness {\n",
      "      max_delta: 0.2\n",
      "    }\n",
      "  }\n",
      "  data_augmentation_options {\n",
      "    random_adjust_contrast {\n",
      "      min_delta: 0.8\n",
      "      max_delta: 1.2\n",
      "    }\n",
      "  }\n",
      "  data_augmentation_options {\n",
      "    random_rotation90 {\n",
      "    }\n",
      "  }\n",
      "  data_augmentation_options {\n",
      "    random_crop_image {\n",
      "      min_object_covered: 0.0\n",
      "      min_aspect_ratio: 0.75\n",
      "      max_aspect_ratio: 1.33\n",
      "      min_area: 0.75\n",
      "      max_area: 1.0\n",
      "      overlap_thresh: 0.0\n",
      "    }\n",
      "  }\n",
      "  data_augmentation_options {\n",
      "    random_jpeg_quality {\n",
      "      min_jpeg_quality: 50\n",
      "      max_jpeg_quality: 100\n",
      "    }\n",
      "  }\n",
      "  sync_replicas: true\n",
      "  optimizer {\n",
      "    momentum_optimizer {\n",
      "      learning_rate {\n",
      "        cosine_decay_learning_rate {\n",
      "          learning_rate_base: 0.01\n",
      "          total_steps: 25000\n",
      "          warmup_learning_rate: 0.003333\n",
      "          warmup_steps: 2000\n",
      "        }\n",
      "      }\n",
      "      momentum_optimizer_value: 0.9\n",
      "    }\n",
      "    use_moving_average: false\n",
      "  }\n",
      "  fine_tune_checkpoint: \"./base_models/ssd/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
      "  num_steps: 8000\n",
      "  startup_delay_steps: 0.0\n",
      "  replicas_to_aggregate: 8\n",
      "  max_number_of_boxes: 100\n",
      "  unpad_groundtruth_tensors: false\n",
      "  fine_tune_checkpoint_type: \"detection\"\n",
      "  use_bfloat16: false\n",
      "  fine_tune_checkpoint_version: V2\n",
      "}\n",
      "train_input_reader {\n",
      "  label_map_path: \"./myModules/label_map_short.pbtxt\"\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/train.record\"\n",
      "  }\n",
      "}\n",
      "eval_config {\n",
      "  metrics_set: \"coco_detection_metrics\"\n",
      "  use_moving_averages: false\n",
      "}\n",
      "eval_input_reader {\n",
      "  label_map_path: \"./myModules/label_map_short.pbtxt\"\n",
      "  shuffle: false\n",
      "  num_epochs: 1\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/train.record\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(pipeline_config_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from path \n",
    "pipeline_config = \"./base_models/faster_rcnn/{name}/pipeline.config\".format(name=BASE_MODEL)\n",
    "fine_tune_checkpoint_path = \"./base_models/faster_rcnn/{name}/checkpoint/ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "# Save to path\n",
    "model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "pipeline_config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "# Upload from path\n",
    "short_labelmap_path = \"./myModules/label_map_short.pbtxt\"\n",
    "short_train_record_path = \"./myModules/records/train.record\"\n",
    "short_test_record_path = \"./myModules/records/test.record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pipeline_config) as f:\n",
    "    config = f.read()\n",
    "\n",
    "with open(pipeline_config_path, 'w') as f:\n",
    "  \n",
    "  # Set labelmap path\n",
    "  config = re.sub('label_map_path: \".*?\"', \n",
    "             'label_map_path: \"{}\"'.format(short_labelmap_path), config)\n",
    "  \n",
    "  # Set fine_tune_checkpoint path\n",
    "  config = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                  'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint_path), config)\n",
    "  \n",
    "  # Set train tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(short_train_record_path), config)\n",
    "  \n",
    "  # Set test tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(short_test_record_path), config)\n",
    "  \n",
    "  # Set number of classes.\n",
    "  config = re.sub('num_classes: [0-9]+',\n",
    "                  'num_classes: {}'.format(NUM_CLASSES), config)\n",
    "  \n",
    "  # Set batch size\n",
    "  config = re.sub('batch_size: [0-9]+',\n",
    "                  'batch_size: {}'.format(BATCH_SIZE), config)\n",
    "  \n",
    "  # Set training steps\n",
    "  config = re.sub('num_steps: [0-9]+',\n",
    "                  'num_steps: {}'.format(NUM_STEPS), config)\n",
    "  \n",
    "    # Set use_bfloat16\n",
    "  config = re.sub('use_bfloat16: (true|false)',\n",
    "                  'use_bfloat16: {}'.format(str(use_bfloat16).lower()), config)\n",
    "  \n",
    "  # Set fine-tune checkpoint type to detection\n",
    "  config = re.sub('fine_tune_checkpoint_type: \"classification\"', \n",
    "             'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n",
    "  \n",
    "  f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pipeline_config_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py \\\n",
    "    --pipeline_config_path={pipeline_config_path} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={NUM_STEPS} \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --num_eval_steps={NUM_EVAL_STEPS}\n",
    "\n",
    "training_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_info(\n",
    "    batch=BATCH_SIZE,\n",
    "    classes=NUM_CLASSES,\n",
    "    steps=NUM_STEPS,\n",
    "    eval_steps=NUM_EVAL_STEPS,\n",
    "    name=MODEL_NAME,\n",
    "    bfloat16=use_bfloat16,\n",
    "    pipeline=pipeline_config,\n",
    "    checkpoint_path=fine_tune_checkpoint_path,\n",
    "    labelmap=short_labelmap_path,\n",
    "    train_record=short_train_record_path,\n",
    "    test_record=short_test_record_path,\n",
    "    config=pipeline_config_path,\n",
    "    trained_model=model_dir,\n",
    "    train_time=training_time,\n",
    "    df_test=df_final_test,\n",
    "    df_train=df_final_train,\n",
    "    notes=NOTES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\model_main_tf2.py \\\n",
    "            --pipeline_config_path={pipeline_config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={model_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "output, error = process.communicate()\n",
    "\n",
    "if process.returncode != 0:\n",
    "    print(error.decode(\"utf-8\"))\n",
    "else:\n",
    "    output_str = output.decode(\"utf-8\")\n",
    "    header = f\"Evaluation Results for: {MODEL_NAME}\\n\\n\"\n",
    "    with open(f\"./myModules/log/{MODEL_NAME}_evaluation_results.txt\", \"w\") as f:\n",
    "        f.write(header)\n",
    "        f.write(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py \\\n",
    "    --pipeline_config_path={pipeline_config_path} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --checkpoint_dir={model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoints(model_dir):\n",
    "    \"\"\"Returns a list of checkpoint paths in the given directory.\"\"\"\n",
    "    checkpoints = []\n",
    "    for filename in os.listdir(model_dir):\n",
    "        if filename.startswith('ckpt-') and filename.endswith('.index'):\n",
    "            checkpoint = os.path.splitext(filename)[0]\n",
    "            checkpoints.append(os.path.join(model_dir, checkpoint))\n",
    "    return sorted(set(checkpoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_checkpoints(pipeline_config_path, model_dir, checkpoints, model_name):\n",
    "    \"\"\"Evaluates all checkpoints and logs the results.\"\"\"\n",
    "    for checkpoint in checkpoints:\n",
    "        eval_command = f\"python C:/Users/Alexej/Desktop/GTSRB/models/research/object_detection/model_main_tf2.py \\\n",
    "            --pipeline_config_path={pipeline_config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={checkpoint} \\\n",
    "            --run_once\"\n",
    "\n",
    "        # Starte den Evaluierungsprozess\n",
    "        process = subprocess.Popen(eval_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "        output, error = process.communicate()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f\"Error evaluating checkpoint {checkpoint}:\")\n",
    "            print(error.decode(\"utf-8\"))\n",
    "        else:\n",
    "            output_str = output.decode(\"utf-8\")\n",
    "            checkpoint_name = os.path.basename(checkpoint)\n",
    "            header = f\"Evaluation Results for MODEL_NAME: {model_name} at Checkpoint: {checkpoint_name}\\n\\n\"\n",
    "            with open(f\"./myModules/log/{model_name}_evaluation_results_{checkpoint_name}.txt\", \"w\") as f:\n",
    "                f.write(header)\n",
    "                f.write(output_str)\n",
    "            print(f\"Evaluation for checkpoint {checkpoint} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = get_checkpoints(model_dir)\n",
    "\n",
    "evaluate_checkpoints(pipeline_config_path, model_dir, checkpoints, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_multiple_images(model, image, min_score_threshold):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Filter out detections with a score below the threshold\n",
    "    scores = output_dict['detection_scores']\n",
    "    high_score_indices = scores >= min_score_threshold\n",
    "\n",
    "    output_dict['detection_boxes'] = output_dict['detection_boxes'][high_score_indices]\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'][high_score_indices]\n",
    "    output_dict['detection_scores'] = output_dict['detection_scores'][high_score_indices]\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path, width, height):\n",
    "    image = Image.open(path)\n",
    "    # Resize the image to a larger size\n",
    "    image = image.resize((width, height))  # Resize to 256x256, you can change this as needed\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes_with_color(image, boxes, classes, scores, category_index, box_color, line_thickness):\n",
    "    for i in range(boxes.shape[0]):\n",
    "        if scores is None or scores[i] > 0.5:\n",
    "            class_id = int(classes[i])\n",
    "            if class_id in category_index.keys():\n",
    "                class_name = category_index[class_id]['name']\n",
    "                display_str = str(class_name)\n",
    "                color = box_color\n",
    "                vis_util.draw_bounding_box_on_image_array(\n",
    "                    image,\n",
    "                    boxes[i][0],\n",
    "                    boxes[i][1],\n",
    "                    boxes[i][2],\n",
    "                    boxes[i][3],\n",
    "                    color=color,\n",
    "                    thickness=line_thickness,\n",
    "                    display_str_list=[display_str],\n",
    "                    use_normalized_coordinates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_legend(image, groundtruth_classes, category_index, iou):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    image_height = image.size[1]\n",
    "    font_size = max(int(image_height * 0.04), 12)  # Schriftgröße auf 4% der Bildhöhe begrenzt, aber mindestens 12\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "\n",
    "    y_offset = int(0.9 * image_height)  # Startoffset für die Legende, z.B. 90% der Bildhöhe\n",
    "    x_offset = int(0.05 * image.size[0])  # Startoffset für die Legende, z.B. 5% der Bildbreite\n",
    "\n",
    "    # Ground Truth Legende\n",
    "    for gt_class in groundtruth_classes:\n",
    "        if gt_class in category_index:\n",
    "            class_name = category_index[gt_class]['name']\n",
    "            draw.text((x_offset, y_offset), f\"GT: {class_name}\\nIoU: {iou:.2f}\", fill=\"yellow\", font=font)\n",
    "            y_offset += int(font_size * 1.5)  # Erhöhe den Offset basierend auf der aktuellen Schriftgröße"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxA, boxB):\n",
    "    yA = max(boxA[0], boxB[0])\n",
    "    xA = max(boxA[1], boxB[1])\n",
    "    yB = min(boxA[2], boxB[2])\n",
    "    xB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    \n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    \n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    \n",
    "    image_pil = Image.fromarray(np.uint8(image_np)).convert(\"RGB\")\n",
    "    add_legend(image_pil, groundtruth_classes, category_index, iou)\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, ax):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    ax.imshow(image_np)\n",
    "    ax.axis('off')  # Hide the axis\n",
    "\n",
    "    # Adding legend to the plot\n",
    "    label_text = category_index[groundtruth_classes[0]]['name']\n",
    "    ax.set_title(f'{label_text}\\nIoU: {iou:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_images_for_labels(df, model, category_index, discard_predictions_below_acc, num_images_to_display):\n",
    "    sorted_labels = sorted(df['Label'].unique())\n",
    "\n",
    "    num_rows = len(sorted_labels)\n",
    "    num_cols = num_images_to_display\n",
    "\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols + num_cols * 0.5, num_rows * 2))\n",
    "    \n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, num_cols)\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for idx, label in enumerate(sorted_labels):\n",
    "        label_df = df[df['Label'] == label]\n",
    "        \n",
    "        random_indices = random.sample(range(len(label_df)), min(num_images_to_display, len(label_df)))\n",
    "\n",
    "        for jdx, r_idx in enumerate(random_indices):\n",
    "            row = label_df.iloc[r_idx]\n",
    "            image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "            groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "            groundtruth_classes = [row['Label']]\n",
    "            \n",
    "            output_dict = run_inference_for_multiple_images(model, image_np, discard_predictions_below_acc)\n",
    "            \n",
    "            plot_idx = idx * num_images_to_display + jdx\n",
    "            \n",
    "            visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, axes[plot_idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_predictions_with_groundtruth (model, df, num_images_to_display, category_index):\n",
    "    \n",
    "    random_indices = random.sample(range(len(df)), num_images_to_display)\n",
    "\n",
    "    # Visualisiere zufällige ausgewählte Bilder\n",
    "    for idx in random_indices:\n",
    "        row = df.iloc[idx]\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_endless_predictions(model, df, category_index):\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_endless_predictions_from_path(model, path, category_index):\n",
    "    \n",
    "    for image_path in glob.glob(path):\n",
    "        image_np = load_image_into_numpy_array(image_path, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=1)\n",
    "        display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './inference/faster_rcnn/{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\exporter_main_v2.py \\\n",
    "    --trained_checkpoint_dir {model_dir} \\\n",
    "    --output_directory {output_directory} \\\n",
    "    --pipeline_config_path {pipeline_config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(short_labelmap_path, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load(f'./{output_directory}/saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Inference Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = './GTSRB/Final_Test/Images/*.ppm'\n",
    "min_score_threshold = 0.6\n",
    "number_of_images_to_display = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_endless_predictions_from_path(model, image_path, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_endless_predictions(model, df_final_test, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_predictions_with_groundtruth(model, df_final_test, number_of_images_to_display, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_images_for_labels(df_final_test, model, category_index, min_score_threshold, number_of_images_to_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_labels = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "# Visualisiere zufällig ausgewählte Bilder und sammle Vorhersagen und Ground Truth\n",
    "for idx, row in df_test_raw.iterrows():\n",
    "    row = df_test_raw.iloc[idx]\n",
    "    image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "    groundtruth_classes = [row['Label']]\n",
    "    \n",
    "    # Führe die Inferenz für das Bild durch\n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "    \n",
    "    # Sammle Vorhersagen und Ground Truth\n",
    "    predicted_class = output_dict['detection_classes'][0]\n",
    "    all_predicted_labels.append(predicted_class)\n",
    "    all_true_labels.append(groundtruth_classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
    "recall = recall_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "precision = precision_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "f1 = f1_score(all_true_labels, all_predicted_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle die Konfusionsmatrix\n",
    "cm = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "# Definiere die Klassenlabels (optional, wenn bekannt)\n",
    "\n",
    "# Plotte die Konfusionsmatrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Vorhersage')\n",
    "plt.ylabel('Wahre Werte')\n",
    "plt.title('Konfusionsmatrix')\n",
    "plt.show()\n",
    "\n",
    "# Drucke den Klassifikationsbericht\n",
    "print(classification_report(all_true_labels, all_predicted_labels, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-Time detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"./myModules/configs/\"\n",
    "CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\Faster_RCNN_640_50_fixed\\\\'\n",
    "LABLE_MAP_PATH = \"./myModules/label_map_short.pbtx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-8')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=1,\n",
    "                min_score_thresh=.9,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        break\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
