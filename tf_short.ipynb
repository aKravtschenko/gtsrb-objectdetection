{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2 \n",
    "import subprocess\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from six import BytesIO\n",
    "\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = './myModules/faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.config'\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "print(\"Model was successfully built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "#Checking for gpu\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "   raise SystemError('GPU device not found')\n",
    "\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(21)\n",
    "tf.compat.v1.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTrain(rootpath, cache_path):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "    Arguments:\n",
    "        rootpath: path to the traffic sign data, for example './GTSRB/Training'\n",
    "        cache_path: path to the cached DataFrame file, default is 'traffic_signs_data.pkl'\n",
    "    Returns:\n",
    "        DataFrame containing labels, image shapes, ROIs, and image paths\n",
    "    '''\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading: {cache_path}\")\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    data = []\n",
    "    # loop over all 43 classes\n",
    "    for c in range(0, 43):\n",
    "        prefix = f\"{rootpath}/{c:05d}/\"  # subdirectory for class\n",
    "        gtFile = open(prefix + f'GT-{c:05d}.csv')  # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';')  # csv parser for annotations file\n",
    "        next(gtReader)  # skip header\n",
    "\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            img_path = prefix + row[0]\n",
    "            img = plt.imread(img_path)  # load image\n",
    "\n",
    "            label = int(row[7])  # the 8th column is the label\n",
    "            height = img.shape[0]  # height of the image\n",
    "            width = img.shape[1]  # width of the image\n",
    "            #channels = img.shape[2] if len(img.shape) > 2 else 1  # channels of the image (default to 1 if grayscale)\n",
    "            roi_x1 = int(row[3])  # ROI X1 coordinate\n",
    "            roi_y1 = int(row[4])  # ROI Y1 coordinate\n",
    "            roi_x2 = int(row[5])  # ROI X2 coordinate\n",
    "            roi_y2 = int(row[6])  # ROI Y2 coordinate\n",
    "\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "\n",
    "        gtFile.close()\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    \n",
    "    df.to_pickle(cache_path)\n",
    "    print(f\"Dataframe saved in: {cache_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cache = readTrafficSignsTrain(train_path, \"./myModules/data/df_train_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:'Speed limit (30km/h)', \n",
    "# 2:'Speed limit (50km/h)',\n",
    "# 12:'Priority road', \n",
    "# 14:'Stop', \n",
    "# 17:'No entry',\n",
    "# 41: 'Ende des Überholverbots',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE_TRAIN = 50 # Default: 50 Images of each class excluding values in :id_mapping. Used for Trainset\n",
    "SAMPLE_SIZE_TEST = 10 # Default: 10 Images of each class excluding values in :id_mapping . Used for Testset\n",
    "\n",
    "IMAGE_WIDTH = 280 # Image width in pixels to resize \n",
    "IMAGE_HEIGHT = 280 # Image height in pixels to resize \n",
    "\n",
    "train_path = './GTSRB/Final_Training/Images'\n",
    "test_path = './GTSRB/Final_Test/Images'\n",
    "\n",
    "id_mapping = {1: 1, 2: 2, 12: 3, 14: 4, 17: 5}\n",
    "unknown_label = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTrain(rootpath):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "    Arguments: path to the traffic sign data, for example './GTSRB/Training'\n",
    "    Returns: DataFrame containing labels, image shapes, ROIs, and image paths'''\n",
    "    \n",
    "    # Initialisiere eine leere Liste für die Daten\n",
    "    data = []\n",
    "\n",
    "    # loop over all 43 classes\n",
    "    for c in range(0, 43):\n",
    "        prefix = f\"{rootpath}/{c:05d}/\"  # subdirectory for class\n",
    "        gtFile = open(prefix + f'GT-{c:05d}.csv')  # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';')  # csv parser for annotations file\n",
    "        next(gtReader)  # skip header\n",
    "\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            img_path = prefix + row[0]\n",
    "            img = plt.imread(img_path)  # load image\n",
    "\n",
    "            label = int(row[7])  # the 8th column is the label\n",
    "            height = img.shape[0]  # height of the image\n",
    "            width = img.shape[1]  # width of the image\n",
    "            #channels = img.shape[2] if len(img.shape) > 2 else 1  # channels of the image (default to 1 if grayscale)\n",
    "            roi_x1 = int(row[3])  # ROI X1 coordinate\n",
    "            roi_y1 = int(row[4])  # ROI Y1 coordinate\n",
    "            roi_x2 = int(row[5])  # ROI X2 coordinate\n",
    "            roi_y2 = int(row[6])  # ROI Y2 coordinate\n",
    "\n",
    "            # Füge die Daten als Zeile hinzu\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "\n",
    "        gtFile.close()\n",
    "\n",
    "    # Erstelle ein DataFrame aus der Liste\n",
    "    df = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsTest(rootpath):\n",
    "    '''Reads the final test data for the German Traffic Sign Recognition Benchmark.\n",
    "    Arguments: path to the final test data, for example './GTSRB/Final_Test/Images'\n",
    "    Returns: DataFrame with image data, labels, image shapes, and ROI\n",
    "    '''\n",
    "    # Pfad zur CSV-Datei\n",
    "    csv_file = os.path.join(rootpath, 'GT-final_test_gt.csv')\n",
    "    \n",
    "    # Lade die CSV-Datei\n",
    "    df = pd.read_csv(csv_file, delimiter=';')\n",
    "    \n",
    "    # Liste zum Speichern der Daten\n",
    "    data = []\n",
    "    \n",
    "    # Schleife über alle Zeilen der CSV-Datei\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.join(rootpath, row['Filename'])\n",
    "        img = plt.imread(img_path)  # Lade das Bild\n",
    "        \n",
    "        if img is not None:\n",
    "            label = int(row['ClassId'])\n",
    "            height, width, channels = img.shape\n",
    "            roi_x1 = int(row['Roi.X1'])\n",
    "            roi_y1 = int(row['Roi.Y1'])\n",
    "            roi_x2 = int(row['Roi.X2'])\n",
    "            roi_y2 = int(row['Roi.Y2'])\n",
    "            \n",
    "            # Füge die Daten als Zeile hinzu\n",
    "            data.append([width, height, roi_x1, roi_y1, roi_x2, roi_y2, img_path, label])\n",
    "        else:\n",
    "            print(f\"Bild {img_path} konnte nicht geladen werden.\")\n",
    "    \n",
    "    # Erstelle ein DataFrame aus der Liste\n",
    "    df_test = pd.DataFrame(data, columns=['Width', 'Height','Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'Path', 'Label'])\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((280, 280))  # Resize to 280x280\n",
    "    return np.array(image).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_labels(df, selected_labels, unknown_label, sample_size):\n",
    "    \"\"\"\n",
    "    Reassign labels in the dataframe according to selected_labels.\n",
    "    Keep all rows for the labels in selected_labels.\n",
    "    For remaining labels, sample a specified number of rows and reassign them to unknown_label.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with a column named 'Label'.\n",
    "    selected_labels (dict): A dictionary mapping old labels to new labels.\n",
    "    unknown_label (int): The label to assign to the remaining sampled rows.\n",
    "    sample_size (int): The number of rows to sample for each remaining label. Default is 15.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new dataframe with reassigned labels.\n",
    "    \"\"\"\n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    for old_label, new_label in selected_labels.items():\n",
    "        label_df = df[df['Label'] == old_label].copy()\n",
    "        label_df['Label'] = new_label\n",
    "        new_df = pd.concat([new_df, label_df])\n",
    "\n",
    "    unique_labels = df['Label'].unique()\n",
    "    remaining_labels = [label for label in unique_labels if label not in selected_labels]\n",
    "\n",
    "    for label in remaining_labels:\n",
    "        label_df = df[df['Label'] == label]\n",
    "        if len(label_df) > sample_size:\n",
    "            selected_rows = label_df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            selected_rows = label_df\n",
    "        selected_rows['Label'] = unknown_label\n",
    "        new_df = pd.concat([new_df, selected_rows])\n",
    "\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = readTrafficSignsTrain(train_path)\n",
    "df_train = pd.DataFrame()\n",
    "df_train = df_train_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_raw = readTrafficSignsTest(test_path)\n",
    "df_test = pd.DataFrame()\n",
    "df_test = df_test_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_train = reassign_labels(df_train, id_mapping, unknown_label, SAMPLE_SIZE_TRAIN)\n",
    "df_final_test = reassign_labels(df_test, id_mapping, unknown_label, SAMPLE_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Verfify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "short_classes = { 1: 'Geschwindigkeitsbegrenzung (30km/h)', \n",
    "                  2: 'Geschwindigkeitsbegrenzung (50km/h)', \n",
    "                  3: 'Vorrangstraße', \n",
    "                  4: 'Stop', \n",
    "                  5: 'Einfahrt verboten',\n",
    "                  6: 'Unbekannt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_train['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_to_display = df_final_test['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_label_df = df_final_train[df_final_train['Label'] == 6]\n",
    "\n",
    "# Wähle zufällig 100 Bilder aus\n",
    "image_paths_to_display = unknown_label_df['Path'].tolist()\n",
    "image_paths_to_display = random.sample(image_paths_to_display, min(len(image_paths_to_display), 100))\n",
    "\n",
    "# Plotten der ausgewählten Bilder\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image_path in enumerate(image_paths_to_display):\n",
    "    image_np = load_image_into_numpy_array(image_path)  # Funktion, um Bild in ein numpy array zu laden\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_final = df_final_train['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_final)\n",
    "\n",
    "label_counts_named_final = {short_classes[key]: value for key, value in label_counts_final.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_final.keys()), y=list(label_counts_named_final.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen für bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_test = df_final_test['Label'].value_counts().sort_index()\n",
    "#label_counts_train = df_train_short['Label'].value_counts().sort_index()\n",
    "\n",
    "num_classes = len(label_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_train = {short_classes[key]: value for key, value in label_counts_train.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_train.keys()), y=list(label_counts_named_train.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Trainingsdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen für bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_named_test = {short_classes[key]: value for key, value in label_counts_test.items()}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_counts_named_test.keys()), y=list(label_counts_named_test.values()), palette='viridis')\n",
    "plt.xlabel('Verkehrsschilder')\n",
    "plt.ylabel('Anzahl der Bilder')\n",
    "plt.title('Verteilung der Verkehrsschilder im Testdatensatz')\n",
    "plt.xticks(rotation=45, ha='right')  # Optional: Rotiere die Beschriftungen für bessere Lesbarkeit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create Tf-records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(row):\n",
    "    img_path = row['Path']\n",
    "    # Lade das Bild und konvertiere es in ein kompatibles Format (z.B. JPEG)\n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    with io.BytesIO() as output:\n",
    "        image.save(output, format=\"JPEG\")\n",
    "        encoded_jpg = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = row['Path'].encode('utf8')\n",
    "    image_format = b'jpeg'  # Ändere dies entsprechend des konvertierten Bildformats\n",
    "    xmins = [row['Roi.X1'] / width]\n",
    "    xmaxs = [row['Roi.X2'] / width]\n",
    "    ymins = [row['Roi.Y1'] / height]\n",
    "    ymaxs = [row['Roi.Y2'] / height]\n",
    "    classes_text = [str(row['Label']).encode('utf8')]\n",
    "    classes = [int(row['Label'])]\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_jpg])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n",
    "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n",
    "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n",
    "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n",
    "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n",
    "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(df, output_path):\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    for _, row in df.iterrows():\n",
    "        tf_example = create_tf_example(row)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tf_record(df_final_train, './myModules/records/train.record')\n",
    "create_tf_record(df_final_test, './myModules/records/test.record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Verify Tensord records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_example(serialized_example):\n",
    "    feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/object/class/text': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(record_file):\n",
    "    raw_dataset = tf.data.TFRecordDataset(record_file)\n",
    "    parsed_dataset = raw_dataset.map(parse_tf_example)\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_at_index(parsed_dataset, index):\n",
    "    for i, tf_example in enumerate(parsed_dataset):\n",
    "        if i == index:\n",
    "            height = tf_example['image/height'].numpy()\n",
    "            width = tf_example['image/width'].numpy()\n",
    "            encoded_image = tf_example['image/encoded'].numpy()\n",
    "            xmin = tf_example['image/object/bbox/xmin'].numpy()\n",
    "            xmax = tf_example['image/object/bbox/xmax'].numpy()\n",
    "            ymin = tf_example['image/object/bbox/ymin'].numpy()\n",
    "            ymax = tf_example['image/object/bbox/ymax'].numpy()\n",
    "            label = tf_example['image/object/class/label'].numpy()\n",
    "\n",
    "            image = tf.image.decode_jpeg(encoded_image)\n",
    "            image_np = image.numpy()\n",
    "\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(image_np)\n",
    "            plt.title(f'Label: {label}')\n",
    "\n",
    "            # Draw the bounding box\n",
    "            plt.gca().add_patch(plt.Rectangle((xmin * width, ymin * height), \n",
    "                                              (xmax - xmin) * width, (ymax - ymin) * height,\n",
    "                                              edgecolor='green', facecolor='none', linewidth=2))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record = read_tfrecord('./myModules/records/train.record')\n",
    "test_record = read_tfrecord('./myModules/records/test.record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image_at_index(test_record, 888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image_at_index(train_record, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"ssd_resnet101_v1_fpn_640x640_coco17_tpu-8\"\n",
    "MODEL_NAME = \"ssd_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model {\n",
    "  ssd {\n",
    "    num_classes: 6\n",
    "    image_resizer {\n",
    "      fixed_shape_resizer {\n",
    "        height: 640\n",
    "        width: 640\n",
    "      }\n",
    "    }\n",
    "    feature_extractor {\n",
    "      type: \"ssd_resnet101_v1_fpn_keras\"\n",
    "      depth_multiplier: 1.0\n",
    "      min_depth: 16\n",
    "      conv_hyperparams {\n",
    "        regularizer {\n",
    "          l2_regularizer {\n",
    "            weight: 0.0004\n",
    "          }\n",
    "        }\n",
    "        initializer {\n",
    "          truncated_normal_initializer {\n",
    "            mean: 0.0\n",
    "            stddev: 0.03\n",
    "          }\n",
    "        }\n",
    "        activation: RELU_6\n",
    "        batch_norm {\n",
    "          decay: 0.997\n",
    "          scale: true\n",
    "          epsilon: 0.001\n",
    "        }\n",
    "      }\n",
    "      override_base_feature_extractor_hyperparams: true\n",
    "      fpn {\n",
    "        min_level: 3\n",
    "        max_level: 7\n",
    "      }\n",
    "    }\n",
    "    box_coder {\n",
    "      faster_rcnn_box_coder {\n",
    "        y_scale: 10.0\n",
    "        x_scale: 10.0\n",
    "        height_scale: 5.0\n",
    "        width_scale: 5.0\n",
    "      }\n",
    "    }\n",
    "    matcher {\n",
    "      argmax_matcher {\n",
    "        matched_threshold: 0.5\n",
    "        unmatched_threshold: 0.5\n",
    "        ignore_thresholds: false\n",
    "        negatives_lower_than_unmatched: true\n",
    "        force_match_for_each_row: true\n",
    "        use_matmul_gather: true\n",
    "      }\n",
    "    }\n",
    "    similarity_calculator {\n",
    "      iou_similarity {\n",
    "      }\n",
    "    }\n",
    "    box_predictor {\n",
    "      weight_shared_convolutional_box_predictor {\n",
    "        conv_hyperparams {\n",
    "          regularizer {\n",
    "            l2_regularizer {\n",
    "              weight: 0.0004\n",
    "            }\n",
    "          }\n",
    "          initializer {\n",
    "            random_normal_initializer {\n",
    "              mean: 0.0\n",
    "              stddev: 0.01\n",
    "            }\n",
    "          }\n",
    "          activation: RELU_6\n",
    "          batch_norm {\n",
    "            decay: 0.997\n",
    "            scale: true\n",
    "            epsilon: 0.001\n",
    "          }\n",
    "        }\n",
    "        depth: 256\n",
    "        num_layers_before_predictor: 4\n",
    "        kernel_size: 3\n",
    "        class_prediction_bias_init: -4.6\n",
    "      }\n",
    "    }\n",
    "    anchor_generator {\n",
    "      multiscale_anchor_generator {\n",
    "        min_level: 3\n",
    "        max_level: 7\n",
    "        anchor_scale: 4.0\n",
    "        aspect_ratios: 1.0\n",
    "        aspect_ratios: 2.0\n",
    "        aspect_ratios: 0.5\n",
    "        scales_per_octave: 2\n",
    "      }\n",
    "    }\n",
    "    post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 1e-8\n",
    "        iou_threshold: 0.6\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 100\n",
    "        use_static_shapes: false\n",
    "      }\n",
    "      score_converter: SIGMOID\n",
    "    }\n",
    "    normalize_loss_by_num_matches: true\n",
    "    loss {\n",
    "      localization_loss {\n",
    "        weighted_smooth_l1 {\n",
    "        }\n",
    "      }\n",
    "      classification_loss {\n",
    "        weighted_sigmoid_focal {\n",
    "          gamma: 2.0\n",
    "          alpha: 0.25\n",
    "        }\n",
    "      }\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }\n",
    "    encode_background_as_zeros: true\n",
    "    normalize_loc_loss_by_codesize: true\n",
    "    inplace_batchnorm_update: true\n",
    "    freeze_batchnorm: false\n",
    "  }\n",
    "}\n",
    "train_config {\n",
    "  batch_size: 4\n",
    "  data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_adjust_brightness {\n",
    "      max_delta: 0.2\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_adjust_contrast {\n",
    "      min_delta: 0.8\n",
    "      max_delta: 1.2\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_rotation90 {\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_crop_image {\n",
    "      min_object_covered: 0.0\n",
    "      min_aspect_ratio: 0.75\n",
    "      max_aspect_ratio: 1.33\n",
    "      min_area: 0.75\n",
    "      max_area: 1.0\n",
    "      overlap_thresh: 0.0\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_jpeg_quality {\n",
    "      min_jpeg_quality: 50\n",
    "      max_jpeg_quality: 100\n",
    "    }\n",
    "  }\n",
    "  sync_replicas: true\n",
    "  optimizer {\n",
    "    momentum_optimizer {\n",
    "      learning_rate {\n",
    "        cosine_decay_learning_rate {\n",
    "          learning_rate_base: 0.01\n",
    "          total_steps: 25000\n",
    "          warmup_learning_rate: 0.003333\n",
    "          warmup_steps: 2000\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.9\n",
    "    }\n",
    "    use_moving_average: false\n",
    "  }\n",
    "  fine_tune_checkpoint: \"./base_models/ssd/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
    "  num_steps: 8000\n",
    "  startup_delay_steps: 0.0\n",
    "  replicas_to_aggregate: 8\n",
    "  max_number_of_boxes: 100\n",
    "  unpad_groundtruth_tensors: false\n",
    "  fine_tune_checkpoint_type: \"detection\"\n",
    "  use_bfloat16: false\n",
    "  fine_tune_checkpoint_version: V2\n",
    "}\n",
    "train_input_reader {\n",
    "  label_map_path: \"./myModules/label_map_short.pbtxt\"\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"./myModules/records/train.record\"\n",
    "  }\n",
    "}\n",
    "eval_config {\n",
    "  metrics_set: \"coco_detection_metrics\"\n",
    "  use_moving_averages: false\n",
    "}\n",
    "eval_input_reader {\n",
    "  label_map_path: \"./myModules/label_map_short.pbtxt\"\n",
    "  shuffle: false\n",
    "  num_epochs: 1\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"./myModules/records/train.record\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "NUM_CLASSES = 6\n",
    "NUM_STEPS = 8000\n",
    "NUM_EVAL_STEPS = 1000\n",
    "use_bfloat16 = False # Use bfloat16 True for TPU\n",
    "\n",
    "NOTES = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_length(df):\n",
    "    count_images = len(df)\n",
    "    labels_count = df['Label'].value_counts()\n",
    "\n",
    "    sorted_labels_count = labels_count.sort_index()\n",
    "    return sorted_labels_count.tolist(), count_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_info(batch, classes, steps, eval_steps, name, bfloat16, pipeline, \n",
    "                   checkpoint_path, labelmap, train_record, test_record, config, \n",
    "                   trained_model, train_time, df_train, df_test, notes):\n",
    "    \n",
    "    \n",
    "    test_count, test_len = count_length(df_test)\n",
    "    train_count, train_len = count_length(df_train)   \n",
    "    log_contents = f\"\"\"\n",
    "Trainingparameters:\n",
    "    BATCH_SIZE = {batch} # Cannot be higher than 2-4 (lack of ressources)\n",
    "    NUM_CLASSES = {classes} # Total number of classes to train is 43 - used for training are only 6-7\n",
    "    NUM_STEPS = {steps}\n",
    "    NUM_EVAL_STEPS = {eval_steps}\n",
    "    MODEL_NAME = {name}\n",
    "    use_bfloat16 = {bfloat16} # Use bfloat16 = True for trainign with TPU\n",
    "    \n",
    "Locations: \n",
    "    Path to Pipeline-Config = {pipeline}\n",
    "    Checkpoint from transfermodel  = {checkpoint_path}\n",
    "\n",
    "    Path to the trainedmodel = {trained_model}\n",
    "    Config of the trainedmodel = {config}\n",
    "    \n",
    "Used records: \n",
    "    Used labelmap = {labelmap}\n",
    "    Used train_record = {train_record}\n",
    "    Used test_record = {test_record}\n",
    "\n",
    "Generell Information: \n",
    "    Time needed for modeltraining = {train_time}\n",
    "    Length of traindataset = {train_len}\n",
    "    Counted values for each Label form Traindataset = {train_count}\n",
    "    Length of testdataset = {test_len}\n",
    "    Counted values for each Label form Testdataset = {test_count}\n",
    "\"\"\"\n",
    "\n",
    "    log_id = uuid.uuid4()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    log_entry = f\"ID: {log_id}\\nTimestamp: {timestamp}\\n\\n{log_contents}\\nNotes: {notes}\\n\"\n",
    "\n",
    "    filename = f\"./myModules/log/{name}_{log_id}.txt\"\n",
    "    with open(filename, \"a\") as file:\n",
    "        file.write(log_entry)\n",
    "        file.write(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from path \n",
    "pipeline_config = \"./base_models/ssd/{name}/pipeline.config\".format(name=BASE_MODEL)\n",
    "fine_tune_checkpoint_path = \"./base_models/ssd/{name}/checkpoint/ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "# Save to path\n",
    "model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\ssd\\\\{name}'.format(name=MODEL_NAME)\n",
    "pipeline_config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "# Upload from path\n",
    "short_labelmap_path = \"./myModules/label_map_short.pbtxt\"\n",
    "short_train_record_path = \"./myModules/records/train.record\"\n",
    "short_test_record_path = \"./myModules/records/test.record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pipeline_config) as f:\n",
    "    config = f.read()\n",
    "\n",
    "with open(pipeline_config_path, 'w') as f:\n",
    "  \n",
    "  # Set labelmap path\n",
    "  config = re.sub('label_map_path: \".*?\"', \n",
    "             'label_map_path: \"{}\"'.format(short_labelmap_path), config)\n",
    "  \n",
    "  # Set fine_tune_checkpoint path\n",
    "  config = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                  'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint_path), config)\n",
    "  \n",
    "  # Set train tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(short_train_record_path), config)\n",
    "  \n",
    "  # Set test tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(short_test_record_path), config)\n",
    "  \n",
    "  # Set number of classes.\n",
    "  config = re.sub('num_classes: [0-9]+',\n",
    "                  'num_classes: {}'.format(NUM_CLASSES), config)\n",
    "  \n",
    "  # Set batch size\n",
    "  config = re.sub('batch_size: [0-9]+',\n",
    "                  'batch_size: {}'.format(BATCH_SIZE), config)\n",
    "  \n",
    "  # Set training steps\n",
    "  config = re.sub('num_steps: [0-9]+',\n",
    "                  'num_steps: {}'.format(NUM_STEPS), config)\n",
    "  \n",
    "    # Set use_bfloat16\n",
    "  config = re.sub('use_bfloat16: (true|false)',\n",
    "                  'use_bfloat16: {}'.format(str(use_bfloat16).lower()), config)\n",
    "  \n",
    "  # Set fine-tune checkpoint type to detection\n",
    "  config = re.sub('fine_tune_checkpoint_type: \"classification\"', \n",
    "             'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n",
    "  \n",
    "  f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model {\n",
      "  ssd {\n",
      "    num_classes: 6\n",
      "    image_resizer {\n",
      "      fixed_shape_resizer {\n",
      "        height: 640\n",
      "        width: 640\n",
      "      }\n",
      "    }\n",
      "    feature_extractor {\n",
      "      type: \"ssd_resnet101_v1_fpn_keras\"\n",
      "      depth_multiplier: 1.0\n",
      "      min_depth: 16\n",
      "      conv_hyperparams {\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 0.0004\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          truncated_normal_initializer {\n",
      "            mean: 0.0\n",
      "            stddev: 0.03\n",
      "          }\n",
      "        }\n",
      "        activation: RELU_6\n",
      "        batch_norm {\n",
      "          decay: 0.997\n",
      "          scale: true\n",
      "          epsilon: 0.001\n",
      "        }\n",
      "      }\n",
      "      override_base_feature_extractor_hyperparams: true\n",
      "      fpn {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "      }\n",
      "    }\n",
      "    box_coder {\n",
      "      faster_rcnn_box_coder {\n",
      "        y_scale: 10.0\n",
      "        x_scale: 10.0\n",
      "        height_scale: 5.0\n",
      "        width_scale: 5.0\n",
      "      }\n",
      "    }\n",
      "    matcher {\n",
      "      argmax_matcher {\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.5\n",
      "        ignore_thresholds: false\n",
      "        negatives_lower_than_unmatched: true\n",
      "        force_match_for_each_row: true\n",
      "        use_matmul_gather: true\n",
      "      }\n",
      "    }\n",
      "    similarity_calculator {\n",
      "      iou_similarity {\n",
      "      }\n",
      "    }\n",
      "    box_predictor {\n",
      "      weight_shared_convolutional_box_predictor {\n",
      "        conv_hyperparams {\n",
      "          regularizer {\n",
      "            l2_regularizer {\n",
      "              weight: 0.0004\n",
      "            }\n",
      "          }\n",
      "          initializer {\n",
      "            random_normal_initializer {\n",
      "              mean: 0.0\n",
      "              stddev: 0.01\n",
      "            }\n",
      "          }\n",
      "          activation: RELU_6\n",
      "          batch_norm {\n",
      "            decay: 0.997\n",
      "            scale: true\n",
      "            epsilon: 0.001\n",
      "          }\n",
      "        }\n",
      "        depth: 256\n",
      "        num_layers_before_predictor: 4\n",
      "        kernel_size: 3\n",
      "        class_prediction_bias_init: -4.6\n",
      "      }\n",
      "    }\n",
      "    anchor_generator {\n",
      "      multiscale_anchor_generator {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "        anchor_scale: 4.0\n",
      "        aspect_ratios: 1.0\n",
      "        aspect_ratios: 2.0\n",
      "        aspect_ratios: 0.5\n",
      "        scales_per_octave: 2\n",
      "      }\n",
      "    }\n",
      "    post_processing {\n",
      "      batch_non_max_suppression {\n",
      "        score_threshold: 1e-8\n",
      "        iou_threshold: 0.6\n",
      "        max_detections_per_class: 100\n",
      "        max_total_detections: 100\n",
      "        use_static_shapes: false\n",
      "      }\n",
      "      score_converter: SIGMOID\n",
      "    }\n",
      "    normalize_loss_by_num_matches: true\n",
      "    loss {\n",
      "      localization_loss {\n",
      "        weighted_smooth_l1 {\n",
      "        }\n",
      "      }\n",
      "      classification_loss {\n",
      "        weighted_sigmoid_focal {\n",
      "          gamma: 2.0\n",
      "          alpha: 0.25\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 1.0\n",
      "    }\n",
      "    encode_background_as_zeros: true\n",
      "    normalize_loc_loss_by_codesize: true\n",
      "    inplace_batchnorm_update: true\n",
      "    freeze_batchnorm: false\n",
      "  }\n",
      "}\n",
      "train_config {\n",
      "  batch_size: 2\n",
      "  sync_replicas: true\n",
      "  optimizer {\n",
      "    momentum_optimizer {\n",
      "      learning_rate {\n",
      "        cosine_decay_learning_rate {\n",
      "          learning_rate_base: 0.01\n",
      "          total_steps: 25000\n",
      "          warmup_learning_rate: 0.003333\n",
      "          warmup_steps: 2000\n",
      "        }\n",
      "      }\n",
      "      momentum_optimizer_value: 0.9\n",
      "    }\n",
      "    use_moving_average: false\n",
      "  }\n",
      "  fine_tune_checkpoint: \"./base_models/ssd/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
      "  num_steps: 8000\n",
      "  startup_delay_steps: 0.0\n",
      "  replicas_to_aggregate: 8\n",
      "  max_number_of_boxes: 100\n",
      "  unpad_groundtruth_tensors: false\n",
      "  fine_tune_checkpoint_type: \"detection\"\n",
      "  use_bfloat16: false\n",
      "  fine_tune_checkpoint_version: V2\n",
      "}\n",
      "train_input_reader {\n",
      "  label_map_path: \"./myModules/label_map_short.pbtxt\"\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/train.record\"\n",
      "  }\n",
      "}\n",
      "eval_config {\n",
      "  metrics_set: \"coco_detection_metrics\"\n",
      "  use_moving_averages: false\n",
      "}\n",
      "eval_input_reader {\n",
      "  label_map_path: \"./myModules/label_map_short.pbtxt\"\n",
      "  shuffle: false\n",
      "  num_epochs: 1\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/train.record\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(pipeline_config_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from path \n",
    "pipeline_config = \"./base_models/faster_rcnn/{name}/pipeline.config\".format(name=BASE_MODEL)\n",
    "fine_tune_checkpoint_path = \"./base_models/faster_rcnn/{name}/checkpoint/ckpt-0\".format(name=BASE_MODEL)\n",
    "\n",
    "# Save to path\n",
    "model_dir = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\{name}'.format(name=MODEL_NAME)\n",
    "pipeline_config_path = './myModules/configs/{name}_config.config'.format(name=MODEL_NAME)\n",
    "\n",
    "# Upload from path\n",
    "short_labelmap_path = \"./myModules/label_map_short.pbtxt\"\n",
    "short_train_record_path = \"./myModules/records/train.record\"\n",
    "short_test_record_path = \"./myModules/records/test.record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pipeline_config) as f:\n",
    "    config = f.read()\n",
    "\n",
    "with open(pipeline_config_path, 'w') as f:\n",
    "  \n",
    "  # Set labelmap path\n",
    "  config = re.sub('label_map_path: \".*?\"', \n",
    "             'label_map_path: \"{}\"'.format(short_labelmap_path), config)\n",
    "  \n",
    "  # Set fine_tune_checkpoint path\n",
    "  config = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                  'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint_path), config)\n",
    "  \n",
    "  # Set train tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(short_train_record_path), config)\n",
    "  \n",
    "  # Set test tf-record file path\n",
    "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', \n",
    "                  'input_path: \"{}\"'.format(short_test_record_path), config)\n",
    "  \n",
    "  # Set number of classes.\n",
    "  config = re.sub('num_classes: [0-9]+',\n",
    "                  'num_classes: {}'.format(NUM_CLASSES), config)\n",
    "  \n",
    "  # Set batch size\n",
    "  config = re.sub('batch_size: [0-9]+',\n",
    "                  'batch_size: {}'.format(BATCH_SIZE), config)\n",
    "  \n",
    "  # Set training steps\n",
    "  config = re.sub('num_steps: [0-9]+',\n",
    "                  'num_steps: {}'.format(NUM_STEPS), config)\n",
    "  \n",
    "    # Set use_bfloat16\n",
    "  config = re.sub('use_bfloat16: (true|false)',\n",
    "                  'use_bfloat16: {}'.format(str(use_bfloat16).lower()), config)\n",
    "  \n",
    "  # Set fine-tune checkpoint type to detection\n",
    "  config = re.sub('fine_tune_checkpoint_type: \"classification\"', \n",
    "             'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n",
    "  \n",
    "  f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model {\n",
      "  ssd {\n",
      "    num_classes: 6\n",
      "    image_resizer {\n",
      "      fixed_shape_resizer {\n",
      "        height: 640\n",
      "        width: 640\n",
      "      }\n",
      "    }\n",
      "    feature_extractor {\n",
      "      type: \"ssd_resnet101_v1_fpn_keras\"\n",
      "      depth_multiplier: 1.0\n",
      "      min_depth: 16\n",
      "      conv_hyperparams {\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 0.0004\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          truncated_normal_initializer {\n",
      "            mean: 0.0\n",
      "            stddev: 0.03\n",
      "          }\n",
      "        }\n",
      "        activation: RELU_6\n",
      "        batch_norm {\n",
      "          decay: 0.997\n",
      "          scale: true\n",
      "          epsilon: 0.001\n",
      "        }\n",
      "      }\n",
      "      override_base_feature_extractor_hyperparams: true\n",
      "      fpn {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "      }\n",
      "    }\n",
      "    box_coder {\n",
      "      faster_rcnn_box_coder {\n",
      "        y_scale: 10.0\n",
      "        x_scale: 10.0\n",
      "        height_scale: 5.0\n",
      "        width_scale: 5.0\n",
      "      }\n",
      "    }\n",
      "    matcher {\n",
      "      argmax_matcher {\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.5\n",
      "        ignore_thresholds: false\n",
      "        negatives_lower_than_unmatched: true\n",
      "        force_match_for_each_row: true\n",
      "        use_matmul_gather: true\n",
      "      }\n",
      "    }\n",
      "    similarity_calculator {\n",
      "      iou_similarity {\n",
      "      }\n",
      "    }\n",
      "    box_predictor {\n",
      "      weight_shared_convolutional_box_predictor {\n",
      "        conv_hyperparams {\n",
      "          regularizer {\n",
      "            l2_regularizer {\n",
      "              weight: 0.0004\n",
      "            }\n",
      "          }\n",
      "          initializer {\n",
      "            random_normal_initializer {\n",
      "              mean: 0.0\n",
      "              stddev: 0.01\n",
      "            }\n",
      "          }\n",
      "          activation: RELU_6\n",
      "          batch_norm {\n",
      "            decay: 0.997\n",
      "            scale: true\n",
      "            epsilon: 0.001\n",
      "          }\n",
      "        }\n",
      "        depth: 256\n",
      "        num_layers_before_predictor: 4\n",
      "        kernel_size: 3\n",
      "        class_prediction_bias_init: -4.6\n",
      "      }\n",
      "    }\n",
      "    anchor_generator {\n",
      "      multiscale_anchor_generator {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "        anchor_scale: 4.0\n",
      "        aspect_ratios: 1.0\n",
      "        aspect_ratios: 2.0\n",
      "        aspect_ratios: 0.5\n",
      "        scales_per_octave: 2\n",
      "      }\n",
      "    }\n",
      "    post_processing {\n",
      "      batch_non_max_suppression {\n",
      "        score_threshold: 1e-8\n",
      "        iou_threshold: 0.6\n",
      "        max_detections_per_class: 100\n",
      "        max_total_detections: 100\n",
      "        use_static_shapes: false\n",
      "      }\n",
      "      score_converter: SIGMOID\n",
      "    }\n",
      "    normalize_loss_by_num_matches: true\n",
      "    loss {\n",
      "      localization_loss {\n",
      "        weighted_smooth_l1 {\n",
      "        }\n",
      "      }\n",
      "      classification_loss {\n",
      "        weighted_sigmoid_focal {\n",
      "          gamma: 2.0\n",
      "          alpha: 0.25\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 1.0\n",
      "    }\n",
      "    encode_background_as_zeros: true\n",
      "    normalize_loc_loss_by_codesize: true\n",
      "    inplace_batchnorm_update: true\n",
      "    freeze_batchnorm: false\n",
      "  }\n",
      "}\n",
      "train_config {\n",
      "  batch_size: 2\n",
      "  sync_replicas: true\n",
      "  optimizer {\n",
      "    momentum_optimizer {\n",
      "      learning_rate {\n",
      "        cosine_decay_learning_rate {\n",
      "          learning_rate_base: 0.01\n",
      "          total_steps: 25000\n",
      "          warmup_learning_rate: 0.003333\n",
      "          warmup_steps: 2000\n",
      "        }\n",
      "      }\n",
      "      momentum_optimizer_value: 0.9\n",
      "    }\n",
      "    use_moving_average: false\n",
      "  }\n",
      "  fine_tune_checkpoint: \"./base_models/ssd/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
      "  num_steps: 8000\n",
      "  startup_delay_steps: 0.0\n",
      "  replicas_to_aggregate: 8\n",
      "  max_number_of_boxes: 100\n",
      "  unpad_groundtruth_tensors: false\n",
      "  fine_tune_checkpoint_type: \"detection\"\n",
      "  use_bfloat16: false\n",
      "  fine_tune_checkpoint_version: V2\n",
      "}\n",
      "train_input_reader {\n",
      "  label_map_path: \"./myModules/label_map_short.pbtxt\"\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/train.record\"\n",
      "  }\n",
      "}\n",
      "eval_config {\n",
      "  metrics_set: \"coco_detection_metrics\"\n",
      "  use_moving_averages: false\n",
      "}\n",
      "eval_input_reader {\n",
      "  label_map_path: \"./myModules/label_map_short.pbtxt\"\n",
      "  shuffle: false\n",
      "  num_epochs: 1\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"./myModules/records/train.record\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(pipeline_config_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "2024-07-16 14:53:21.653747: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-16 14:53:22.396282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6304 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "I0716 14:53:27.985013 14044 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: 8000\n",
      "I0716 14:53:27.995011 14044 config_util.py:552] Maybe overwriting train_steps: 8000\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0716 14:53:27.995011 14044 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W0716 14:53:28.041012 14044 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['./myModules/records/train.record']\n",
      "I0716 14:53:28.056015 14044 dataset_builder.py:162] Reading unweighted datasets: ['./myModules/records/train.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['./myModules/records/train.record']\n",
      "I0716 14:53:28.057016 14044 dataset_builder.py:79] Reading record datasets for input file: ['./myModules/records/train.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I0716 14:53:28.058017 14044 dataset_builder.py:80] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0716 14:53:28.058017 14044 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W0716 14:53:28.068525 14044 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0716 14:53:28.090529 14044 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W0716 14:53:57.294700 14044 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W0716 14:54:00.528099 14044 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0716 14:54:02.269770 14044 deprecation.py:350] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n",
      "I0716 14:54:11.607876  6528 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0716 14:54:20.666355 13960 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-16 14:54:25.418124: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 20460000 exceeds 10% of free system memory.\n",
      "2024-07-16 14:54:26.036432: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 20460000 exceeds 10% of free system memory.\n",
      "2024-07-16 14:54:26.043075: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 20460000 exceeds 10% of free system memory.\n",
      "2024-07-16 14:54:26.069056: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 20460000 exceeds 10% of free system memory.\n",
      "2024-07-16 14:54:26.074261: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 20460000 exceeds 10% of free system memory.\n",
      "2024-07-16 14:54:27.477401: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0716 14:54:35.609705 14044 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0716 14:54:35.612741 14044 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0716 14:54:35.614771 14044 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0716 14:54:35.615741 14044 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0716 14:54:35.618772 14044 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0716 14:54:35.619770 14044 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0716 14:54:35.623743 14044 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0716 14:54:35.624742 14044 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0716 14:54:35.627346 14044 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0716 14:54:35.627346 14044 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0716 14:54:49.758385  2272 deprecation.py:554] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "I0716 14:54:52.321123  2272 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0716 14:54:59.530574  5408 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0716 14:55:06.399001  2012 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0716 14:55:13.347335 12140 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-07-16 14:55:34.886479: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-16 14:55:34.893230: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "INFO:tensorflow:Step 100 per-step time 0.855s\n",
      "I0716 14:56:14.150318 14044 model_lib_v2.py:705] Step 100 per-step time 0.855s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.12294626,\n",
      " 'Loss/localization_loss': 0.05353504,\n",
      " 'Loss/regularization_loss': 0.28768575,\n",
      " 'Loss/total_loss': 0.46416706,\n",
      " 'learning_rate': 0.00366635}\n",
      "I0716 14:56:14.169963 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.12294626,\n",
      " 'Loss/localization_loss': 0.05353504,\n",
      " 'Loss/regularization_loss': 0.28768575,\n",
      " 'Loss/total_loss': 0.46416706,\n",
      " 'learning_rate': 0.00366635}\n",
      "INFO:tensorflow:Step 200 per-step time 0.416s\n",
      "I0716 14:56:55.650521 14044 model_lib_v2.py:705] Step 200 per-step time 0.416s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.18624371,\n",
      " 'Loss/localization_loss': 0.052656263,\n",
      " 'Loss/regularization_loss': 0.2875937,\n",
      " 'Loss/total_loss': 0.52649367,\n",
      " 'learning_rate': 0.0039997}\n",
      "I0716 14:56:55.651522 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.18624371,\n",
      " 'Loss/localization_loss': 0.052656263,\n",
      " 'Loss/regularization_loss': 0.2875937,\n",
      " 'Loss/total_loss': 0.52649367,\n",
      " 'learning_rate': 0.0039997}\n",
      "INFO:tensorflow:Step 300 per-step time 0.411s\n",
      "I0716 14:57:36.741063 14044 model_lib_v2.py:705] Step 300 per-step time 0.411s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.16067904,\n",
      " 'Loss/localization_loss': 0.03822358,\n",
      " 'Loss/regularization_loss': 0.2871178,\n",
      " 'Loss/total_loss': 0.48602045,\n",
      " 'learning_rate': 0.00433305}\n",
      "I0716 14:57:36.741063 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.16067904,\n",
      " 'Loss/localization_loss': 0.03822358,\n",
      " 'Loss/regularization_loss': 0.2871178,\n",
      " 'Loss/total_loss': 0.48602045,\n",
      " 'learning_rate': 0.00433305}\n",
      "INFO:tensorflow:Step 400 per-step time 0.405s\n",
      "I0716 14:58:17.335170 14044 model_lib_v2.py:705] Step 400 per-step time 0.405s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.18354644,\n",
      " 'Loss/localization_loss': 0.04039882,\n",
      " 'Loss/regularization_loss': 0.28654227,\n",
      " 'Loss/total_loss': 0.51048756,\n",
      " 'learning_rate': 0.0046664}\n",
      "I0716 14:58:17.335170 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.18354644,\n",
      " 'Loss/localization_loss': 0.04039882,\n",
      " 'Loss/regularization_loss': 0.28654227,\n",
      " 'Loss/total_loss': 0.51048756,\n",
      " 'learning_rate': 0.0046664}\n",
      "INFO:tensorflow:Step 500 per-step time 0.413s\n",
      "I0716 14:58:58.882616 14044 model_lib_v2.py:705] Step 500 per-step time 0.413s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.28822422,\n",
      " 'Loss/localization_loss': 0.036590755,\n",
      " 'Loss/regularization_loss': 0.28585795,\n",
      " 'Loss/total_loss': 0.61067295,\n",
      " 'learning_rate': 0.00499975}\n",
      "I0716 14:58:58.883619 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.28822422,\n",
      " 'Loss/localization_loss': 0.036590755,\n",
      " 'Loss/regularization_loss': 0.28585795,\n",
      " 'Loss/total_loss': 0.61067295,\n",
      " 'learning_rate': 0.00499975}\n",
      "INFO:tensorflow:Step 600 per-step time 0.462s\n",
      "I0716 14:59:44.824244 14044 model_lib_v2.py:705] Step 600 per-step time 0.462s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.22486989,\n",
      " 'Loss/localization_loss': 0.044783734,\n",
      " 'Loss/regularization_loss': 0.28505632,\n",
      " 'Loss/total_loss': 0.5547099,\n",
      " 'learning_rate': 0.0053331}\n",
      "I0716 14:59:44.824244 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.22486989,\n",
      " 'Loss/localization_loss': 0.044783734,\n",
      " 'Loss/regularization_loss': 0.28505632,\n",
      " 'Loss/total_loss': 0.5547099,\n",
      " 'learning_rate': 0.0053331}\n",
      "INFO:tensorflow:Step 700 per-step time 0.442s\n",
      "I0716 15:00:29.030973 14044 model_lib_v2.py:705] Step 700 per-step time 0.442s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.24430844,\n",
      " 'Loss/localization_loss': 0.047691494,\n",
      " 'Loss/regularization_loss': 0.28419745,\n",
      " 'Loss/total_loss': 0.5761974,\n",
      " 'learning_rate': 0.0056664497}\n",
      "I0716 15:00:29.031973 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.24430844,\n",
      " 'Loss/localization_loss': 0.047691494,\n",
      " 'Loss/regularization_loss': 0.28419745,\n",
      " 'Loss/total_loss': 0.5761974,\n",
      " 'learning_rate': 0.0056664497}\n",
      "INFO:tensorflow:Step 800 per-step time 0.419s\n",
      "I0716 15:01:10.910639 14044 model_lib_v2.py:705] Step 800 per-step time 0.419s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2374441,\n",
      " 'Loss/localization_loss': 0.10309058,\n",
      " 'Loss/regularization_loss': 0.28323492,\n",
      " 'Loss/total_loss': 0.62376964,\n",
      " 'learning_rate': 0.0059998}\n",
      "I0716 15:01:10.910639 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.2374441,\n",
      " 'Loss/localization_loss': 0.10309058,\n",
      " 'Loss/regularization_loss': 0.28323492,\n",
      " 'Loss/total_loss': 0.62376964,\n",
      " 'learning_rate': 0.0059998}\n",
      "INFO:tensorflow:Step 900 per-step time 0.476s\n",
      "I0716 15:01:58.482184 14044 model_lib_v2.py:705] Step 900 per-step time 0.476s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.20143968,\n",
      " 'Loss/localization_loss': 0.01563494,\n",
      " 'Loss/regularization_loss': 0.28226113,\n",
      " 'Loss/total_loss': 0.49933577,\n",
      " 'learning_rate': 0.00633315}\n",
      "I0716 15:01:58.483576 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.20143968,\n",
      " 'Loss/localization_loss': 0.01563494,\n",
      " 'Loss/regularization_loss': 0.28226113,\n",
      " 'Loss/total_loss': 0.49933577,\n",
      " 'learning_rate': 0.00633315}\n",
      "INFO:tensorflow:Step 1000 per-step time 0.461s\n",
      "I0716 15:02:44.579293 14044 model_lib_v2.py:705] Step 1000 per-step time 0.461s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2839252,\n",
      " 'Loss/localization_loss': 0.021503936,\n",
      " 'Loss/regularization_loss': 0.28120193,\n",
      " 'Loss/total_loss': 0.58663106,\n",
      " 'learning_rate': 0.0066665}\n",
      "I0716 15:02:44.579293 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.2839252,\n",
      " 'Loss/localization_loss': 0.021503936,\n",
      " 'Loss/regularization_loss': 0.28120193,\n",
      " 'Loss/total_loss': 0.58663106,\n",
      " 'learning_rate': 0.0066665}\n",
      "INFO:tensorflow:Step 1100 per-step time 0.466s\n",
      "I0716 15:03:31.144354 14044 model_lib_v2.py:705] Step 1100 per-step time 0.466s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.21199387,\n",
      " 'Loss/localization_loss': 0.1063037,\n",
      " 'Loss/regularization_loss': 0.28005904,\n",
      " 'Loss/total_loss': 0.5983566,\n",
      " 'learning_rate': 0.0069998503}\n",
      "I0716 15:03:31.144354 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.21199387,\n",
      " 'Loss/localization_loss': 0.1063037,\n",
      " 'Loss/regularization_loss': 0.28005904,\n",
      " 'Loss/total_loss': 0.5983566,\n",
      " 'learning_rate': 0.0069998503}\n",
      "INFO:tensorflow:Step 1200 per-step time 0.418s\n",
      "I0716 15:04:13.027247 14044 model_lib_v2.py:705] Step 1200 per-step time 0.418s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.21969804,\n",
      " 'Loss/localization_loss': 0.05063954,\n",
      " 'Loss/regularization_loss': 0.27898458,\n",
      " 'Loss/total_loss': 0.5493221,\n",
      " 'learning_rate': 0.0073332}\n",
      "I0716 15:04:13.027247 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.21969804,\n",
      " 'Loss/localization_loss': 0.05063954,\n",
      " 'Loss/regularization_loss': 0.27898458,\n",
      " 'Loss/total_loss': 0.5493221,\n",
      " 'learning_rate': 0.0073332}\n",
      "INFO:tensorflow:Step 1300 per-step time 0.433s\n",
      "I0716 15:04:56.314371 14044 model_lib_v2.py:705] Step 1300 per-step time 0.433s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1980011,\n",
      " 'Loss/localization_loss': 0.027833726,\n",
      " 'Loss/regularization_loss': 0.27779615,\n",
      " 'Loss/total_loss': 0.503631,\n",
      " 'learning_rate': 0.00766655}\n",
      "I0716 15:04:56.314371 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.1980011,\n",
      " 'Loss/localization_loss': 0.027833726,\n",
      " 'Loss/regularization_loss': 0.27779615,\n",
      " 'Loss/total_loss': 0.503631,\n",
      " 'learning_rate': 0.00766655}\n",
      "INFO:tensorflow:Step 1400 per-step time 0.439s\n",
      "I0716 15:05:40.152962 14044 model_lib_v2.py:705] Step 1400 per-step time 0.439s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.24911441,\n",
      " 'Loss/localization_loss': 0.03175685,\n",
      " 'Loss/regularization_loss': 0.27658027,\n",
      " 'Loss/total_loss': 0.55745155,\n",
      " 'learning_rate': 0.0079999}\n",
      "I0716 15:05:40.152962 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.24911441,\n",
      " 'Loss/localization_loss': 0.03175685,\n",
      " 'Loss/regularization_loss': 0.27658027,\n",
      " 'Loss/total_loss': 0.55745155,\n",
      " 'learning_rate': 0.0079999}\n",
      "INFO:tensorflow:Step 1500 per-step time 0.421s\n",
      "I0716 15:06:22.269333 14044 model_lib_v2.py:705] Step 1500 per-step time 0.421s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.20660862,\n",
      " 'Loss/localization_loss': 0.022833014,\n",
      " 'Loss/regularization_loss': 0.27570432,\n",
      " 'Loss/total_loss': 0.50514597,\n",
      " 'learning_rate': 0.00833325}\n",
      "I0716 15:06:22.270337 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.20660862,\n",
      " 'Loss/localization_loss': 0.022833014,\n",
      " 'Loss/regularization_loss': 0.27570432,\n",
      " 'Loss/total_loss': 0.50514597,\n",
      " 'learning_rate': 0.00833325}\n",
      "INFO:tensorflow:Step 1600 per-step time 0.446s\n",
      "I0716 15:07:06.854769 14044 model_lib_v2.py:705] Step 1600 per-step time 0.446s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2467352,\n",
      " 'Loss/localization_loss': 0.056725744,\n",
      " 'Loss/regularization_loss': 0.2747712,\n",
      " 'Loss/total_loss': 0.57823217,\n",
      " 'learning_rate': 0.0086666}\n",
      "I0716 15:07:06.854769 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.2467352,\n",
      " 'Loss/localization_loss': 0.056725744,\n",
      " 'Loss/regularization_loss': 0.2747712,\n",
      " 'Loss/total_loss': 0.57823217,\n",
      " 'learning_rate': 0.0086666}\n",
      "INFO:tensorflow:Step 1700 per-step time 0.418s\n",
      "I0716 15:07:48.639265 14044 model_lib_v2.py:705] Step 1700 per-step time 0.418s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2794266,\n",
      " 'Loss/localization_loss': 0.03723353,\n",
      " 'Loss/regularization_loss': 0.27385813,\n",
      " 'Loss/total_loss': 0.59051824,\n",
      " 'learning_rate': 0.008999949}\n",
      "I0716 15:07:48.639265 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.2794266,\n",
      " 'Loss/localization_loss': 0.03723353,\n",
      " 'Loss/regularization_loss': 0.27385813,\n",
      " 'Loss/total_loss': 0.59051824,\n",
      " 'learning_rate': 0.008999949}\n",
      "INFO:tensorflow:Step 1800 per-step time 0.394s\n",
      "I0716 15:08:28.103928 14044 model_lib_v2.py:705] Step 1800 per-step time 0.394s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1662901,\n",
      " 'Loss/localization_loss': 0.06033039,\n",
      " 'Loss/regularization_loss': 0.27271712,\n",
      " 'Loss/total_loss': 0.4993376,\n",
      " 'learning_rate': 0.0093332995}\n",
      "I0716 15:08:28.104931 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.1662901,\n",
      " 'Loss/localization_loss': 0.06033039,\n",
      " 'Loss/regularization_loss': 0.27271712,\n",
      " 'Loss/total_loss': 0.4993376,\n",
      " 'learning_rate': 0.0093332995}\n",
      "INFO:tensorflow:Step 1900 per-step time 0.423s\n",
      "I0716 15:09:10.349487 14044 model_lib_v2.py:705] Step 1900 per-step time 0.423s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.18855359,\n",
      " 'Loss/localization_loss': 0.03544283,\n",
      " 'Loss/regularization_loss': 0.27174434,\n",
      " 'Loss/total_loss': 0.49574077,\n",
      " 'learning_rate': 0.00966665}\n",
      "I0716 15:09:10.349487 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.18855359,\n",
      " 'Loss/localization_loss': 0.03544283,\n",
      " 'Loss/regularization_loss': 0.27174434,\n",
      " 'Loss/total_loss': 0.49574077,\n",
      " 'learning_rate': 0.00966665}\n",
      "INFO:tensorflow:Step 2000 per-step time 0.402s\n",
      "I0716 15:09:50.586464 14044 model_lib_v2.py:705] Step 2000 per-step time 0.402s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.16586864,\n",
      " 'Loss/localization_loss': 0.024125487,\n",
      " 'Loss/regularization_loss': 0.27060822,\n",
      " 'Loss/total_loss': 0.46060234,\n",
      " 'learning_rate': 0.01}\n",
      "I0716 15:09:50.586464 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.16586864,\n",
      " 'Loss/localization_loss': 0.024125487,\n",
      " 'Loss/regularization_loss': 0.27060822,\n",
      " 'Loss/total_loss': 0.46060234,\n",
      " 'learning_rate': 0.01}\n",
      "INFO:tensorflow:Step 2100 per-step time 0.487s\n",
      "I0716 15:10:39.319288 14044 model_lib_v2.py:705] Step 2100 per-step time 0.487s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1980577,\n",
      " 'Loss/localization_loss': 0.023714056,\n",
      " 'Loss/regularization_loss': 0.26951513,\n",
      " 'Loss/total_loss': 0.49128687,\n",
      " 'learning_rate': 0.009999534}\n",
      "I0716 15:10:39.319288 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.1980577,\n",
      " 'Loss/localization_loss': 0.023714056,\n",
      " 'Loss/regularization_loss': 0.26951513,\n",
      " 'Loss/total_loss': 0.49128687,\n",
      " 'learning_rate': 0.009999534}\n",
      "INFO:tensorflow:Step 2200 per-step time 0.415s\n",
      "I0716 15:11:20.815084 14044 model_lib_v2.py:705] Step 2200 per-step time 0.415s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09980714,\n",
      " 'Loss/localization_loss': 0.015945066,\n",
      " 'Loss/regularization_loss': 0.2682511,\n",
      " 'Loss/total_loss': 0.38400328,\n",
      " 'learning_rate': 0.009998134}\n",
      "I0716 15:11:20.816090 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.09980714,\n",
      " 'Loss/localization_loss': 0.015945066,\n",
      " 'Loss/regularization_loss': 0.2682511,\n",
      " 'Loss/total_loss': 0.38400328,\n",
      " 'learning_rate': 0.009998134}\n",
      "INFO:tensorflow:Step 2300 per-step time 0.376s\n",
      "I0716 15:11:59.091125 14044 model_lib_v2.py:705] Step 2300 per-step time 0.376s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.12399317,\n",
      " 'Loss/localization_loss': 0.049637854,\n",
      " 'Loss/regularization_loss': 0.26687154,\n",
      " 'Loss/total_loss': 0.44050258,\n",
      " 'learning_rate': 0.009995802}\n",
      "I0716 15:11:59.091125 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.12399317,\n",
      " 'Loss/localization_loss': 0.049637854,\n",
      " 'Loss/regularization_loss': 0.26687154,\n",
      " 'Loss/total_loss': 0.44050258,\n",
      " 'learning_rate': 0.009995802}\n",
      "INFO:tensorflow:Step 2400 per-step time 0.396s\n",
      "I0716 15:12:38.315055 14044 model_lib_v2.py:705] Step 2400 per-step time 0.396s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.23581383,\n",
      " 'Loss/localization_loss': 0.049529262,\n",
      " 'Loss/regularization_loss': 0.26554608,\n",
      " 'Loss/total_loss': 0.55088913,\n",
      " 'learning_rate': 0.009992538}\n",
      "I0716 15:12:38.315055 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.23581383,\n",
      " 'Loss/localization_loss': 0.049529262,\n",
      " 'Loss/regularization_loss': 0.26554608,\n",
      " 'Loss/total_loss': 0.55088913,\n",
      " 'learning_rate': 0.009992538}\n",
      "INFO:tensorflow:Step 2500 per-step time 0.394s\n",
      "I0716 15:13:17.428780 14044 model_lib_v2.py:705] Step 2500 per-step time 0.394s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13208002,\n",
      " 'Loss/localization_loss': 0.033782914,\n",
      " 'Loss/regularization_loss': 0.26418743,\n",
      " 'Loss/total_loss': 0.43005037,\n",
      " 'learning_rate': 0.009988343}\n",
      "I0716 15:13:17.428780 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.13208002,\n",
      " 'Loss/localization_loss': 0.033782914,\n",
      " 'Loss/regularization_loss': 0.26418743,\n",
      " 'Loss/total_loss': 0.43005037,\n",
      " 'learning_rate': 0.009988343}\n",
      "INFO:tensorflow:Step 2600 per-step time 0.358s\n",
      "I0716 15:13:53.237723 14044 model_lib_v2.py:705] Step 2600 per-step time 0.358s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.3218994,\n",
      " 'Loss/localization_loss': 0.03605623,\n",
      " 'Loss/regularization_loss': 0.26278582,\n",
      " 'Loss/total_loss': 0.6207415,\n",
      " 'learning_rate': 0.009983217}\n",
      "I0716 15:13:53.237723 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.3218994,\n",
      " 'Loss/localization_loss': 0.03605623,\n",
      " 'Loss/regularization_loss': 0.26278582,\n",
      " 'Loss/total_loss': 0.6207415,\n",
      " 'learning_rate': 0.009983217}\n",
      "INFO:tensorflow:Step 2700 per-step time 0.397s\n",
      "I0716 15:14:33.010695 14044 model_lib_v2.py:705] Step 2700 per-step time 0.397s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13038172,\n",
      " 'Loss/localization_loss': 0.017854523,\n",
      " 'Loss/regularization_loss': 0.26150513,\n",
      " 'Loss/total_loss': 0.40974137,\n",
      " 'learning_rate': 0.009977162}\n",
      "I0716 15:14:33.010695 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.13038172,\n",
      " 'Loss/localization_loss': 0.017854523,\n",
      " 'Loss/regularization_loss': 0.26150513,\n",
      " 'Loss/total_loss': 0.40974137,\n",
      " 'learning_rate': 0.009977162}\n",
      "INFO:tensorflow:Step 2800 per-step time 0.398s\n",
      "I0716 15:15:12.653594 14044 model_lib_v2.py:705] Step 2800 per-step time 0.398s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.07948295,\n",
      " 'Loss/localization_loss': 0.010265268,\n",
      " 'Loss/regularization_loss': 0.26021865,\n",
      " 'Loss/total_loss': 0.34996688,\n",
      " 'learning_rate': 0.009970179}\n",
      "I0716 15:15:12.653594 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.07948295,\n",
      " 'Loss/localization_loss': 0.010265268,\n",
      " 'Loss/regularization_loss': 0.26021865,\n",
      " 'Loss/total_loss': 0.34996688,\n",
      " 'learning_rate': 0.009970179}\n",
      "INFO:tensorflow:Step 2900 per-step time 0.429s\n",
      "I0716 15:15:55.604283 14044 model_lib_v2.py:705] Step 2900 per-step time 0.429s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.07513424,\n",
      " 'Loss/localization_loss': 0.01510974,\n",
      " 'Loss/regularization_loss': 0.25910142,\n",
      " 'Loss/total_loss': 0.3493454,\n",
      " 'learning_rate': 0.009962266}\n",
      "I0716 15:15:55.604283 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.07513424,\n",
      " 'Loss/localization_loss': 0.01510974,\n",
      " 'Loss/regularization_loss': 0.25910142,\n",
      " 'Loss/total_loss': 0.3493454,\n",
      " 'learning_rate': 0.009962266}\n",
      "INFO:tensorflow:Step 3000 per-step time 0.388s\n",
      "I0716 15:16:34.368237 14044 model_lib_v2.py:705] Step 3000 per-step time 0.388s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.18163756,\n",
      " 'Loss/localization_loss': 0.071698286,\n",
      " 'Loss/regularization_loss': 0.25790948,\n",
      " 'Loss/total_loss': 0.5112453,\n",
      " 'learning_rate': 0.00995343}\n",
      "I0716 15:16:34.368237 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.18163756,\n",
      " 'Loss/localization_loss': 0.071698286,\n",
      " 'Loss/regularization_loss': 0.25790948,\n",
      " 'Loss/total_loss': 0.5112453,\n",
      " 'learning_rate': 0.00995343}\n",
      "INFO:tensorflow:Step 3100 per-step time 0.418s\n",
      "I0716 15:17:16.199017 14044 model_lib_v2.py:705] Step 3100 per-step time 0.418s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.18785481,\n",
      " 'Loss/localization_loss': 0.03967275,\n",
      " 'Loss/regularization_loss': 0.25710225,\n",
      " 'Loss/total_loss': 0.4846298,\n",
      " 'learning_rate': 0.009943668}\n",
      "I0716 15:17:16.200027 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.18785481,\n",
      " 'Loss/localization_loss': 0.03967275,\n",
      " 'Loss/regularization_loss': 0.25710225,\n",
      " 'Loss/total_loss': 0.4846298,\n",
      " 'learning_rate': 0.009943668}\n",
      "INFO:tensorflow:Step 3200 per-step time 0.380s\n",
      "I0716 15:17:54.320228 14044 model_lib_v2.py:705] Step 3200 per-step time 0.380s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1934435,\n",
      " 'Loss/localization_loss': 0.028217303,\n",
      " 'Loss/regularization_loss': 0.25591052,\n",
      " 'Loss/total_loss': 0.47757134,\n",
      " 'learning_rate': 0.009932985}\n",
      "I0716 15:17:54.320228 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.1934435,\n",
      " 'Loss/localization_loss': 0.028217303,\n",
      " 'Loss/regularization_loss': 0.25591052,\n",
      " 'Loss/total_loss': 0.47757134,\n",
      " 'learning_rate': 0.009932985}\n",
      "INFO:tensorflow:Step 3300 per-step time 0.399s\n",
      "I0716 15:18:34.130093 14044 model_lib_v2.py:705] Step 3300 per-step time 0.399s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.14976647,\n",
      " 'Loss/localization_loss': 0.03320012,\n",
      " 'Loss/regularization_loss': 0.2547981,\n",
      " 'Loss/total_loss': 0.4377647,\n",
      " 'learning_rate': 0.00992138}\n",
      "I0716 15:18:34.131091 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.14976647,\n",
      " 'Loss/localization_loss': 0.03320012,\n",
      " 'Loss/regularization_loss': 0.2547981,\n",
      " 'Loss/total_loss': 0.4377647,\n",
      " 'learning_rate': 0.00992138}\n",
      "INFO:tensorflow:Step 3400 per-step time 0.386s\n",
      "I0716 15:19:12.813341 14044 model_lib_v2.py:705] Step 3400 per-step time 0.386s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.15034115,\n",
      " 'Loss/localization_loss': 0.031975005,\n",
      " 'Loss/regularization_loss': 0.25368926,\n",
      " 'Loss/total_loss': 0.4360054,\n",
      " 'learning_rate': 0.009908859}\n",
      "I0716 15:19:12.814341 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.15034115,\n",
      " 'Loss/localization_loss': 0.031975005,\n",
      " 'Loss/regularization_loss': 0.25368926,\n",
      " 'Loss/total_loss': 0.4360054,\n",
      " 'learning_rate': 0.009908859}\n",
      "INFO:tensorflow:Step 3500 per-step time 0.403s\n",
      "I0716 15:19:53.053534 14044 model_lib_v2.py:705] Step 3500 per-step time 0.403s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.14738308,\n",
      " 'Loss/localization_loss': 0.11485099,\n",
      " 'Loss/regularization_loss': 0.25279307,\n",
      " 'Loss/total_loss': 0.51502717,\n",
      " 'learning_rate': 0.00989542}\n",
      "I0716 15:19:53.053534 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.14738308,\n",
      " 'Loss/localization_loss': 0.11485099,\n",
      " 'Loss/regularization_loss': 0.25279307,\n",
      " 'Loss/total_loss': 0.51502717,\n",
      " 'learning_rate': 0.00989542}\n",
      "INFO:tensorflow:Step 3600 per-step time 0.361s\n",
      "I0716 15:20:29.155683 14044 model_lib_v2.py:705] Step 3600 per-step time 0.361s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.3145405,\n",
      " 'Loss/localization_loss': 0.069290265,\n",
      " 'Loss/regularization_loss': 0.25232717,\n",
      " 'Loss/total_loss': 0.636158,\n",
      " 'learning_rate': 0.009881069}\n",
      "I0716 15:20:29.155683 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.3145405,\n",
      " 'Loss/localization_loss': 0.069290265,\n",
      " 'Loss/regularization_loss': 0.25232717,\n",
      " 'Loss/total_loss': 0.636158,\n",
      " 'learning_rate': 0.009881069}\n",
      "INFO:tensorflow:Step 3700 per-step time 0.352s\n",
      "I0716 15:21:04.395349 14044 model_lib_v2.py:705] Step 3700 per-step time 0.352s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.17883775,\n",
      " 'Loss/localization_loss': 0.08528357,\n",
      " 'Loss/regularization_loss': 0.25125462,\n",
      " 'Loss/total_loss': 0.5153759,\n",
      " 'learning_rate': 0.009865807}\n",
      "I0716 15:21:04.395349 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.17883775,\n",
      " 'Loss/localization_loss': 0.08528357,\n",
      " 'Loss/regularization_loss': 0.25125462,\n",
      " 'Loss/total_loss': 0.5153759,\n",
      " 'learning_rate': 0.009865807}\n",
      "INFO:tensorflow:Step 3800 per-step time 0.369s\n",
      "I0716 15:21:41.404868 14044 model_lib_v2.py:705] Step 3800 per-step time 0.369s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2053853,\n",
      " 'Loss/localization_loss': 0.049404986,\n",
      " 'Loss/regularization_loss': 0.25004634,\n",
      " 'Loss/total_loss': 0.5048366,\n",
      " 'learning_rate': 0.009849637}\n",
      "I0716 15:21:41.404868 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.2053853,\n",
      " 'Loss/localization_loss': 0.049404986,\n",
      " 'Loss/regularization_loss': 0.25004634,\n",
      " 'Loss/total_loss': 0.5048366,\n",
      " 'learning_rate': 0.009849637}\n",
      "INFO:tensorflow:Step 3900 per-step time 0.407s\n",
      "I0716 15:22:21.937675 14044 model_lib_v2.py:705] Step 3900 per-step time 0.407s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1522846,\n",
      " 'Loss/localization_loss': 0.0358886,\n",
      " 'Loss/regularization_loss': 0.24877246,\n",
      " 'Loss/total_loss': 0.43694568,\n",
      " 'learning_rate': 0.009832562}\n",
      "I0716 15:22:21.937675 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.1522846,\n",
      " 'Loss/localization_loss': 0.0358886,\n",
      " 'Loss/regularization_loss': 0.24877246,\n",
      " 'Loss/total_loss': 0.43694568,\n",
      " 'learning_rate': 0.009832562}\n",
      "INFO:tensorflow:Step 4000 per-step time 0.386s\n",
      "I0716 15:23:00.536687 14044 model_lib_v2.py:705] Step 4000 per-step time 0.386s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.23846473,\n",
      " 'Loss/localization_loss': 0.102541715,\n",
      " 'Loss/regularization_loss': 0.24768718,\n",
      " 'Loss/total_loss': 0.5886936,\n",
      " 'learning_rate': 0.0098145865}\n",
      "I0716 15:23:00.537687 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.23846473,\n",
      " 'Loss/localization_loss': 0.102541715,\n",
      " 'Loss/regularization_loss': 0.24768718,\n",
      " 'Loss/total_loss': 0.5886936,\n",
      " 'learning_rate': 0.0098145865}\n",
      "INFO:tensorflow:Step 4100 per-step time 0.433s\n",
      "I0716 15:23:43.781972 14044 model_lib_v2.py:705] Step 4100 per-step time 0.433s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.23682432,\n",
      " 'Loss/localization_loss': 0.06020712,\n",
      " 'Loss/regularization_loss': 0.24684156,\n",
      " 'Loss/total_loss': 0.543873,\n",
      " 'learning_rate': 0.009795712}\n",
      "I0716 15:23:43.781972 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.23682432,\n",
      " 'Loss/localization_loss': 0.06020712,\n",
      " 'Loss/regularization_loss': 0.24684156,\n",
      " 'Loss/total_loss': 0.543873,\n",
      " 'learning_rate': 0.009795712}\n",
      "INFO:tensorflow:Step 4200 per-step time 0.395s\n",
      "I0716 15:24:23.247038 14044 model_lib_v2.py:705] Step 4200 per-step time 0.395s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.18209846,\n",
      " 'Loss/localization_loss': 0.039558623,\n",
      " 'Loss/regularization_loss': 0.24590583,\n",
      " 'Loss/total_loss': 0.4675629,\n",
      " 'learning_rate': 0.009775942}\n",
      "I0716 15:24:23.247038 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.18209846,\n",
      " 'Loss/localization_loss': 0.039558623,\n",
      " 'Loss/regularization_loss': 0.24590583,\n",
      " 'Loss/total_loss': 0.4675629,\n",
      " 'learning_rate': 0.009775942}\n",
      "INFO:tensorflow:Step 4300 per-step time 0.397s\n",
      "I0716 15:25:03.044804 14044 model_lib_v2.py:705] Step 4300 per-step time 0.397s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13990675,\n",
      " 'Loss/localization_loss': 0.06943383,\n",
      " 'Loss/regularization_loss': 0.24526158,\n",
      " 'Loss/total_loss': 0.45460215,\n",
      " 'learning_rate': 0.009755282}\n",
      "I0716 15:25:03.045828 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.13990675,\n",
      " 'Loss/localization_loss': 0.06943383,\n",
      " 'Loss/regularization_loss': 0.24526158,\n",
      " 'Loss/total_loss': 0.45460215,\n",
      " 'learning_rate': 0.009755282}\n",
      "INFO:tensorflow:Step 4400 per-step time 0.394s\n",
      "I0716 15:25:42.378992 14044 model_lib_v2.py:705] Step 4400 per-step time 0.394s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1955597,\n",
      " 'Loss/localization_loss': 0.018757382,\n",
      " 'Loss/regularization_loss': 0.24415922,\n",
      " 'Loss/total_loss': 0.4584763,\n",
      " 'learning_rate': 0.009733735}\n",
      "I0716 15:25:42.378992 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.1955597,\n",
      " 'Loss/localization_loss': 0.018757382,\n",
      " 'Loss/regularization_loss': 0.24415922,\n",
      " 'Loss/total_loss': 0.4584763,\n",
      " 'learning_rate': 0.009733735}\n",
      "INFO:tensorflow:Step 4500 per-step time 0.394s\n",
      "I0716 15:26:21.822664 14044 model_lib_v2.py:705] Step 4500 per-step time 0.394s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1834794,\n",
      " 'Loss/localization_loss': 0.08380975,\n",
      " 'Loss/regularization_loss': 0.24312538,\n",
      " 'Loss/total_loss': 0.51041454,\n",
      " 'learning_rate': 0.009711305}\n",
      "I0716 15:26:21.822664 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.1834794,\n",
      " 'Loss/localization_loss': 0.08380975,\n",
      " 'Loss/regularization_loss': 0.24312538,\n",
      " 'Loss/total_loss': 0.51041454,\n",
      " 'learning_rate': 0.009711305}\n",
      "INFO:tensorflow:Step 4600 per-step time 0.394s\n",
      "I0716 15:27:01.179031 14044 model_lib_v2.py:705] Step 4600 per-step time 0.394s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2577576,\n",
      " 'Loss/localization_loss': 0.08070827,\n",
      " 'Loss/regularization_loss': 0.24192816,\n",
      " 'Loss/total_loss': 0.580394,\n",
      " 'learning_rate': 0.009687995}\n",
      "I0716 15:27:01.179031 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.2577576,\n",
      " 'Loss/localization_loss': 0.08070827,\n",
      " 'Loss/regularization_loss': 0.24192816,\n",
      " 'Loss/total_loss': 0.580394,\n",
      " 'learning_rate': 0.009687995}\n",
      "INFO:tensorflow:Step 4700 per-step time 0.394s\n",
      "I0716 15:27:40.528830 14044 model_lib_v2.py:705] Step 4700 per-step time 0.394s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2321379,\n",
      " 'Loss/localization_loss': 0.089458995,\n",
      " 'Loss/regularization_loss': 0.24086389,\n",
      " 'Loss/total_loss': 0.5624608,\n",
      " 'learning_rate': 0.009663811}\n",
      "I0716 15:27:40.529830 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.2321379,\n",
      " 'Loss/localization_loss': 0.089458995,\n",
      " 'Loss/regularization_loss': 0.24086389,\n",
      " 'Loss/total_loss': 0.5624608,\n",
      " 'learning_rate': 0.009663811}\n",
      "INFO:tensorflow:Step 4800 per-step time 0.394s\n",
      "I0716 15:28:19.953298 14044 model_lib_v2.py:705] Step 4800 per-step time 0.394s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09507759,\n",
      " 'Loss/localization_loss': 0.011809034,\n",
      " 'Loss/regularization_loss': 0.23981968,\n",
      " 'Loss/total_loss': 0.3467063,\n",
      " 'learning_rate': 0.0096387565}\n",
      "I0716 15:28:19.953298 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.09507759,\n",
      " 'Loss/localization_loss': 0.011809034,\n",
      " 'Loss/regularization_loss': 0.23981968,\n",
      " 'Loss/total_loss': 0.3467063,\n",
      " 'learning_rate': 0.0096387565}\n",
      "INFO:tensorflow:Step 4900 per-step time 0.394s\n",
      "I0716 15:28:59.363871 14044 model_lib_v2.py:705] Step 4900 per-step time 0.394s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.24240081,\n",
      " 'Loss/localization_loss': 0.14878474,\n",
      " 'Loss/regularization_loss': 0.23855674,\n",
      " 'Loss/total_loss': 0.62974226,\n",
      " 'learning_rate': 0.009612837}\n",
      "I0716 15:28:59.363871 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.24240081,\n",
      " 'Loss/localization_loss': 0.14878474,\n",
      " 'Loss/regularization_loss': 0.23855674,\n",
      " 'Loss/total_loss': 0.62974226,\n",
      " 'learning_rate': 0.009612837}\n",
      "INFO:tensorflow:Step 5000 per-step time 0.395s\n",
      "I0716 15:29:38.854964 14044 model_lib_v2.py:705] Step 5000 per-step time 0.395s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2201816,\n",
      " 'Loss/localization_loss': 0.060979426,\n",
      " 'Loss/regularization_loss': 0.23749848,\n",
      " 'Loss/total_loss': 0.5186595,\n",
      " 'learning_rate': 0.009586057}\n",
      "I0716 15:29:38.854964 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.2201816,\n",
      " 'Loss/localization_loss': 0.060979426,\n",
      " 'Loss/regularization_loss': 0.23749848,\n",
      " 'Loss/total_loss': 0.5186595,\n",
      " 'learning_rate': 0.009586057}\n",
      "INFO:tensorflow:Step 5100 per-step time 0.447s\n",
      "I0716 15:30:23.647843 14044 model_lib_v2.py:705] Step 5100 per-step time 0.447s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.19582747,\n",
      " 'Loss/localization_loss': 0.052276615,\n",
      " 'Loss/regularization_loss': 0.23637705,\n",
      " 'Loss/total_loss': 0.48448113,\n",
      " 'learning_rate': 0.00955842}\n",
      "I0716 15:30:23.647843 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.19582747,\n",
      " 'Loss/localization_loss': 0.052276615,\n",
      " 'Loss/regularization_loss': 0.23637705,\n",
      " 'Loss/total_loss': 0.48448113,\n",
      " 'learning_rate': 0.00955842}\n",
      "INFO:tensorflow:Step 5200 per-step time 0.381s\n",
      "I0716 15:31:01.625087 14044 model_lib_v2.py:705] Step 5200 per-step time 0.381s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.10808206,\n",
      " 'Loss/localization_loss': 0.043191247,\n",
      " 'Loss/regularization_loss': 0.23510677,\n",
      " 'Loss/total_loss': 0.38638008,\n",
      " 'learning_rate': 0.009529933}\n",
      "I0716 15:31:01.625087 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.10808206,\n",
      " 'Loss/localization_loss': 0.043191247,\n",
      " 'Loss/regularization_loss': 0.23510677,\n",
      " 'Loss/total_loss': 0.38638008,\n",
      " 'learning_rate': 0.009529933}\n",
      "INFO:tensorflow:Step 5300 per-step time 0.382s\n",
      "I0716 15:31:39.778101 14044 model_lib_v2.py:705] Step 5300 per-step time 0.382s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13838093,\n",
      " 'Loss/localization_loss': 0.045867,\n",
      " 'Loss/regularization_loss': 0.23376517,\n",
      " 'Loss/total_loss': 0.4180131,\n",
      " 'learning_rate': 0.009500602}\n",
      "I0716 15:31:39.778101 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.13838093,\n",
      " 'Loss/localization_loss': 0.045867,\n",
      " 'Loss/regularization_loss': 0.23376517,\n",
      " 'Loss/total_loss': 0.4180131,\n",
      " 'learning_rate': 0.009500602}\n",
      "INFO:tensorflow:Step 5400 per-step time 0.381s\n",
      "I0716 15:32:17.913290 14044 model_lib_v2.py:705] Step 5400 per-step time 0.381s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.23308069,\n",
      " 'Loss/localization_loss': 0.017005427,\n",
      " 'Loss/regularization_loss': 0.2324634,\n",
      " 'Loss/total_loss': 0.48254952,\n",
      " 'learning_rate': 0.009470431}\n",
      "I0716 15:32:17.913290 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.23308069,\n",
      " 'Loss/localization_loss': 0.017005427,\n",
      " 'Loss/regularization_loss': 0.2324634,\n",
      " 'Loss/total_loss': 0.48254952,\n",
      " 'learning_rate': 0.009470431}\n",
      "INFO:tensorflow:Step 5500 per-step time 0.348s\n",
      "I0716 15:32:52.650697 14044 model_lib_v2.py:705] Step 5500 per-step time 0.348s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.29367727,\n",
      " 'Loss/localization_loss': 0.035857197,\n",
      " 'Loss/regularization_loss': 0.23120368,\n",
      " 'Loss/total_loss': 0.56073815,\n",
      " 'learning_rate': 0.009439426}\n",
      "I0716 15:32:52.651707 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.29367727,\n",
      " 'Loss/localization_loss': 0.035857197,\n",
      " 'Loss/regularization_loss': 0.23120368,\n",
      " 'Loss/total_loss': 0.56073815,\n",
      " 'learning_rate': 0.009439426}\n",
      "INFO:tensorflow:Step 5600 per-step time 0.352s\n",
      "I0716 15:33:27.856894 14044 model_lib_v2.py:705] Step 5600 per-step time 0.352s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.43715528,\n",
      " 'Loss/localization_loss': 0.026532045,\n",
      " 'Loss/regularization_loss': 0.2298775,\n",
      " 'Loss/total_loss': 0.69356483,\n",
      " 'learning_rate': 0.009407593}\n",
      "I0716 15:33:27.856894 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.43715528,\n",
      " 'Loss/localization_loss': 0.026532045,\n",
      " 'Loss/regularization_loss': 0.2298775,\n",
      " 'Loss/total_loss': 0.69356483,\n",
      " 'learning_rate': 0.009407593}\n",
      "INFO:tensorflow:Step 5700 per-step time 0.404s\n",
      "I0716 15:34:08.311102 14044 model_lib_v2.py:705] Step 5700 per-step time 0.404s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.20473112,\n",
      " 'Loss/localization_loss': 0.045886848,\n",
      " 'Loss/regularization_loss': 0.22853665,\n",
      " 'Loss/total_loss': 0.47915462,\n",
      " 'learning_rate': 0.009374937}\n",
      "I0716 15:34:08.311102 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.20473112,\n",
      " 'Loss/localization_loss': 0.045886848,\n",
      " 'Loss/regularization_loss': 0.22853665,\n",
      " 'Loss/total_loss': 0.47915462,\n",
      " 'learning_rate': 0.009374937}\n",
      "INFO:tensorflow:Step 5800 per-step time 0.367s\n",
      "I0716 15:34:44.893608 14044 model_lib_v2.py:705] Step 5800 per-step time 0.367s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.21231568,\n",
      " 'Loss/localization_loss': 0.017234381,\n",
      " 'Loss/regularization_loss': 0.22714359,\n",
      " 'Loss/total_loss': 0.45669365,\n",
      " 'learning_rate': 0.009341464}\n",
      "I0716 15:34:44.893608 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.21231568,\n",
      " 'Loss/localization_loss': 0.017234381,\n",
      " 'Loss/regularization_loss': 0.22714359,\n",
      " 'Loss/total_loss': 0.45669365,\n",
      " 'learning_rate': 0.009341464}\n",
      "INFO:tensorflow:Step 5900 per-step time 0.362s\n",
      "I0716 15:35:21.103960 14044 model_lib_v2.py:705] Step 5900 per-step time 0.362s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.19437905,\n",
      " 'Loss/localization_loss': 0.050717223,\n",
      " 'Loss/regularization_loss': 0.2258187,\n",
      " 'Loss/total_loss': 0.47091496,\n",
      " 'learning_rate': 0.009307182}\n",
      "I0716 15:35:21.103960 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.19437905,\n",
      " 'Loss/localization_loss': 0.050717223,\n",
      " 'Loss/regularization_loss': 0.2258187,\n",
      " 'Loss/total_loss': 0.47091496,\n",
      " 'learning_rate': 0.009307182}\n",
      "INFO:tensorflow:Step 6000 per-step time 0.358s\n",
      "I0716 15:35:56.915134 14044 model_lib_v2.py:705] Step 6000 per-step time 0.358s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.19248146,\n",
      " 'Loss/localization_loss': 0.028363746,\n",
      " 'Loss/regularization_loss': 0.22451189,\n",
      " 'Loss/total_loss': 0.44535708,\n",
      " 'learning_rate': 0.009272097}\n",
      "I0716 15:35:56.915134 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.19248146,\n",
      " 'Loss/localization_loss': 0.028363746,\n",
      " 'Loss/regularization_loss': 0.22451189,\n",
      " 'Loss/total_loss': 0.44535708,\n",
      " 'learning_rate': 0.009272097}\n",
      "INFO:tensorflow:Step 6100 per-step time 0.407s\n",
      "I0716 15:36:37.550500 14044 model_lib_v2.py:705] Step 6100 per-step time 0.407s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.33842346,\n",
      " 'Loss/localization_loss': 0.11224904,\n",
      " 'Loss/regularization_loss': 0.22317517,\n",
      " 'Loss/total_loss': 0.6738477,\n",
      " 'learning_rate': 0.009236214}\n",
      "I0716 15:36:37.550500 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.33842346,\n",
      " 'Loss/localization_loss': 0.11224904,\n",
      " 'Loss/regularization_loss': 0.22317517,\n",
      " 'Loss/total_loss': 0.6738477,\n",
      " 'learning_rate': 0.009236214}\n",
      "INFO:tensorflow:Step 6200 per-step time 0.367s\n",
      "I0716 15:37:14.306477 14044 model_lib_v2.py:705] Step 6200 per-step time 0.367s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.25430265,\n",
      " 'Loss/localization_loss': 0.028274374,\n",
      " 'Loss/regularization_loss': 0.22188155,\n",
      " 'Loss/total_loss': 0.50445855,\n",
      " 'learning_rate': 0.009199542}\n",
      "I0716 15:37:14.306477 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.25430265,\n",
      " 'Loss/localization_loss': 0.028274374,\n",
      " 'Loss/regularization_loss': 0.22188155,\n",
      " 'Loss/total_loss': 0.50445855,\n",
      " 'learning_rate': 0.009199542}\n",
      "INFO:tensorflow:Step 6300 per-step time 0.354s\n",
      "I0716 15:37:49.611544 14044 model_lib_v2.py:705] Step 6300 per-step time 0.354s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.19020285,\n",
      " 'Loss/localization_loss': 0.056239367,\n",
      " 'Loss/regularization_loss': 0.220622,\n",
      " 'Loss/total_loss': 0.4670642,\n",
      " 'learning_rate': 0.009162085}\n",
      "I0716 15:37:49.612513 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.19020285,\n",
      " 'Loss/localization_loss': 0.056239367,\n",
      " 'Loss/regularization_loss': 0.220622,\n",
      " 'Loss/total_loss': 0.4670642,\n",
      " 'learning_rate': 0.009162085}\n",
      "INFO:tensorflow:Step 6400 per-step time 0.400s\n",
      "I0716 15:38:29.578941 14044 model_lib_v2.py:705] Step 6400 per-step time 0.400s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.24278787,\n",
      " 'Loss/localization_loss': 0.01815411,\n",
      " 'Loss/regularization_loss': 0.21938121,\n",
      " 'Loss/total_loss': 0.4803232,\n",
      " 'learning_rate': 0.0091238525}\n",
      "I0716 15:38:29.578941 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.24278787,\n",
      " 'Loss/localization_loss': 0.01815411,\n",
      " 'Loss/regularization_loss': 0.21938121,\n",
      " 'Loss/total_loss': 0.4803232,\n",
      " 'learning_rate': 0.0091238525}\n",
      "INFO:tensorflow:Step 6500 per-step time 0.351s\n",
      "I0716 15:39:04.706930 14044 model_lib_v2.py:705] Step 6500 per-step time 0.351s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.23202491,\n",
      " 'Loss/localization_loss': 0.029599318,\n",
      " 'Loss/regularization_loss': 0.21816449,\n",
      " 'Loss/total_loss': 0.47978872,\n",
      " 'learning_rate': 0.00908485}\n",
      "I0716 15:39:04.706930 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.23202491,\n",
      " 'Loss/localization_loss': 0.029599318,\n",
      " 'Loss/regularization_loss': 0.21816449,\n",
      " 'Loss/total_loss': 0.47978872,\n",
      " 'learning_rate': 0.00908485}\n",
      "INFO:tensorflow:Step 6600 per-step time 0.351s\n",
      "I0716 15:39:39.734545 14044 model_lib_v2.py:705] Step 6600 per-step time 0.351s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1781776,\n",
      " 'Loss/localization_loss': 0.13883543,\n",
      " 'Loss/regularization_loss': 0.21696973,\n",
      " 'Loss/total_loss': 0.53398275,\n",
      " 'learning_rate': 0.009045085}\n",
      "I0716 15:39:39.734545 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.1781776,\n",
      " 'Loss/localization_loss': 0.13883543,\n",
      " 'Loss/regularization_loss': 0.21696973,\n",
      " 'Loss/total_loss': 0.53398275,\n",
      " 'learning_rate': 0.009045085}\n",
      "INFO:tensorflow:Step 6700 per-step time 0.366s\n",
      "I0716 15:40:16.323994 14044 model_lib_v2.py:705] Step 6700 per-step time 0.366s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.23033327,\n",
      " 'Loss/localization_loss': 0.045119792,\n",
      " 'Loss/regularization_loss': 0.21575859,\n",
      " 'Loss/total_loss': 0.49121165,\n",
      " 'learning_rate': 0.009004565}\n",
      "I0716 15:40:16.323994 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.23033327,\n",
      " 'Loss/localization_loss': 0.045119792,\n",
      " 'Loss/regularization_loss': 0.21575859,\n",
      " 'Loss/total_loss': 0.49121165,\n",
      " 'learning_rate': 0.009004565}\n",
      "INFO:tensorflow:Step 6800 per-step time 0.365s\n",
      "I0716 15:40:52.840265 14044 model_lib_v2.py:705] Step 6800 per-step time 0.365s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.19149886,\n",
      " 'Loss/localization_loss': 0.058580015,\n",
      " 'Loss/regularization_loss': 0.2146332,\n",
      " 'Loss/total_loss': 0.46471208,\n",
      " 'learning_rate': 0.008963299}\n",
      "I0716 15:40:52.840265 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.19149886,\n",
      " 'Loss/localization_loss': 0.058580015,\n",
      " 'Loss/regularization_loss': 0.2146332,\n",
      " 'Loss/total_loss': 0.46471208,\n",
      " 'learning_rate': 0.008963299}\n",
      "INFO:tensorflow:Step 6900 per-step time 0.370s\n",
      "I0716 15:41:29.840579 14044 model_lib_v2.py:705] Step 6900 per-step time 0.370s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.29079947,\n",
      " 'Loss/localization_loss': 0.08339947,\n",
      " 'Loss/regularization_loss': 0.21357395,\n",
      " 'Loss/total_loss': 0.5877729,\n",
      " 'learning_rate': 0.0089212945}\n",
      "I0716 15:41:29.840579 14044 model_lib_v2.py:708] {'Loss/classification_loss': 0.29079947,\n",
      " 'Loss/localization_loss': 0.08339947,\n",
      " 'Loss/regularization_loss': 0.21357395,\n",
      " 'Loss/total_loss': 0.5877729,\n",
      " 'learning_rate': 0.0089212945}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py \\\n",
    "    --pipeline_config_path={pipeline_config_path} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={NUM_STEPS} \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --num_eval_steps={NUM_EVAL_STEPS}\n",
    "\n",
    "training_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_info(\n",
    "    batch=BATCH_SIZE,\n",
    "    classes=NUM_CLASSES,\n",
    "    steps=NUM_STEPS,\n",
    "    eval_steps=NUM_EVAL_STEPS,\n",
    "    name=MODEL_NAME,\n",
    "    bfloat16=use_bfloat16,\n",
    "    pipeline=pipeline_config,\n",
    "    checkpoint_path=fine_tune_checkpoint_path,\n",
    "    labelmap=short_labelmap_path,\n",
    "    train_record=short_train_record_path,\n",
    "    test_record=short_test_record_path,\n",
    "    config=pipeline_config_path,\n",
    "    trained_model=model_dir,\n",
    "    train_time=training_time,\n",
    "    df_test=df_final_test,\n",
    "    df_train=df_final_train,\n",
    "    notes=NOTES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"python C:\\\\Users\\\\Alexej\\\\Desktop\\\\GTSRB\\\\models\\\\research\\\\object_detection\\\\model_main_tf2.py \\\n",
    "            --pipeline_config_path={pipeline_config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={model_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "output, error = process.communicate()\n",
    "\n",
    "if process.returncode != 0:\n",
    "    print(error.decode(\"utf-8\"))\n",
    "else:\n",
    "    output_str = output.decode(\"utf-8\")\n",
    "    header = f\"Evaluation Results for: {MODEL_NAME}\\n\\n\"\n",
    "    with open(f\"./myModules/log/{MODEL_NAME}_evaluation_results.txt\", \"w\") as f:\n",
    "        f.write(header)\n",
    "        f.write(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\model_main_tf2.py \\\n",
    "    --pipeline_config_path={pipeline_config_path} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --checkpoint_dir={model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoints(model_dir):\n",
    "    \"\"\"Returns a list of checkpoint paths in the given directory.\"\"\"\n",
    "    checkpoints = []\n",
    "    for filename in os.listdir(model_dir):\n",
    "        if filename.startswith('ckpt-') and filename.endswith('.index'):\n",
    "            checkpoint = os.path.splitext(filename)[0]\n",
    "            checkpoints.append(os.path.join(model_dir, checkpoint))\n",
    "    return sorted(set(checkpoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_checkpoints(pipeline_config_path, model_dir, checkpoints, model_name):\n",
    "    \"\"\"Evaluates all checkpoints and logs the results.\"\"\"\n",
    "    for checkpoint in checkpoints:\n",
    "        eval_command = f\"python C:/Users/Alexej/Desktop/GTSRB/models/research/object_detection/model_main_tf2.py \\\n",
    "            --pipeline_config_path={pipeline_config_path} \\\n",
    "            --model_dir={model_dir} \\\n",
    "            --checkpoint_dir={checkpoint} \\\n",
    "            --run_once\"\n",
    "\n",
    "        # Starte den Evaluierungsprozess\n",
    "        process = subprocess.Popen(eval_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "        output, error = process.communicate()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f\"Error evaluating checkpoint {checkpoint}:\")\n",
    "            print(error.decode(\"utf-8\"))\n",
    "        else:\n",
    "            output_str = output.decode(\"utf-8\")\n",
    "            checkpoint_name = os.path.basename(checkpoint)\n",
    "            header = f\"Evaluation Results for MODEL_NAME: {model_name} at Checkpoint: {checkpoint_name}\\n\\n\"\n",
    "            with open(f\"./myModules/log/{model_name}_evaluation_results_{checkpoint_name}.txt\", \"w\") as f:\n",
    "                f.write(header)\n",
    "                f.write(output_str)\n",
    "            print(f\"Evaluation for checkpoint {checkpoint} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = get_checkpoints(model_dir)\n",
    "\n",
    "evaluate_checkpoints(pipeline_config_path, model_dir, checkpoints, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_multiple_images(model, image, min_score_threshold):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Filter out detections with a score below the threshold\n",
    "    scores = output_dict['detection_scores']\n",
    "    high_score_indices = scores >= min_score_threshold\n",
    "\n",
    "    output_dict['detection_boxes'] = output_dict['detection_boxes'][high_score_indices]\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'][high_score_indices]\n",
    "    output_dict['detection_scores'] = output_dict['detection_scores'][high_score_indices]\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "            image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path, width, height):\n",
    "    image = Image.open(path)\n",
    "    # Resize the image to a larger size\n",
    "    image = image.resize((width, height))  # Resize to 256x256, you can change this as needed\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes_with_color(image, boxes, classes, scores, category_index, box_color, line_thickness):\n",
    "    for i in range(boxes.shape[0]):\n",
    "        if scores is None or scores[i] > 0.5:\n",
    "            class_id = int(classes[i])\n",
    "            if class_id in category_index.keys():\n",
    "                class_name = category_index[class_id]['name']\n",
    "                display_str = str(class_name)\n",
    "                color = box_color\n",
    "                vis_util.draw_bounding_box_on_image_array(\n",
    "                    image,\n",
    "                    boxes[i][0],\n",
    "                    boxes[i][1],\n",
    "                    boxes[i][2],\n",
    "                    boxes[i][3],\n",
    "                    color=color,\n",
    "                    thickness=line_thickness,\n",
    "                    display_str_list=[display_str],\n",
    "                    use_normalized_coordinates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_legend(image, groundtruth_classes, category_index, iou):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    image_height = image.size[1]\n",
    "    font_size = max(int(image_height * 0.04), 12)  # Schriftgröße auf 4% der Bildhöhe begrenzt, aber mindestens 12\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "\n",
    "    y_offset = int(0.9 * image_height)  # Startoffset für die Legende, z.B. 90% der Bildhöhe\n",
    "    x_offset = int(0.05 * image.size[0])  # Startoffset für die Legende, z.B. 5% der Bildbreite\n",
    "\n",
    "    # Ground Truth Legende\n",
    "    for gt_class in groundtruth_classes:\n",
    "        if gt_class in category_index:\n",
    "            class_name = category_index[gt_class]['name']\n",
    "            draw.text((x_offset, y_offset), f\"GT: {class_name}\\nIoU: {iou:.2f}\", fill=\"yellow\", font=font)\n",
    "            y_offset += int(font_size * 1.5)  # Erhöhe den Offset basierend auf der aktuellen Schriftgröße"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxA, boxB):\n",
    "    yA = max(boxA[0], boxB[0])\n",
    "    xA = max(boxA[1], boxB[1])\n",
    "    yB = min(boxA[2], boxB[2])\n",
    "    xB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    \n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    \n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    \n",
    "    image_pil = Image.fromarray(np.uint8(image_np)).convert(\"RGB\")\n",
    "    add_legend(image_pil, groundtruth_classes, category_index, iou)\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, ax):\n",
    "    # Visualize ground truth with red color\n",
    "    visualize_boxes_with_color(\n",
    "        image_np,\n",
    "        np.array(groundtruth_boxes),\n",
    "        np.array(groundtruth_classes),\n",
    "        np.ones(len(groundtruth_boxes)),\n",
    "        category_index,\n",
    "        box_color='red',\n",
    "        line_thickness=1)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "    \n",
    "    iou = compute_iou(groundtruth_boxes[0], output_dict['detection_boxes'][0])\n",
    "    ax.imshow(image_np)\n",
    "    ax.axis('off')  # Hide the axis\n",
    "\n",
    "    # Adding legend to the plot\n",
    "    label_text = category_index[groundtruth_classes[0]]['name']\n",
    "    ax.set_title(f'{label_text}\\nIoU: {iou:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_images_for_labels(df, model, category_index, discard_predictions_below_acc, num_images_to_display):\n",
    "    sorted_labels = sorted(df['Label'].unique())\n",
    "\n",
    "    num_rows = len(sorted_labels)\n",
    "    num_cols = num_images_to_display\n",
    "\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols + num_cols * 0.5, num_rows * 2))\n",
    "    \n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, num_cols)\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for idx, label in enumerate(sorted_labels):\n",
    "        label_df = df[df['Label'] == label]\n",
    "        \n",
    "        random_indices = random.sample(range(len(label_df)), min(num_images_to_display, len(label_df)))\n",
    "\n",
    "        for jdx, r_idx in enumerate(random_indices):\n",
    "            row = label_df.iloc[r_idx]\n",
    "            image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "            groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "            groundtruth_classes = [row['Label']]\n",
    "            \n",
    "            output_dict = run_inference_for_multiple_images(model, image_np, discard_predictions_below_acc)\n",
    "            \n",
    "            plot_idx = idx * num_images_to_display + jdx\n",
    "            \n",
    "            visualize_predictions_and_groundtruth_plot(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index, axes[plot_idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_predictions_with_groundtruth (model, df, num_images_to_display, category_index):\n",
    "    \n",
    "    random_indices = random.sample(range(len(df)), num_images_to_display)\n",
    "\n",
    "    # Visualisiere zufällige ausgewählte Bilder\n",
    "    for idx in random_indices:\n",
    "        row = df.iloc[idx]\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_endless_predictions(model, df, category_index):\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        groundtruth_boxes = [[row['Roi.Y1'] / row['Height'], row['Roi.X1'] / row['Width'], row['Roi.Y2'] / row['Height'], row['Roi.X2'] / row['Width']]]\n",
    "        groundtruth_classes = [row['Label']]\n",
    "        \n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        visualize_predictions_and_groundtruth(image_np, output_dict, groundtruth_boxes, groundtruth_classes, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_endless_predictions_from_path(model, path, category_index):\n",
    "    \n",
    "    for image_path in glob.glob(path):\n",
    "        image_np = load_image_into_numpy_array(image_path, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "        output_dict = run_inference_for_single_image(model, image_np)\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=1)\n",
    "        display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './inference/faster_rcnn/{name}'.format(name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 15:50:49.369969: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-16 15:50:50.455308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6358 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "WARNING:tensorflow:From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "W0716 15:50:54.562166 19836 deprecation.py:623] From c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "I0716 15:50:59.244663 19836 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0716 15:51:09.514073 19836 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\exporter_main_v2.py\", line 164, in <module>\n",
      "    app.run(main)\n",
      "  File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\absl\\app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\absl\\app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\exporter_main_v2.py\", line 157, in main\n",
      "    exporter_lib_v2.export_inference_graph(\n",
      "  File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\exporter_lib_v2.py\", line 271, in export_inference_graph\n",
      "    status.assert_existing_objects_matched()\n",
      "  File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint.py\", line 951, in assert_existing_objects_matched\n",
      "    raise AssertionError(\n",
      "AssertionError: No checkpoint specified (save_path=None); nothing is being restored.\n"
     ]
    }
   ],
   "source": [
    "!python C:\\Users\\Alexej\\Desktop\\GTSRB\\models\\research\\object_detection\\exporter_main_v2.py \\\n",
    "    --trained_checkpoint_dir {model_dir} \\\n",
    "    --output_directory {output_directory} \\\n",
    "    --pipeline_config_path {pipeline_config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(short_labelmap_path, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load(f'./{output_directory}/saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Inference Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = './GTSRB/Final_Test/Images/*.ppm'\n",
    "min_score_threshold = 0.6\n",
    "number_of_images_to_display = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_endless_predictions_from_path(model, image_path, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_endless_predictions(model, df_final_test, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_predictions_with_groundtruth(model, df_final_test, number_of_images_to_display, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_images_for_labels(df_final_test, model, category_index, min_score_threshold, number_of_images_to_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_labels = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "# Visualisiere zufällig ausgewählte Bilder und sammle Vorhersagen und Ground Truth\n",
    "for idx, row in df_test_raw.iterrows():\n",
    "    row = df_test_raw.iloc[idx]\n",
    "    image_np = load_image_into_numpy_array(row['Path'], IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "    groundtruth_classes = [row['Label']]\n",
    "    \n",
    "    # Führe die Inferenz für das Bild durch\n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "    \n",
    "    # Sammle Vorhersagen und Ground Truth\n",
    "    predicted_class = output_dict['detection_classes'][0]\n",
    "    all_predicted_labels.append(predicted_class)\n",
    "    all_true_labels.append(groundtruth_classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
    "recall = recall_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "precision = precision_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "f1 = f1_score(all_true_labels, all_predicted_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle die Konfusionsmatrix\n",
    "cm = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "# Definiere die Klassenlabels (optional, wenn bekannt)\n",
    "\n",
    "# Plotte die Konfusionsmatrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Vorhersage')\n",
    "plt.ylabel('Wahre Werte')\n",
    "plt.title('Konfusionsmatrix')\n",
    "plt.show()\n",
    "\n",
    "# Drucke den Klassifikationsbericht\n",
    "print(classification_report(all_true_labels, all_predicted_labels, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-Time detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"./myModules/configs/Faster_RCNN_640_50_fixed_config.config\"\n",
    "CHECKPOINT_PATH = 'D:\\\\Desktop-Short\\\\trained_models\\\\faster_rcnn\\\\Faster_RCNN_640_50_fixed'\n",
    "LABLE_MAP_PATH = \"./myModules/label_map_long.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-8')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(LABLE_MAP_PATH, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load('./inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Laden des Modells\n",
    "def load_model(model_path):\n",
    "    saved_model = tf.saved_model.load(model_path)\n",
    "    return saved_model\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "model_path = './inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model'\n",
    "detection_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_UserObject' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     configs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv2\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m detection_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[58], line 4\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m():\n\u001b[0;32m      3\u001b[0m     configs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv2\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: '_UserObject' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Funktion zum Erstellen des Detektionsmodells und Wiederherstellen des Checkpoints\n",
    "def load_model():\n",
    "    configs = tf.compat.v2.saved_model.load('./inference/faster_rcnn/Faster_RCNN_640_50_fixed/saved_model')\n",
    "    return configs['model']\n",
    "\n",
    "detection_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Alexej\\AppData\\Local\\Temp\\ipykernel_2616\\4291391041.py\", line 11, in detect_fn  *\n        image, shapes = detection_model.preprocess(image)\n    File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 689, in preprocess  *\n        (resized_inputs,\n    File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\utils\\shape_utils.py\", line 492, in resize_images_and_return_shapes  *\n        outputs = static_or_dynamic_map_fn(\n    File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\utils\\shape_utils.py\", line 246, in static_or_dynamic_map_fn  *\n        outputs = [fn(arg) for arg in tf.unstack(elems)]\n    File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\core\\preprocessor.py\", line 3035, in resize_to_range  *\n        raise ValueError('Image should be 3D tensor')\n\n    ValueError: Image should be 3D tensor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m image_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(frame)\n\u001b[0;32m      6\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(np\u001b[38;5;241m.\u001b[39mexpand_dims(image_np, \u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 7\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m num_detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(detections\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_detections\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     10\u001b[0m detections \u001b[38;5;241m=\u001b[39m {key: value[\u001b[38;5;241m0\u001b[39m, :num_detections]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     11\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m detections\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file1q3vyeh1.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__detect_fn\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m (image, shapes) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(detection_model)\u001b[38;5;241m.\u001b[39mpreprocess, (ag__\u001b[38;5;241m.\u001b[39mld(image),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m prediction_dict \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(detection_model)\u001b[38;5;241m.\u001b[39mpredict, (ag__\u001b[38;5;241m.\u001b[39mld(image), ag__\u001b[38;5;241m.\u001b[39mld(shapes)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m detections \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(detection_model)\u001b[38;5;241m.\u001b[39mpostprocess, (ag__\u001b[38;5;241m.\u001b[39mld(prediction_dict), ag__\u001b[38;5;241m.\u001b[39mld(shapes)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_88ecwpl.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__preprocess\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     32\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 34\u001b[0m     (resized_inputs, true_image_shapes) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(shape_utils)\u001b[38;5;241m.\u001b[39mresize_images_and_return_shapes, (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_image_resizer_fn), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m         do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_2pv66m0.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__resize_images_and_return_shapes\u001b[1;34m(inputs, image_resizer_fn)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     36\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(inputs)\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfloat32, if_body, else_body, get_state, set_state, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(static_or_dynamic_map_fn), (ag__\u001b[38;5;241m.\u001b[39mld(image_resizer_fn),), \u001b[38;5;28mdict\u001b[39m(elems\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(inputs), dtype\u001b[38;5;241m=\u001b[39m[ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfloat32, ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mint32]), fscope)\n\u001b[0;32m     38\u001b[0m resized_inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(outputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m true_image_shapes \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(outputs)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filez2vfjks2.py:186\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__static_or_dynamic_map_fn\u001b[1;34m(fn, elems, dtype, parallel_iterations, back_prop)\u001b[0m\n\u001b[0;32m    184\u001b[0m arg_tuples \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marg_tuples\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 186\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28misinstance\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(elems), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlist\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), if_body_5, else_body_5, get_state_7, set_state_7, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_return\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretval_\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_12\u001b[39m():\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (do_return, retval_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filez2vfjks2.py:179\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__static_or_dynamic_map_fn.<locals>.else_body_5\u001b[1;34m()\u001b[0m\n\u001b[0;32m    177\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(fn), (ag__\u001b[38;5;241m.\u001b[39mld(arg),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39munstack, (ag__\u001b[38;5;241m.\u001b[39mld(elems),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)]\n\u001b[0;32m    178\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 179\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mor_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43melems_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43melems_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_6\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_6\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdo_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretval_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filez2vfjks2.py:177\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__static_or_dynamic_map_fn.<locals>.else_body_5.<locals>.else_body_4\u001b[1;34m()\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body_4\u001b[39m():\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m do_return, retval_, outputs\n\u001b[1;32m--> 177\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(fn), (ag__\u001b[38;5;241m.\u001b[39mld(arg),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39munstack, (ag__\u001b[38;5;241m.\u001b[39mld(elems),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filez2vfjks2.py:177\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body_4\u001b[39m():\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m do_return, retval_, outputs\n\u001b[1;32m--> 177\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39munstack, (ag__\u001b[38;5;241m.\u001b[39mld(elems),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file79c563p7.py:65\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__resize_to_range\u001b[1;34m(image, masks, min_dimension, max_dimension, method, align_corners, pad_to_max_dimension, per_channel_pad_value)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body\u001b[39m():\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlen\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(image)\u001b[38;5;241m.\u001b[39mget_shape, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m, if_body, else_body, get_state, set_state, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;129m@ag__\u001b[39m\u001b[38;5;241m.\u001b[39mautograph_artifact\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_resize_landscape_image\u001b[39m(image):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mFunctionScope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_resize_landscape_image\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfscope_1\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mSTD) \u001b[38;5;28;01mas\u001b[39;00m fscope_1:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file79c563p7.py:61\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__resize_to_range.<locals>.if_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mif_body\u001b[39m():\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;167;01mValueError\u001b[39;00m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage should be 3D tensor\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Alexej\\AppData\\Local\\Temp\\ipykernel_2616\\4291391041.py\", line 11, in detect_fn  *\n        image, shapes = detection_model.preprocess(image)\n    File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 689, in preprocess  *\n        (resized_inputs,\n    File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\utils\\shape_utils.py\", line 492, in resize_images_and_return_shapes  *\n        outputs = static_or_dynamic_map_fn(\n    File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\utils\\shape_utils.py\", line 246, in static_or_dynamic_map_fn  *\n        outputs = [fn(arg) for arg in tf.unstack(elems)]\n    File \"c:\\Users\\Alexej\\miniconda3\\envs\\tf-keras\\lib\\site-packages\\object_detection\\core\\preprocessor.py\", line 3035, in resize_to_range  *\n        raise ValueError('Image should be 3D tensor')\n\n    ValueError: Image should be 3D tensor\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=1,\n",
    "                min_score_thresh=.9,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Laden der Label Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(LABLE_MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Konvertieren des Frames zu einem Tensor\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(frame, axis=0), dtype=tf.float32)\n",
    "    \n",
    "    # Durchführung der Objekterkennung\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Verarbeitung der erkannten Objekte\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Visualisierung der erkannten Objekte auf dem Frame\n",
    "    viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "        frame,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'],\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=100,  # Anzahl der maximal zu zeichnenden Boxen\n",
    "        min_score_thresh=0.5    # Minimale Vertrauensschwelle für die Anzeige\n",
    "    )\n",
    "\n",
    "    # Anzeigen des Frames mit den erkannten Objekten\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    \n",
    "    # Stoppen der Erfassung bei Drücken der 'q'-Taste\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
